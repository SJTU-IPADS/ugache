succeed=True
[CUDA] cuda: usage: 5.34 GB
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g2 24}, {link #2 : g3 24},
1 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g3 24}, {link #2 : g0 24},
2 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g0 24}, {link #2 : g1 24},
3 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g1 24}, {link #2 : g2 24},
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID
Set parameter TimeLimit to value 200
Set parameter MIPGap to value 0.05
Set parameter LogFile to value "cppsolver.log"
Set parameter Threads to value 40
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Gurobi Optimizer version 9.5.1 build v9.5.1rc2 (linux64)
Thread count: 40 physical cores, 80 logical processors, using up to 40 threads
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Optimize a model with 20204 rows, 6601 columns and 62104 nonzeros
Model fingerprint: 0xb3ec4809
Variable types: 5 continuous, 6596 integer (6596 binary)
Coefficient statistics:
  Matrix range     [2e-09, 8e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 8e+03]
Warning: Model contains large matrix coefficient range
         Consider reformulating model or setting NumericFocus parameter
         to avoid numerical issues.
Found heuristic solution: objective 2.000000e+09
Presolve removed 14006 rows and 37 columns
Presolve time: 0.08s
Presolved: 6198 rows, 6564 columns, 29721 nonzeros
Found heuristic solution: objective 35059.439580
Variable types: 1 continuous, 6563 integer (6562 binary)

Root relaxation: objective 7.105457e+03, 6899 iterations, 0.09 seconds (0.07 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0 7105.45743    0   21 35059.4396 7105.45743  79.7%     -    0s
H    0     0                    7123.9267523 7105.45743  0.26%     -    0s

Explored 1 nodes (9757 simplex iterations) in 0.27 seconds (0.21 work units)
Thread count was 40 (of 80 available processors)

Solution count 3: 7123.93 35059.4 2e+09 

Optimal solution found (tolerance 5.00e-02)
Best objective 7.123926752251e+03, best bound 7.105457434824e+03, gap 0.2593%
coll_cache:optimal_local_rate=0.244977,0.20765,0.190988,0.227021,
coll_cache:optimal_remote_rate=0.625659,0.662986,0.679648,0.643615,
coll_cache:optimal_cpu_rate=0.129364,0.129364,0.129364,0.129364,
z=7123.93
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
config:eval_tsp="2023-08-06 08:46:54"
config:num_worker=4
config:num_intra_size=4
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=com-friendster
config:epochs=4
config:batchsize=2000
config:skip_epoch=2
config:local_step=250
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.07
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=40
config:unsupervised=False
config:classnum=100
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7f076d25e820>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=4
Rank=0, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.4152665138244629
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 9.298324584960938e-06, transfer latency: 0.3872373104095459
epoch=4 total_steps=500
presamping
presamping takes 7.391474485397339
start training...
[Epoch 0][Step 0], time=1.1567611694335938, ext_time=0.017409086227416992, train_time=1.1322715282440186
[Epoch 0][Step 1], time=0.038712501525878906, ext_time=0.012223243713378906, train_time=0.02054762840270996
[Epoch 0][Step 2], time=0.03740549087524414, ext_time=0.012160062789916992, train_time=0.020221948623657227
[Epoch 0][Step 3], time=0.03738546371459961, ext_time=0.012103080749511719, train_time=0.02030467987060547
[Epoch 0][Step 4], time=0.036707401275634766, ext_time=0.011929988861083984, train_time=0.01990342140197754
[Epoch 0][Step 5], time=0.03766036033630371, ext_time=0.012195587158203125, train_time=0.020423412322998047
[Epoch 0][Step 6], time=0.03711295127868652, ext_time=0.011924505233764648, train_time=0.020316123962402344
[Epoch 0][Step 7], time=0.036077260971069336, ext_time=0.012023687362670898, train_time=0.019137859344482422
[Epoch 0][Step 8], time=0.03706479072570801, ext_time=0.01189279556274414, train_time=0.020281553268432617
[Epoch 0][Step 9], time=0.03605818748474121, ext_time=0.012188911437988281, train_time=0.018878698348999023
[Epoch 0][Step 10], time=0.0361785888671875, ext_time=0.01199483871459961, train_time=0.019278526306152344
[Epoch 0][Step 11], time=0.03668212890625, ext_time=0.012104034423828125, train_time=0.019603490829467773
[Epoch 0][Step 12], time=0.03665471076965332, ext_time=0.012042999267578125, train_time=0.019712448120117188
[Epoch 0][Step 13], time=0.03710174560546875, ext_time=0.012053489685058594, train_time=0.020087718963623047
[Epoch 0][Step 14], time=0.03632616996765137, ext_time=0.012374639511108398, train_time=0.018912076950073242
[Epoch 0][Step 15], time=0.03574800491333008, ext_time=0.012060880661010742, train_time=0.0187833309173584
[Epoch 0][Step 16], time=0.03665590286254883, ext_time=0.011960506439208984, train_time=0.01975560188293457
[Epoch 0][Step 17], time=0.036917686462402344, ext_time=0.011850118637084961, train_time=0.02024364471435547
[Epoch 0][Step 18], time=0.03674030303955078, ext_time=0.01181793212890625, train_time=0.020072460174560547
[Epoch 0][Step 19], time=0.035981178283691406, ext_time=0.011964559555053711, train_time=0.01909351348876953
[Epoch 0][Step 20], time=0.03685712814331055, ext_time=0.011990070343017578, train_time=0.019960641860961914
[Epoch 0][Step 21], time=0.03699612617492676, ext_time=0.012258052825927734, train_time=0.019746780395507812
[Epoch 0][Step 22], time=0.03720426559448242, ext_time=0.012240409851074219, train_time=0.020030975341796875
[Epoch 0][Step 23], time=0.03811025619506836, ext_time=0.012228965759277344, train_time=0.0207827091217041
[Epoch 0][Step 24], time=0.03651595115661621, ext_time=0.011885881423950195, train_time=0.019746780395507812
[Epoch 0][Step 25], time=0.037970542907714844, ext_time=0.01233983039855957, train_time=0.02059340476989746
[Epoch 0][Step 26], time=0.03726530075073242, ext_time=0.012002229690551758, train_time=0.020285844802856445
[Epoch 0][Step 27], time=0.03665876388549805, ext_time=0.011864423751831055, train_time=0.019917964935302734
[Epoch 0][Step 28], time=0.0362398624420166, ext_time=0.011922836303710938, train_time=0.019475698471069336
[Epoch 0][Step 29], time=0.03718900680541992, ext_time=0.012095928192138672, train_time=0.02013564109802246
[Epoch 0][Step 30], time=0.03643608093261719, ext_time=0.011824846267700195, train_time=0.019760608673095703
[Epoch 0][Step 31], time=0.03691864013671875, ext_time=0.012302398681640625, train_time=0.01964116096496582
[Epoch 0][Step 32], time=0.03646731376647949, ext_time=0.012063741683959961, train_time=0.01946544647216797
[Epoch 0][Step 33], time=0.03653073310852051, ext_time=0.012116432189941406, train_time=0.019512653350830078
[Epoch 0][Step 34], time=0.038710832595825195, ext_time=0.011909961700439453, train_time=0.01930832862854004
[Epoch 0][Step 35], time=0.03694725036621094, ext_time=0.012308359146118164, train_time=0.0196530818939209
[Epoch 0][Step 36], time=0.036223649978637695, ext_time=0.011977195739746094, train_time=0.019321918487548828
[Epoch 0][Step 37], time=0.03709077835083008, ext_time=0.011991262435913086, train_time=0.020205020904541016
[Epoch 0][Step 38], time=0.037604331970214844, ext_time=0.012224912643432617, train_time=0.02038097381591797
[Epoch 0][Step 39], time=0.037401437759399414, ext_time=0.011916637420654297, train_time=0.02063155174255371
[Epoch 0][Step 40], time=0.037378549575805664, ext_time=0.011626958847045898, train_time=0.020966053009033203
[Epoch 0][Step 41], time=0.036450862884521484, ext_time=0.012152671813964844, train_time=0.019364595413208008
[Epoch 0][Step 42], time=0.0397343635559082, ext_time=0.012243986129760742, train_time=0.0225067138671875
[Epoch 0][Step 43], time=0.036429405212402344, ext_time=0.01200246810913086, train_time=0.019170284271240234
[Epoch 0][Step 44], time=0.03767085075378418, ext_time=0.011814594268798828, train_time=0.021006107330322266
[Epoch 0][Step 45], time=0.036478519439697266, ext_time=0.012299776077270508, train_time=0.019219398498535156
[Epoch 0][Step 46], time=0.03680062294006348, ext_time=0.012401103973388672, train_time=0.019385337829589844
[Epoch 0][Step 47], time=0.03983426094055176, ext_time=0.012066364288330078, train_time=0.02280259132385254
[Epoch 0][Step 48], time=0.03623175621032715, ext_time=0.011769533157348633, train_time=0.019640445709228516
[Epoch 0][Step 49], time=0.0364837646484375, ext_time=0.01201319694519043, train_time=0.01957249641418457
[Epoch 0][Step 50], time=0.03710293769836426, ext_time=0.012058734893798828, train_time=0.02006077766418457
[Epoch 0][Step 51], time=0.037471771240234375, ext_time=0.012506961822509766, train_time=0.019867658615112305
[Epoch 0][Step 52], time=0.03722190856933594, ext_time=0.012006759643554688, train_time=0.020299196243286133
[Epoch 0][Step 53], time=0.03729510307312012, ext_time=0.011760711669921875, train_time=0.020627975463867188
[Epoch 0][Step 54], time=0.03647899627685547, ext_time=0.011876106262207031, train_time=0.019728422164916992
[Epoch 0][Step 55], time=0.03685641288757324, ext_time=0.012143373489379883, train_time=0.01976752281188965
[Epoch 0][Step 56], time=0.03690052032470703, ext_time=0.012018918991088867, train_time=0.019951581954956055
[Epoch 0][Step 57], time=0.03769493103027344, ext_time=0.011804819107055664, train_time=0.021053791046142578
[Epoch 0][Step 58], time=0.038626909255981445, ext_time=0.012384414672851562, train_time=0.021224021911621094
[Epoch 0][Step 59], time=0.03750443458557129, ext_time=0.012138128280639648, train_time=0.020364999771118164
[Epoch 0][Step 60], time=0.03714942932128906, ext_time=0.01204061508178711, train_time=0.02022242546081543
[Epoch 0][Step 61], time=0.03649139404296875, ext_time=0.011993408203125, train_time=0.01958751678466797
[Epoch 0][Step 62], time=0.03634762763977051, ext_time=0.01222991943359375, train_time=0.019164323806762695
[Epoch 0][Step 63], time=0.036573171615600586, ext_time=0.011851310729980469, train_time=0.019850730895996094
[Epoch 0][Step 64], time=0.036627769470214844, ext_time=0.012068748474121094, train_time=0.019600391387939453
[Epoch 0][Step 65], time=0.03619027137756348, ext_time=0.011996746063232422, train_time=0.019254207611083984
[Epoch 0][Step 66], time=0.03677678108215332, ext_time=0.011778116226196289, train_time=0.020138025283813477
[Epoch 0][Step 67], time=0.03713560104370117, ext_time=0.012302160263061523, train_time=0.019886016845703125
[Epoch 0][Step 68], time=0.03699898719787598, ext_time=0.012453556060791016, train_time=0.01949477195739746
[Epoch 0][Step 69], time=0.03635883331298828, ext_time=0.011652469635009766, train_time=0.019621610641479492
[Epoch 0][Step 70], time=0.03770303726196289, ext_time=0.011971235275268555, train_time=0.02083730697631836
[Epoch 0][Step 71], time=0.03695797920227051, ext_time=0.012353181838989258, train_time=0.019560813903808594
[Epoch 0][Step 72], time=0.03710150718688965, ext_time=0.012012481689453125, train_time=0.02015089988708496
[Epoch 0][Step 73], time=0.03703927993774414, ext_time=0.012245893478393555, train_time=0.019788742065429688
[Epoch 0][Step 74], time=0.0368649959564209, ext_time=0.012044668197631836, train_time=0.019907236099243164
[Epoch 0][Step 75], time=0.03764843940734863, ext_time=0.012382745742797852, train_time=0.02021002769470215
[Epoch 0][Step 76], time=0.03724312782287598, ext_time=0.012124061584472656, train_time=0.020193099975585938
[Epoch 0][Step 77], time=0.03983759880065918, ext_time=0.011955738067626953, train_time=0.022983312606811523
[Epoch 0][Step 78], time=0.036769866943359375, ext_time=0.01190638542175293, train_time=0.019927501678466797
[Epoch 0][Step 79], time=0.03724241256713867, ext_time=0.012094974517822266, train_time=0.020192861557006836
[Epoch 0][Step 80], time=0.03686928749084473, ext_time=0.011512279510498047, train_time=0.020607948303222656
[Epoch 0][Step 81], time=0.037210702896118164, ext_time=0.011985540390014648, train_time=0.020262479782104492
[Epoch 0][Step 82], time=0.037334442138671875, ext_time=0.012242555618286133, train_time=0.020157337188720703
[Epoch 0][Step 83], time=0.03764462471008301, ext_time=0.011785268783569336, train_time=0.02095770835876465
[Epoch 0][Step 84], time=0.03792881965637207, ext_time=0.012195825576782227, train_time=0.02076888084411621
[Epoch 0][Step 85], time=0.03660702705383301, ext_time=0.011984586715698242, train_time=0.019773244857788086
[Epoch 0][Step 86], time=0.0370335578918457, ext_time=0.012274026870727539, train_time=0.019781827926635742
[Epoch 0][Step 87], time=0.03702878952026367, ext_time=0.011833429336547852, train_time=0.020323753356933594
[Epoch 0][Step 88], time=0.03751039505004883, ext_time=0.012237071990966797, train_time=0.020282983779907227
[Epoch 0][Step 89], time=0.0366208553314209, ext_time=0.011769294738769531, train_time=0.02008366584777832
[Epoch 0][Step 90], time=0.037636756896972656, ext_time=0.012090206146240234, train_time=0.020600318908691406
[Epoch 0][Step 91], time=0.03768467903137207, ext_time=0.01219630241394043, train_time=0.01997852325439453
[Epoch 0][Step 92], time=0.03875327110290527, ext_time=0.011746644973754883, train_time=0.022200345993041992
[Epoch 0][Step 93], time=0.03776812553405762, ext_time=0.011986017227172852, train_time=0.020827770233154297
[Epoch 0][Step 94], time=0.03691911697387695, ext_time=0.012321710586547852, train_time=0.019570112228393555
[Epoch 0][Step 95], time=0.036956787109375, ext_time=0.011800289154052734, train_time=0.020311832427978516
[Epoch 0][Step 96], time=0.03716468811035156, ext_time=0.011774539947509766, train_time=0.020495176315307617
[Epoch 0][Step 97], time=0.037847280502319336, ext_time=0.012126445770263672, train_time=0.020770788192749023
[Epoch 0][Step 98], time=0.03668355941772461, ext_time=0.012038230895996094, train_time=0.019803285598754883
[Epoch 0][Step 99], time=0.03801679611206055, ext_time=0.011908292770385742, train_time=0.021256446838378906
[Epoch 0][Step 100], time=0.03722548484802246, ext_time=0.011908531188964844, train_time=0.020422935485839844
[Epoch 0][Step 101], time=0.03641676902770996, ext_time=0.01187276840209961, train_time=0.01970386505126953
[Epoch 0][Step 102], time=0.036828041076660156, ext_time=0.011950016021728516, train_time=0.019994497299194336
[Epoch 0][Step 103], time=0.03714489936828613, ext_time=0.012345075607299805, train_time=0.019747257232666016
[Epoch 0][Step 104], time=0.03656125068664551, ext_time=0.011915206909179688, train_time=0.019777297973632812
[Epoch 0][Step 105], time=0.03684210777282715, ext_time=0.012410879135131836, train_time=0.019466876983642578
[Epoch 0][Step 106], time=0.03794431686401367, ext_time=0.012042760848999023, train_time=0.020987987518310547
[Epoch 0][Step 107], time=0.036715030670166016, ext_time=0.011994361877441406, train_time=0.019841432571411133
[Epoch 0][Step 108], time=0.03700399398803711, ext_time=0.011768817901611328, train_time=0.02035045623779297
[Epoch 0][Step 109], time=0.03685498237609863, ext_time=0.01219487190246582, train_time=0.019697904586791992
[Epoch 0][Step 110], time=0.03692817687988281, ext_time=0.012263774871826172, train_time=0.01965045928955078
[Epoch 0][Step 111], time=0.03726339340209961, ext_time=0.012402534484863281, train_time=0.019880294799804688
[Epoch 0][Step 112], time=0.03658032417297363, ext_time=0.011887550354003906, train_time=0.019825220108032227
[Epoch 0][Step 113], time=0.0369725227355957, ext_time=0.012091636657714844, train_time=0.01995110511779785
[Epoch 0][Step 114], time=0.0363774299621582, ext_time=0.011831283569335938, train_time=0.019693374633789062
[Epoch 0][Step 115], time=0.03770732879638672, ext_time=0.011800765991210938, train_time=0.021075725555419922
[Epoch 0][Step 116], time=0.03651738166809082, ext_time=0.011840343475341797, train_time=0.01979804039001465
[Epoch 0][Step 117], time=0.03722357749938965, ext_time=0.011698246002197266, train_time=0.02071833610534668
[Epoch 0][Step 118], time=0.03744983673095703, ext_time=0.012262821197509766, train_time=0.020160913467407227
[Epoch 0][Step 119], time=0.0374298095703125, ext_time=0.012038230895996094, train_time=0.02047133445739746
[Epoch 0][Step 120], time=0.03644132614135742, ext_time=0.012014150619506836, train_time=0.01953864097595215
[Epoch 0][Step 121], time=0.03675651550292969, ext_time=0.011631965637207031, train_time=0.020258188247680664
[Epoch 0][Step 122], time=0.036162614822387695, ext_time=0.01198267936706543, train_time=0.019326448440551758
[Epoch 0][Step 123], time=0.037082672119140625, ext_time=0.012027263641357422, train_time=0.02010321617126465
[Epoch 0][Step 124], time=0.037071943283081055, ext_time=0.012285947799682617, train_time=0.01978468894958496
[Epoch 0], time=5.764747619628906, loss=3.874769449234009
[Epoch 1][Step 0], time=0.03860163688659668, ext_time=0.012428045272827148, train_time=0.02102494239807129
[Epoch 1][Step 1], time=0.03625774383544922, ext_time=0.012122154235839844, train_time=0.019155025482177734
[Epoch 1][Step 2], time=0.03672337532043457, ext_time=0.012190818786621094, train_time=0.01943206787109375
[Epoch 1][Step 3], time=0.037194252014160156, ext_time=0.012131929397583008, train_time=0.020114898681640625
[Epoch 1][Step 4], time=0.03720355033874512, ext_time=0.011899709701538086, train_time=0.020432233810424805
[Epoch 1][Step 5], time=0.036914825439453125, ext_time=0.012253761291503906, train_time=0.01968669891357422
[Epoch 1][Step 6], time=0.0399320125579834, ext_time=0.011919260025024414, train_time=0.023165225982666016
[Epoch 1][Step 7], time=0.03620409965515137, ext_time=0.011986494064331055, train_time=0.01931452751159668
[Epoch 1][Step 8], time=0.036841392517089844, ext_time=0.01192784309387207, train_time=0.020019054412841797
[Epoch 1][Step 9], time=0.03654932975769043, ext_time=0.012144327163696289, train_time=0.019397735595703125
[Epoch 1][Step 10], time=0.03679037094116211, ext_time=0.012053489685058594, train_time=0.019817829132080078
[Epoch 1][Step 11], time=0.03636908531188965, ext_time=0.012070655822753906, train_time=0.019316673278808594
[Epoch 1][Step 12], time=0.03682279586791992, ext_time=0.011978387832641602, train_time=0.01995849609375
[Epoch 1][Step 13], time=0.03769040107727051, ext_time=0.012169122695922852, train_time=0.020566940307617188
[Epoch 1][Step 14], time=0.036871910095214844, ext_time=0.012379646301269531, train_time=0.01943659782409668
[Epoch 1][Step 15], time=0.03592872619628906, ext_time=0.012060880661010742, train_time=0.01895928382873535
[Epoch 1][Step 16], time=0.03679084777832031, ext_time=0.011991024017333984, train_time=0.01986408233642578
[Epoch 1][Step 17], time=0.03730344772338867, ext_time=0.01195073127746582, train_time=0.020509719848632812
[Epoch 1][Step 18], time=0.03716421127319336, ext_time=0.011749029159545898, train_time=0.020526647567749023
[Epoch 1][Step 19], time=0.03580188751220703, ext_time=0.011854887008666992, train_time=0.01906871795654297
[Epoch 1][Step 20], time=0.03628873825073242, ext_time=0.011983156204223633, train_time=0.019420385360717773
[Epoch 1][Step 21], time=0.03713059425354004, ext_time=0.012221336364746094, train_time=0.01993393898010254
[Epoch 1][Step 22], time=0.03635287284851074, ext_time=0.012123346328735352, train_time=0.019273996353149414
[Epoch 1][Step 23], time=0.03670930862426758, ext_time=0.012282371520996094, train_time=0.0194244384765625
[Epoch 1][Step 24], time=0.036724090576171875, ext_time=0.01190042495727539, train_time=0.019988536834716797
[Epoch 1][Step 25], time=0.03688836097717285, ext_time=0.012418270111083984, train_time=0.019418001174926758
[Epoch 1][Step 26], time=0.036756277084350586, ext_time=0.012027978897094727, train_time=0.019840002059936523
[Epoch 1][Step 27], time=0.03641819953918457, ext_time=0.011969327926635742, train_time=0.019577503204345703
[Epoch 1][Step 28], time=0.036339521408081055, ext_time=0.011925935745239258, train_time=0.01958775520324707
[Epoch 1][Step 29], time=0.03690910339355469, ext_time=0.012155771255493164, train_time=0.01979970932006836
[Epoch 1][Step 30], time=0.03621625900268555, ext_time=0.01187443733215332, train_time=0.01950979232788086
[Epoch 1][Step 31], time=0.03698134422302246, ext_time=0.01234292984008789, train_time=0.019685983657836914
[Epoch 1][Step 32], time=0.036861419677734375, ext_time=0.012053489685058594, train_time=0.019872426986694336
[Epoch 1][Step 33], time=0.03666877746582031, ext_time=0.01221609115600586, train_time=0.01955556869506836
[Epoch 1][Step 34], time=0.036416053771972656, ext_time=0.011874914169311523, train_time=0.019708871841430664
[Epoch 1][Step 35], time=0.036826372146606445, ext_time=0.012338638305664062, train_time=0.019487619400024414
[Epoch 1][Step 36], time=0.03673362731933594, ext_time=0.011978626251220703, train_time=0.019803762435913086
[Epoch 1][Step 37], time=0.03699517250061035, ext_time=0.012038707733154297, train_time=0.02005767822265625
[Epoch 1][Step 38], time=0.037489891052246094, ext_time=0.012247562408447266, train_time=0.020231246948242188
[Epoch 1][Step 39], time=0.03732657432556152, ext_time=0.011865854263305664, train_time=0.02060246467590332
[Epoch 1][Step 40], time=0.03734254837036133, ext_time=0.011562347412109375, train_time=0.02099752426147461
[Epoch 1][Step 41], time=0.03666043281555176, ext_time=0.01215672492980957, train_time=0.01955890655517578
[Epoch 1][Step 42], time=0.03704357147216797, ext_time=0.012234210968017578, train_time=0.019805908203125
[Epoch 1][Step 43], time=0.03634762763977051, ext_time=0.011958599090576172, train_time=0.019499540328979492
[Epoch 1][Step 44], time=0.03691434860229492, ext_time=0.011744976043701172, train_time=0.020342111587524414
[Epoch 1][Step 45], time=0.03858518600463867, ext_time=0.012267351150512695, train_time=0.021364212036132812
[Epoch 1][Step 46], time=0.03690600395202637, ext_time=0.01236581802368164, train_time=0.019524097442626953
[Epoch 1][Step 47], time=0.03905963897705078, ext_time=0.012069463729858398, train_time=0.022063255310058594
[Epoch 1][Step 48], time=0.03656005859375, ext_time=0.011795520782470703, train_time=0.019959449768066406
[Epoch 1][Step 49], time=0.03744196891784668, ext_time=0.012043476104736328, train_time=0.020513296127319336
[Epoch 1][Step 50], time=0.03641986846923828, ext_time=0.012089252471923828, train_time=0.019377470016479492
[Epoch 1][Step 51], time=0.03741931915283203, ext_time=0.012480020523071289, train_time=0.019855737686157227
[Epoch 1][Step 52], time=0.036678314208984375, ext_time=0.011972427368164062, train_time=0.019811391830444336
[Epoch 1][Step 53], time=0.03629183769226074, ext_time=0.011809110641479492, train_time=0.01962900161743164
[Epoch 1][Step 54], time=0.03680849075317383, ext_time=0.011876344680786133, train_time=0.020055770874023438
[Epoch 1][Step 55], time=0.03671693801879883, ext_time=0.012156963348388672, train_time=0.01951313018798828
[Epoch 1][Step 56], time=0.03649497032165527, ext_time=0.011929035186767578, train_time=0.019654512405395508
[Epoch 1][Step 57], time=0.03739643096923828, ext_time=0.011723518371582031, train_time=0.020823001861572266
[Epoch 1][Step 58], time=0.03717827796936035, ext_time=0.012358427047729492, train_time=0.0197904109954834
[Epoch 1][Step 59], time=0.037270307540893555, ext_time=0.012098073959350586, train_time=0.020155906677246094
[Epoch 1][Step 60], time=0.03658699989318848, ext_time=0.01203608512878418, train_time=0.0196533203125
[Epoch 1][Step 61], time=0.03671455383300781, ext_time=0.01212310791015625, train_time=0.019701480865478516
[Epoch 1][Step 62], time=0.03651714324951172, ext_time=0.012275218963623047, train_time=0.019296884536743164
[Epoch 1][Step 63], time=0.03634524345397949, ext_time=0.011912345886230469, train_time=0.01956343650817871
[Epoch 1][Step 64], time=0.03643679618835449, ext_time=0.012091398239135742, train_time=0.019383907318115234
[Epoch 1][Step 65], time=0.03660917282104492, ext_time=0.011991500854492188, train_time=0.019690513610839844
[Epoch 1][Step 66], time=0.03625082969665527, ext_time=0.01180577278137207, train_time=0.019595861434936523
[Epoch 1][Step 67], time=0.036502838134765625, ext_time=0.012286186218261719, train_time=0.019208908081054688
[Epoch 1][Step 68], time=0.03704190254211426, ext_time=0.012392520904541016, train_time=0.01956915855407715
[Epoch 1][Step 69], time=0.03585195541381836, ext_time=0.011601686477661133, train_time=0.01945185661315918
[Epoch 1][Step 70], time=0.03706026077270508, ext_time=0.011953592300415039, train_time=0.020201921463012695
[Epoch 1][Step 71], time=0.03675508499145508, ext_time=0.012337684631347656, train_time=0.019417524337768555
[Epoch 1][Step 72], time=0.03718423843383789, ext_time=0.01204371452331543, train_time=0.019896984100341797
[Epoch 1][Step 73], time=0.03665947914123535, ext_time=0.01225423812866211, train_time=0.019397974014282227
[Epoch 1][Step 74], time=0.0361785888671875, ext_time=0.012039661407470703, train_time=0.019251585006713867
[Epoch 1][Step 75], time=0.03686118125915527, ext_time=0.01237344741821289, train_time=0.019473552703857422
[Epoch 1][Step 76], time=0.03678584098815918, ext_time=0.0120391845703125, train_time=0.019825458526611328
[Epoch 1][Step 77], time=0.036768198013305664, ext_time=0.011918783187866211, train_time=0.019960403442382812
[Epoch 1][Step 78], time=0.03585052490234375, ext_time=0.011847257614135742, train_time=0.01918339729309082
[Epoch 1][Step 79], time=0.0366058349609375, ext_time=0.012129068374633789, train_time=0.019573211669921875
[Epoch 1][Step 80], time=0.03597235679626465, ext_time=0.011500358581542969, train_time=0.01974654197692871
[Epoch 1][Step 81], time=0.03818058967590332, ext_time=0.012024879455566406, train_time=0.021192073822021484
[Epoch 1][Step 82], time=0.03665733337402344, ext_time=0.012139081954956055, train_time=0.01959514617919922
[Epoch 1][Step 83], time=0.036431312561035156, ext_time=0.011749982833862305, train_time=0.019815683364868164
[Epoch 1][Step 84], time=0.03722214698791504, ext_time=0.012168169021606445, train_time=0.020091533660888672
[Epoch 1][Step 85], time=0.03629112243652344, ext_time=0.01196432113647461, train_time=0.019492626190185547
[Epoch 1][Step 86], time=0.03746986389160156, ext_time=0.012265443801879883, train_time=0.020247459411621094
[Epoch 1][Step 87], time=0.0366666316986084, ext_time=0.011868715286254883, train_time=0.0198974609375
[Epoch 1][Step 88], time=0.03646349906921387, ext_time=0.012235403060913086, train_time=0.0192413330078125
[Epoch 1][Step 89], time=0.03640007972717285, ext_time=0.011791706085205078, train_time=0.01984429359436035
[Epoch 1][Step 90], time=0.03712582588195801, ext_time=0.01208949089050293, train_time=0.020092487335205078
[Epoch 1][Step 91], time=0.03627133369445801, ext_time=0.01206350326538086, train_time=0.019274473190307617
[Epoch 1][Step 92], time=0.03596949577331543, ext_time=0.01174616813659668, train_time=0.019431114196777344
[Epoch 1][Step 93], time=0.036357879638671875, ext_time=0.011986732482910156, train_time=0.01943516731262207
[Epoch 1][Step 94], time=0.03724527359008789, ext_time=0.012353658676147461, train_time=0.019865036010742188
[Epoch 1][Step 95], time=0.0365297794342041, ext_time=0.011735916137695312, train_time=0.01993560791015625
[Epoch 1][Step 96], time=0.03708529472351074, ext_time=0.01171731948852539, train_time=0.02050948143005371
[Epoch 1][Step 97], time=0.037430524826049805, ext_time=0.01220560073852539, train_time=0.020280838012695312
[Epoch 1][Step 98], time=0.036615848541259766, ext_time=0.012102603912353516, train_time=0.019625425338745117
[Epoch 1][Step 99], time=0.037583112716674805, ext_time=0.011960983276367188, train_time=0.020774126052856445
[Epoch 1][Step 100], time=0.03679943084716797, ext_time=0.011948347091674805, train_time=0.019979238510131836
[Epoch 1][Step 101], time=0.03624892234802246, ext_time=0.011806011199951172, train_time=0.01960897445678711
[Epoch 1][Step 102], time=0.036798954010009766, ext_time=0.011957645416259766, train_time=0.019978046417236328
[Epoch 1][Step 103], time=0.037262678146362305, ext_time=0.012427091598510742, train_time=0.01977825164794922
[Epoch 1][Step 104], time=0.03762698173522949, ext_time=0.011919498443603516, train_time=0.020859479904174805
[Epoch 1][Step 105], time=0.03700995445251465, ext_time=0.012343406677246094, train_time=0.019692182540893555
[Epoch 1][Step 106], time=0.03660845756530762, ext_time=0.012062311172485352, train_time=0.01962447166442871
[Epoch 1][Step 107], time=0.0363006591796875, ext_time=0.0119171142578125, train_time=0.01946878433227539
[Epoch 1][Step 108], time=0.036322593688964844, ext_time=0.011831283569335938, train_time=0.01962113380432129
[Epoch 1][Step 109], time=0.03689289093017578, ext_time=0.012140274047851562, train_time=0.0197908878326416
[Epoch 1][Step 110], time=0.03688240051269531, ext_time=0.012214422225952148, train_time=0.019692659378051758
[Epoch 1][Step 111], time=0.03669285774230957, ext_time=0.012364625930786133, train_time=0.019303321838378906
[Epoch 1][Step 112], time=0.03647947311401367, ext_time=0.011933565139770508, train_time=0.019644975662231445
[Epoch 1][Step 113], time=0.04002094268798828, ext_time=0.01204681396484375, train_time=0.02303600311279297
[Epoch 1][Step 114], time=0.039574384689331055, ext_time=0.011818170547485352, train_time=0.022893905639648438
[Epoch 1][Step 115], time=0.03737139701843262, ext_time=0.011811017990112305, train_time=0.020754575729370117
[Epoch 1][Step 116], time=0.03771400451660156, ext_time=0.011781454086303711, train_time=0.0210726261138916
[Epoch 1][Step 117], time=0.036933183670043945, ext_time=0.011723041534423828, train_time=0.020387887954711914
[Epoch 1][Step 118], time=0.03661632537841797, ext_time=0.012254476547241211, train_time=0.019367456436157227
[Epoch 1][Step 119], time=0.037754058837890625, ext_time=0.012000799179077148, train_time=0.02082681655883789
[Epoch 1][Step 120], time=0.0366663932800293, ext_time=0.01197504997253418, train_time=0.01981520652770996
[Epoch 1][Step 121], time=0.03628087043762207, ext_time=0.011661529541015625, train_time=0.019774675369262695
[Epoch 1][Step 122], time=0.03995060920715332, ext_time=0.012046098709106445, train_time=0.02268815040588379
[Epoch 1][Step 123], time=0.03663945198059082, ext_time=0.012075424194335938, train_time=0.01930689811706543
[Epoch 1][Step 124], time=0.03734469413757324, ext_time=0.012287616729736328, train_time=0.019765377044677734
[Epoch 1], time=4.6236572265625, loss=3.159688949584961
[Epoch 2][Step 0], time=0.03787088394165039, ext_time=0.012506484985351562, train_time=0.019910573959350586
[Epoch 2][Step 1], time=0.036621809005737305, ext_time=0.012195348739624023, train_time=0.019178390502929688
[Epoch 2][Step 2], time=0.0363621711730957, ext_time=0.012131214141845703, train_time=0.019281387329101562
[Epoch 2][Step 3], time=0.037008047103881836, ext_time=0.012169599533081055, train_time=0.019921302795410156
[Epoch 2][Step 4], time=0.03664517402648926, ext_time=0.011846780776977539, train_time=0.019954204559326172
[Epoch 2][Step 5], time=0.03672456741333008, ext_time=0.012293815612792969, train_time=0.019443035125732422
[Epoch 2][Step 6], time=0.03673386573791504, ext_time=0.01184391975402832, train_time=0.020055532455444336
[Epoch 2][Step 7], time=0.03590989112854004, ext_time=0.011977195739746094, train_time=0.019023895263671875
[Epoch 2][Step 8], time=0.03662729263305664, ext_time=0.011926412582397461, train_time=0.019801616668701172
[Epoch 2][Step 9], time=0.03634071350097656, ext_time=0.012183189392089844, train_time=0.01920461654663086
[Epoch 2][Step 10], time=0.036421775817871094, ext_time=0.012031078338623047, train_time=0.019514083862304688
[Epoch 2][Step 11], time=0.03658127784729004, ext_time=0.012083053588867188, train_time=0.019540071487426758
[Epoch 2][Step 12], time=0.03656506538391113, ext_time=0.012006282806396484, train_time=0.01965641975402832
[Epoch 2][Step 13], time=0.03734540939331055, ext_time=0.012127399444580078, train_time=0.02025580406188965
[Epoch 2][Step 14], time=0.03699803352355957, ext_time=0.012328386306762695, train_time=0.019645214080810547
[Epoch 2][Step 15], time=0.036093950271606445, ext_time=0.012176275253295898, train_time=0.019036054611206055
[Epoch 2][Step 16], time=0.036566972732543945, ext_time=0.011962175369262695, train_time=0.019703149795532227
[Epoch 2][Step 17], time=0.03635430335998535, ext_time=0.012003421783447266, train_time=0.01929163932800293
[Epoch 2][Step 18], time=0.03829169273376465, ext_time=0.011799097061157227, train_time=0.021632909774780273
[Epoch 2][Step 19], time=0.03599119186401367, ext_time=0.011903762817382812, train_time=0.019215822219848633
[Epoch 2][Step 20], time=0.03629779815673828, ext_time=0.011923551559448242, train_time=0.0195157527923584
[Epoch 2][Step 21], time=0.03683614730834961, ext_time=0.012305974960327148, train_time=0.01955890655517578
[Epoch 2][Step 22], time=0.036321401596069336, ext_time=0.012130022048950195, train_time=0.01924729347229004
[Epoch 2][Step 23], time=0.036579132080078125, ext_time=0.012276649475097656, train_time=0.019311189651489258
[Epoch 2][Step 24], time=0.03625988960266113, ext_time=0.01189875602722168, train_time=0.01952838897705078
[Epoch 2][Step 25], time=0.03686046600341797, ext_time=0.01239919662475586, train_time=0.01942753791809082
[Epoch 2][Step 26], time=0.036974191665649414, ext_time=0.011995077133178711, train_time=0.020097732543945312
[Epoch 2][Step 27], time=0.036173105239868164, ext_time=0.011920928955078125, train_time=0.019382238388061523
[Epoch 2][Step 28], time=0.03602743148803711, ext_time=0.011883258819580078, train_time=0.0193021297454834
[Epoch 2][Step 29], time=0.0373387336730957, ext_time=0.012189149856567383, train_time=0.02017664909362793
[Epoch 2][Step 30], time=0.03599739074707031, ext_time=0.01190042495727539, train_time=0.019285917282104492
[Epoch 2][Step 31], time=0.03636670112609863, ext_time=0.012313365936279297, train_time=0.019102096557617188
[Epoch 2][Step 32], time=0.037127017974853516, ext_time=0.012035369873046875, train_time=0.020177125930786133
[Epoch 2][Step 33], time=0.03710365295410156, ext_time=0.012125015258789062, train_time=0.020061016082763672
[Epoch 2][Step 34], time=0.03680849075317383, ext_time=0.011858463287353516, train_time=0.02012038230895996
[Epoch 2][Step 35], time=0.03650856018066406, ext_time=0.012360334396362305, train_time=0.019179105758666992
[Epoch 2][Step 36], time=0.035941362380981445, ext_time=0.012019872665405273, train_time=0.01901555061340332
[Epoch 2][Step 37], time=0.03690910339355469, ext_time=0.011972188949584961, train_time=0.02006053924560547
[Epoch 2][Step 38], time=0.03712010383605957, ext_time=0.012183427810668945, train_time=0.019972801208496094
[Epoch 2][Step 39], time=0.036920785903930664, ext_time=0.01190185546875, train_time=0.020187854766845703
[Epoch 2][Step 40], time=0.0367739200592041, ext_time=0.01153874397277832, train_time=0.020464420318603516
[Epoch 2][Step 41], time=0.03684830665588379, ext_time=0.012121915817260742, train_time=0.01980113983154297
[Epoch 2][Step 42], time=0.03662228584289551, ext_time=0.012263059616088867, train_time=0.019382476806640625
[Epoch 2][Step 43], time=0.03601241111755371, ext_time=0.011949300765991211, train_time=0.019195556640625
[Epoch 2][Step 44], time=0.03650665283203125, ext_time=0.011734485626220703, train_time=0.019944429397583008
[Epoch 2][Step 45], time=0.03635406494140625, ext_time=0.012220144271850586, train_time=0.019177675247192383
[Epoch 2][Step 46], time=0.03677725791931152, ext_time=0.012354135513305664, train_time=0.019430160522460938
[Epoch 2][Step 47], time=0.03727984428405762, ext_time=0.012082576751708984, train_time=0.020270347595214844
[Epoch 2][Step 48], time=0.036234378814697266, ext_time=0.011774301528930664, train_time=0.019668102264404297
[Epoch 2][Step 49], time=0.03641629219055176, ext_time=0.012063026428222656, train_time=0.01945948600769043
[Epoch 2][Step 50], time=0.03636431694030762, ext_time=0.012060880661010742, train_time=0.019336223602294922
[Epoch 2][Step 51], time=0.03729891777038574, ext_time=0.012479305267333984, train_time=0.019722938537597656
[Epoch 2][Step 52], time=0.037319183349609375, ext_time=0.01201486587524414, train_time=0.02040576934814453
[Epoch 2][Step 53], time=0.03591775894165039, ext_time=0.011801481246948242, train_time=0.019220352172851562
[Epoch 2][Step 54], time=0.035952091217041016, ext_time=0.011888742446899414, train_time=0.0192258358001709
[Epoch 2][Step 55], time=0.03651165962219238, ext_time=0.012122154235839844, train_time=0.01948070526123047
[Epoch 2][Step 56], time=0.036394596099853516, ext_time=0.01193547248840332, train_time=0.019557714462280273
[Epoch 2][Step 57], time=0.03707742691040039, ext_time=0.011781692504882812, train_time=0.020476818084716797
[Epoch 2][Step 58], time=0.03664565086364746, ext_time=0.012326717376708984, train_time=0.019295930862426758
[Epoch 2][Step 59], time=0.03725576400756836, ext_time=0.012096881866455078, train_time=0.020168066024780273
[Epoch 2][Step 60], time=0.03656172752380371, ext_time=0.01200103759765625, train_time=0.019682645797729492
[Epoch 2][Step 61], time=0.0361483097076416, ext_time=0.012044429779052734, train_time=0.01921701431274414
[Epoch 2][Step 62], time=0.0365595817565918, ext_time=0.01224660873413086, train_time=0.019367218017578125
[Epoch 2][Step 63], time=0.03601717948913574, ext_time=0.011974334716796875, train_time=0.019161462783813477
[Epoch 2][Step 64], time=0.03622865676879883, ext_time=0.012118339538574219, train_time=0.019129514694213867
[Epoch 2][Step 65], time=0.03655076026916504, ext_time=0.011954784393310547, train_time=0.01966381072998047
[Epoch 2][Step 66], time=0.0364532470703125, ext_time=0.011771202087402344, train_time=0.019809722900390625
[Epoch 2][Step 67], time=0.037222862243652344, ext_time=0.012297868728637695, train_time=0.019981861114501953
[Epoch 2][Step 68], time=0.03733325004577637, ext_time=0.012432575225830078, train_time=0.01986408233642578
[Epoch 2][Step 69], time=0.03714632987976074, ext_time=0.011706829071044922, train_time=0.02062201499938965
[Epoch 2][Step 70], time=0.0372471809387207, ext_time=0.01201009750366211, train_time=0.01992177963256836
[Epoch 2][Step 71], time=0.03679776191711426, ext_time=0.01236867904663086, train_time=0.01938652992248535
[Epoch 2][Step 72], time=0.03655219078063965, ext_time=0.011971235275268555, train_time=0.019635915756225586
[Epoch 2][Step 73], time=0.03694915771484375, ext_time=0.012291669845581055, train_time=0.019622087478637695
[Epoch 2][Step 74], time=0.03642153739929199, ext_time=0.012013673782348633, train_time=0.019506216049194336
[Epoch 2][Step 75], time=0.03706002235412598, ext_time=0.01241302490234375, train_time=0.019630908966064453
[Epoch 2][Step 76], time=0.03658723831176758, ext_time=0.012012243270874023, train_time=0.019663095474243164
[Epoch 2][Step 77], time=0.03665661811828613, ext_time=0.011983394622802734, train_time=0.01978015899658203
[Epoch 2][Step 78], time=0.03557324409484863, ext_time=0.011820793151855469, train_time=0.01891636848449707
[Epoch 2][Step 79], time=0.036330223083496094, ext_time=0.012113094329833984, train_time=0.019276857376098633
[Epoch 2][Step 80], time=0.036023616790771484, ext_time=0.011579751968383789, train_time=0.019666433334350586
[Epoch 2][Step 81], time=0.03676342964172363, ext_time=0.012001752853393555, train_time=0.0197908878326416
[Epoch 2][Step 82], time=0.03630852699279785, ext_time=0.01213526725769043, train_time=0.019271135330200195
[Epoch 2][Step 83], time=0.036218881607055664, ext_time=0.01177072525024414, train_time=0.01960158348083496
[Epoch 2][Step 84], time=0.036660194396972656, ext_time=0.012174606323242188, train_time=0.01952815055847168
[Epoch 2][Step 85], time=0.03602433204650879, ext_time=0.011950254440307617, train_time=0.01923847198486328
[Epoch 2][Step 86], time=0.03708195686340332, ext_time=0.012212991714477539, train_time=0.01990365982055664
[Epoch 2][Step 87], time=0.037703514099121094, ext_time=0.011783123016357422, train_time=0.02101278305053711
[Epoch 2][Step 88], time=0.03675580024719238, ext_time=0.012250661849975586, train_time=0.019496679306030273
[Epoch 2][Step 89], time=0.03674006462097168, ext_time=0.011701583862304688, train_time=0.020277738571166992
[Epoch 2][Step 90], time=0.03652167320251465, ext_time=0.012089729309082031, train_time=0.019500017166137695
[Epoch 2][Step 91], time=0.03634500503540039, ext_time=0.01204061508178711, train_time=0.01933884620666504
[Epoch 2][Step 92], time=0.03581738471984863, ext_time=0.011808395385742188, train_time=0.019147396087646484
[Epoch 2][Step 93], time=0.03645896911621094, ext_time=0.011986494064331055, train_time=0.019489288330078125
[Epoch 2][Step 94], time=0.036911725997924805, ext_time=0.012304306030273438, train_time=0.019598960876464844
[Epoch 2][Step 95], time=0.036221981048583984, ext_time=0.011743545532226562, train_time=0.019614219665527344
[Epoch 2][Step 96], time=0.03794431686401367, ext_time=0.011742591857910156, train_time=0.021342992782592773
[Epoch 2][Step 97], time=0.03711223602294922, ext_time=0.012160539627075195, train_time=0.019987106323242188
[Epoch 2][Step 98], time=0.036223411560058594, ext_time=0.012077808380126953, train_time=0.019310474395751953
[Epoch 2][Step 99], time=0.03718304634094238, ext_time=0.011887788772583008, train_time=0.02045726776123047
[Epoch 2][Step 100], time=0.03667736053466797, ext_time=0.011943817138671875, train_time=0.019854307174682617
[Epoch 2][Step 101], time=0.03633284568786621, ext_time=0.011869430541992188, train_time=0.019638538360595703
[Epoch 2][Step 102], time=0.03658294677734375, ext_time=0.011946439743041992, train_time=0.01975274085998535
[Epoch 2][Step 103], time=0.037081241607666016, ext_time=0.012442827224731445, train_time=0.019611835479736328
[Epoch 2][Step 104], time=0.03672933578491211, ext_time=0.011844635009765625, train_time=0.020023345947265625
[Epoch 2][Step 105], time=0.037093400955200195, ext_time=0.012376070022583008, train_time=0.019582271575927734
[Epoch 2][Step 106], time=0.0365598201751709, ext_time=0.012011051177978516, train_time=0.019620895385742188
[Epoch 2][Step 107], time=0.03644847869873047, ext_time=0.011916399002075195, train_time=0.019648075103759766
[Epoch 2][Step 108], time=0.03626537322998047, ext_time=0.011813879013061523, train_time=0.019568204879760742
[Epoch 2][Step 109], time=0.036438703536987305, ext_time=0.012194156646728516, train_time=0.019272565841674805
[Epoch 2][Step 110], time=0.036531925201416016, ext_time=0.012191295623779297, train_time=0.019385099411010742
[Epoch 2][Step 111], time=0.03703022003173828, ext_time=0.012365341186523438, train_time=0.019699573516845703
[Epoch 2][Step 112], time=0.03631448745727539, ext_time=0.011879920959472656, train_time=0.019553661346435547
[Epoch 2][Step 113], time=0.03641057014465332, ext_time=0.01201319694519043, train_time=0.019414901733398438
[Epoch 2][Step 114], time=0.03617739677429199, ext_time=0.011846065521240234, train_time=0.01948094367980957
[Epoch 2][Step 115], time=0.03655719757080078, ext_time=0.011790275573730469, train_time=0.01996922492980957
[Epoch 2][Step 116], time=0.03592109680175781, ext_time=0.011854887008666992, train_time=0.01921820640563965
[Epoch 2][Step 117], time=0.03664660453796387, ext_time=0.011733531951904297, train_time=0.020102977752685547
[Epoch 2][Step 118], time=0.03702259063720703, ext_time=0.012221574783325195, train_time=0.019822359085083008
[Epoch 2][Step 119], time=0.03718709945678711, ext_time=0.012116193771362305, train_time=0.020153284072875977
[Epoch 2][Step 120], time=0.03609728813171387, ext_time=0.011966705322265625, train_time=0.019234418869018555
[Epoch 2][Step 121], time=0.0362091064453125, ext_time=0.011684894561767578, train_time=0.019704341888427734
[Epoch 2][Step 122], time=0.0364222526550293, ext_time=0.011919975280761719, train_time=0.019669055938720703
[Epoch 2][Step 123], time=0.0367588996887207, ext_time=0.012018680572509766, train_time=0.019800186157226562
[Epoch 2][Step 124], time=0.03682684898376465, ext_time=0.012252569198608398, train_time=0.019575834274291992
[Epoch 2], time=4.588022947311401, loss=2.483710527420044
[Epoch 3][Step 0], time=0.03810000419616699, ext_time=0.012506484985351562, train_time=0.02046680450439453
[Epoch 3][Step 1], time=0.0358426570892334, ext_time=0.012131214141845703, train_time=0.018752336502075195
[Epoch 3][Step 2], time=0.036295413970947266, ext_time=0.012145280838012695, train_time=0.01920032501220703
[Epoch 3][Step 3], time=0.036749839782714844, ext_time=0.01218104362487793, train_time=0.01965022087097168
[Epoch 3][Step 4], time=0.036566734313964844, ext_time=0.011861801147460938, train_time=0.019856929779052734
[Epoch 3][Step 5], time=0.03684401512145996, ext_time=0.012216329574584961, train_time=0.019670486450195312
[Epoch 3][Step 6], time=0.036665916442871094, ext_time=0.011854171752929688, train_time=0.01993393898010254
[Epoch 3][Step 7], time=0.03594374656677246, ext_time=0.01197671890258789, train_time=0.019062042236328125
[Epoch 3][Step 8], time=0.03627133369445801, ext_time=0.011908769607543945, train_time=0.01947760581970215
[Epoch 3][Step 9], time=0.03624439239501953, ext_time=0.012155771255493164, train_time=0.01910710334777832
[Epoch 3][Step 10], time=0.036234140396118164, ext_time=0.011931419372558594, train_time=0.019418001174926758
[Epoch 3][Step 11], time=0.03673386573791504, ext_time=0.012021780014038086, train_time=0.019744873046875
[Epoch 3][Step 12], time=0.03699231147766113, ext_time=0.012027502059936523, train_time=0.020079851150512695
[Epoch 3][Step 13], time=0.037291765213012695, ext_time=0.012084722518920898, train_time=0.020244598388671875
[Epoch 3][Step 14], time=0.03664541244506836, ext_time=0.012291431427001953, train_time=0.01930093765258789
[Epoch 3][Step 15], time=0.03589773178100586, ext_time=0.012080669403076172, train_time=0.018931150436401367
[Epoch 3][Step 16], time=0.03696179389953613, ext_time=0.012078046798706055, train_time=0.019972562789916992
[Epoch 3][Step 17], time=0.036309242248535156, ext_time=0.011884212493896484, train_time=0.019580841064453125
[Epoch 3][Step 18], time=0.036804914474487305, ext_time=0.011827707290649414, train_time=0.020101308822631836
[Epoch 3][Step 19], time=0.03533315658569336, ext_time=0.011856794357299805, train_time=0.01859140396118164
[Epoch 3][Step 20], time=0.036115169525146484, ext_time=0.011961221694946289, train_time=0.019288301467895508
[Epoch 3][Step 21], time=0.036684513092041016, ext_time=0.012227296829223633, train_time=0.01949334144592285
[Epoch 3][Step 22], time=0.03599190711975098, ext_time=0.012064456939697266, train_time=0.018984317779541016
[Epoch 3][Step 23], time=0.036495208740234375, ext_time=0.012273788452148438, train_time=0.019215106964111328
[Epoch 3][Step 24], time=0.036253929138183594, ext_time=0.011884927749633789, train_time=0.019519567489624023
[Epoch 3][Step 25], time=0.03679394721984863, ext_time=0.012381792068481445, train_time=0.019376039505004883
[Epoch 3][Step 26], time=0.03697943687438965, ext_time=0.011942148208618164, train_time=0.02013421058654785
[Epoch 3][Step 27], time=0.03618121147155762, ext_time=0.011955499649047852, train_time=0.019343137741088867
[Epoch 3][Step 28], time=0.03604555130004883, ext_time=0.011939048767089844, train_time=0.019295215606689453
[Epoch 3][Step 29], time=0.03780198097229004, ext_time=0.01216745376586914, train_time=0.020676612854003906
[Epoch 3][Step 30], time=0.03624916076660156, ext_time=0.011893510818481445, train_time=0.019552946090698242
[Epoch 3][Step 31], time=0.03699660301208496, ext_time=0.012276172637939453, train_time=0.019771575927734375
[Epoch 3][Step 32], time=0.036975860595703125, ext_time=0.012113809585571289, train_time=0.019941329956054688
[Epoch 3][Step 33], time=0.03611636161804199, ext_time=0.012164831161499023, train_time=0.018886566162109375
[Epoch 3][Step 34], time=0.03659653663635254, ext_time=0.011879682540893555, train_time=0.019858360290527344
[Epoch 3][Step 35], time=0.037339210510253906, ext_time=0.012280702590942383, train_time=0.020043373107910156
[Epoch 3][Step 36], time=0.03624844551086426, ext_time=0.012026786804199219, train_time=0.01926565170288086
[Epoch 3][Step 37], time=0.03693580627441406, ext_time=0.011928796768188477, train_time=0.020124197006225586
[Epoch 3][Step 38], time=0.03690600395202637, ext_time=0.012169599533081055, train_time=0.01977086067199707
[Epoch 3][Step 39], time=0.0368800163269043, ext_time=0.011884927749633789, train_time=0.02013230323791504
[Epoch 3][Step 40], time=0.03707575798034668, ext_time=0.01155996322631836, train_time=0.020729541778564453
[Epoch 3][Step 41], time=0.036797285079956055, ext_time=0.01219630241394043, train_time=0.019636869430541992
[Epoch 3][Step 42], time=0.03639030456542969, ext_time=0.012252330780029297, train_time=0.019118547439575195
[Epoch 3][Step 43], time=0.036039113998413086, ext_time=0.011942625045776367, train_time=0.01919102668762207
[Epoch 3][Step 44], time=0.03659844398498535, ext_time=0.011751413345336914, train_time=0.019984722137451172
[Epoch 3][Step 45], time=0.03625893592834473, ext_time=0.012253284454345703, train_time=0.019056320190429688
[Epoch 3][Step 46], time=0.03661704063415527, ext_time=0.012326240539550781, train_time=0.019281625747680664
[Epoch 3][Step 47], time=0.03730177879333496, ext_time=0.012048482894897461, train_time=0.02033686637878418
[Epoch 3][Step 48], time=0.03582501411437988, ext_time=0.011748313903808594, train_time=0.019275188446044922
[Epoch 3][Step 49], time=0.03639864921569824, ext_time=0.012033462524414062, train_time=0.019466638565063477
[Epoch 3][Step 50], time=0.036157846450805664, ext_time=0.012054681777954102, train_time=0.019131898880004883
[Epoch 3][Step 51], time=0.03710436820983887, ext_time=0.012537479400634766, train_time=0.01945209503173828
[Epoch 3][Step 52], time=0.036768436431884766, ext_time=0.011931896209716797, train_time=0.019922971725463867
[Epoch 3][Step 53], time=0.03656458854675293, ext_time=0.011756420135498047, train_time=0.019922733306884766
[Epoch 3][Step 54], time=0.0361025333404541, ext_time=0.01182866096496582, train_time=0.01941204071044922
[Epoch 3][Step 55], time=0.03658747673034668, ext_time=0.012089729309082031, train_time=0.019567489624023438
[Epoch 3][Step 56], time=0.03634142875671387, ext_time=0.011944293975830078, train_time=0.01949334144592285
[Epoch 3][Step 57], time=0.03711509704589844, ext_time=0.011781454086303711, train_time=0.020520925521850586
[Epoch 3][Step 58], time=0.036421775817871094, ext_time=0.012320995330810547, train_time=0.019089698791503906
[Epoch 3][Step 59], time=0.03726911544799805, ext_time=0.012115001678466797, train_time=0.02017664909362793
[Epoch 3][Step 60], time=0.03712964057922363, ext_time=0.011997699737548828, train_time=0.020256757736206055
[Epoch 3][Step 61], time=0.036109209060668945, ext_time=0.011983871459960938, train_time=0.019225597381591797
[Epoch 3][Step 62], time=0.03632545471191406, ext_time=0.012302637100219727, train_time=0.019028902053833008
[Epoch 3][Step 63], time=0.03616690635681152, ext_time=0.011825799942016602, train_time=0.019479751586914062
[Epoch 3][Step 64], time=0.03615713119506836, ext_time=0.012095212936401367, train_time=0.01909804344177246
[Epoch 3][Step 65], time=0.036315202713012695, ext_time=0.01197052001953125, train_time=0.01942586898803711
[Epoch 3][Step 66], time=0.036534786224365234, ext_time=0.011756420135498047, train_time=0.01991748809814453
[Epoch 3][Step 67], time=0.036629438400268555, ext_time=0.012315750122070312, train_time=0.01936173439025879
[Epoch 3][Step 68], time=0.037119150161743164, ext_time=0.012430191040039062, train_time=0.019656896591186523
[Epoch 3][Step 69], time=0.03565526008605957, ext_time=0.011635541915893555, train_time=0.019145965576171875
[Epoch 3][Step 70], time=0.03670859336853027, ext_time=0.011914253234863281, train_time=0.01988506317138672
[Epoch 3][Step 71], time=0.036672353744506836, ext_time=0.01224970817565918, train_time=0.01940178871154785
[Epoch 3][Step 72], time=0.03762006759643555, ext_time=0.01197052001953125, train_time=0.020711183547973633
[Epoch 3][Step 73], time=0.037288665771484375, ext_time=0.012240171432495117, train_time=0.02003955841064453
[Epoch 3][Step 74], time=0.036048173904418945, ext_time=0.011977434158325195, train_time=0.019151926040649414
[Epoch 3][Step 75], time=0.03716611862182617, ext_time=0.012391805648803711, train_time=0.019758224487304688
[Epoch 3][Step 76], time=0.03684544563293457, ext_time=0.01203465461730957, train_time=0.019880294799804688
[Epoch 3][Step 77], time=0.03689265251159668, ext_time=0.011999368667602539, train_time=0.019980907440185547
[Epoch 3][Step 78], time=0.035505056381225586, ext_time=0.01183938980102539, train_time=0.018834352493286133
[Epoch 3][Step 79], time=0.036364078521728516, ext_time=0.012149810791015625, train_time=0.0193173885345459
[Epoch 3][Step 80], time=0.036302804946899414, ext_time=0.011543989181518555, train_time=0.020026683807373047
[Epoch 3][Step 81], time=0.036035776138305664, ext_time=0.012022733688354492, train_time=0.01903533935546875
[Epoch 3][Step 82], time=0.03617429733276367, ext_time=0.012154817581176758, train_time=0.019103288650512695
[Epoch 3][Step 83], time=0.03644537925720215, ext_time=0.011879682540893555, train_time=0.019699573516845703
[Epoch 3][Step 84], time=0.037604570388793945, ext_time=0.012204885482788086, train_time=0.020448684692382812
[Epoch 3][Step 85], time=0.03711414337158203, ext_time=0.01200413703918457, train_time=0.020282268524169922
[Epoch 3][Step 86], time=0.0367741584777832, ext_time=0.012282133102416992, train_time=0.019433021545410156
[Epoch 3][Step 87], time=0.036641597747802734, ext_time=0.011788368225097656, train_time=0.01995682716369629
[Epoch 3][Step 88], time=0.03661942481994629, ext_time=0.012227773666381836, train_time=0.019427061080932617
[Epoch 3][Step 89], time=0.03647255897521973, ext_time=0.011690378189086914, train_time=0.02003788948059082
[Epoch 3][Step 90], time=0.036744117736816406, ext_time=0.012085676193237305, train_time=0.019722700119018555
[Epoch 3][Step 91], time=0.03655743598937988, ext_time=0.01208639144897461, train_time=0.019527673721313477
[Epoch 3][Step 92], time=0.03629922866821289, ext_time=0.011801958084106445, train_time=0.019687414169311523
[Epoch 3][Step 93], time=0.03637266159057617, ext_time=0.011983394622802734, train_time=0.01940608024597168
[Epoch 3][Step 94], time=0.03647136688232422, ext_time=0.012345314025878906, train_time=0.01909661293029785
[Epoch 3][Step 95], time=0.036665916442871094, ext_time=0.011791229248046875, train_time=0.020017147064208984
[Epoch 3][Step 96], time=0.037442922592163086, ext_time=0.011760473251342773, train_time=0.020808696746826172
[Epoch 3][Step 97], time=0.037793636322021484, ext_time=0.012162446975708008, train_time=0.020678043365478516
[Epoch 3][Step 98], time=0.036344289779663086, ext_time=0.012051820755004883, train_time=0.01943492889404297
[Epoch 3][Step 99], time=0.037342071533203125, ext_time=0.011905670166015625, train_time=0.020564794540405273
[Epoch 3][Step 100], time=0.03664708137512207, ext_time=0.011922359466552734, train_time=0.019841432571411133
[Epoch 3][Step 101], time=0.03618574142456055, ext_time=0.011866331100463867, train_time=0.01948857307434082
[Epoch 3][Step 102], time=0.036363840103149414, ext_time=0.01191568374633789, train_time=0.01956343650817871
[Epoch 3][Step 103], time=0.037552833557128906, ext_time=0.012613534927368164, train_time=0.019888877868652344
[Epoch 3][Step 104], time=0.03691220283508301, ext_time=0.011914968490600586, train_time=0.020112276077270508
[Epoch 3][Step 105], time=0.03668522834777832, ext_time=0.01236867904663086, train_time=0.019362926483154297
[Epoch 3][Step 106], time=0.03672218322753906, ext_time=0.01203608512878418, train_time=0.0197756290435791
[Epoch 3][Step 107], time=0.03679680824279785, ext_time=0.011914491653442383, train_time=0.01999068260192871
[Epoch 3][Step 108], time=0.0363469123840332, ext_time=0.011839151382446289, train_time=0.019614219665527344
[Epoch 3][Step 109], time=0.036447763442993164, ext_time=0.01221919059753418, train_time=0.019261837005615234
[Epoch 3][Step 110], time=0.03680014610290527, ext_time=0.012191295623779297, train_time=0.0196535587310791
[Epoch 3][Step 111], time=0.03704428672790527, ext_time=0.012311458587646484, train_time=0.019771099090576172
[Epoch 3][Step 112], time=0.0364840030670166, ext_time=0.011809110641479492, train_time=0.019779205322265625
[Epoch 3][Step 113], time=0.03704261779785156, ext_time=0.012078523635864258, train_time=0.020038604736328125
[Epoch 3][Step 114], time=0.038298606872558594, ext_time=0.011790037155151367, train_time=0.02164459228515625
[Epoch 3][Step 115], time=0.03622770309448242, ext_time=0.011811971664428711, train_time=0.019624710083007812
[Epoch 3][Step 116], time=0.03638434410095215, ext_time=0.011804580688476562, train_time=0.019701242446899414
[Epoch 3][Step 117], time=0.03670549392700195, ext_time=0.011687278747558594, train_time=0.020215749740600586
[Epoch 3][Step 118], time=0.03683805465698242, ext_time=0.012295722961425781, train_time=0.01954364776611328
[Epoch 3][Step 119], time=0.03721141815185547, ext_time=0.012043952941894531, train_time=0.020238876342773438
[Epoch 3][Step 120], time=0.036524295806884766, ext_time=0.011972904205322266, train_time=0.019632816314697266
[Epoch 3][Step 121], time=0.03622841835021973, ext_time=0.011647462844848633, train_time=0.019642353057861328
[Epoch 3][Step 122], time=0.036540985107421875, ext_time=0.011934518814086914, train_time=0.01976633071899414
[Epoch 3][Step 123], time=0.036721229553222656, ext_time=0.012008905410766602, train_time=0.019786834716796875
[Epoch 3][Step 124], time=0.0366978645324707, ext_time=0.012249469757080078, train_time=0.01945209503173828
[Epoch 3], time=4.587977170944214, loss=1.8749489784240723
    [Step(average) Profiler Level 1 E3 S499]
        L1  sample           0.004968 | send           0.000000
        L1  recv             0.000000 | copy           0.012059 | convert time 0.000000 | train  0.019710
        L1  feature nbytes  960.41 MB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes     133.27 MB | remote nbytes  620.39 MB
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S499]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.012059
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S499]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000673 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.011334 | cache combine cache 0.000592 | cache combine remote 0.005390
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S499]
        p50.00_tail_logl2featcopy=0.012053
        p90.00_tail_logl2featcopy=0.012329
        p95.00_tail_logl2featcopy=0.012431
        p99.00_tail_logl2featcopy=0.012604
        p99.90_tail_logl2featcopy=0.017485
[CUDA] cuda: usage: 13.53 GB
Rank=3, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.40035486221313477
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 1.0728836059570312e-05, transfer latency: 0.3806607723236084
presamping
presamping takes 7.444995880126953
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   67869 KB |    1696 MB |    2117 GB |    2117 GB |
|       from large pool |   55888 KB |    1686 MB |    2098 GB |    2098 GB |
|       from small pool |   11980 KB |      19 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Active memory         |   67869 KB |    1696 MB |    2117 GB |    2117 GB |
|       from large pool |   55888 KB |    1686 MB |    2098 GB |    2098 GB |
|       from small pool |   11980 KB |      19 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2328 MB |    2328 MB |    2328 MB |       0 B  |
|       from large pool |    2298 MB |    2298 MB |    2298 MB |       0 B  |
|       from small pool |      30 MB |      30 MB |      30 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  124643 KB |    1880 MB |    1841 GB |    1841 GB |
|       from large pool |  112047 KB |    1868 MB |    1822 GB |    1821 GB |
|       from small pool |   12595 KB |      17 MB |      19 GB |      19 GB |
|---------------------------------------------------------------------------|
| Allocations           |      66    |      95    |  136048    |  135982    |
|       from large pool |      12    |      23    |   44756    |   44744    |
|       from small pool |      54    |      73    |   91292    |   91238    |
|---------------------------------------------------------------------------|
| Active allocs         |      66    |      95    |  136048    |  135982    |
|       from large pool |      12    |      23    |   44756    |   44744    |
|       from small pool |      54    |      73    |   91292    |   91238    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      26    |      26    |      26    |       0    |
|       from large pool |      11    |      11    |      11    |       0    |
|       from small pool |      15    |      15    |      15    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      42    |      52    |   50488    |   50446    |
|       from large pool |       7    |      18    |   24814    |   24807    |
|       from small pool |      35    |      41    |   25674    |   25639    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 19.565424 seconds
[EPOCH_TIME] 4.891356 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 4.588132 seconds
Rank=2, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.3950846195220947
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 9.5367431640625e-06, transfer latency: 0.36443495750427246
presamping
presamping takes 7.913341045379639
Rank=1, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.42314791679382324
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 1.1444091796875e-05, transfer latency: 0.39022231101989746
presamping
presamping takes 8.078725576400757

