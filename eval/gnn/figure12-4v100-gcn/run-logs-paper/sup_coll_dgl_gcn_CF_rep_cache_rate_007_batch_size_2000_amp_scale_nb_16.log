succeed=True
[CUDA] cuda: usage: 5.32 GB
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
coll_cache:optimal_rep_storage=0.07
coll_cache:optimal_part_storage=0
coll_cache:optimal_cpu_storage=0.93
coll_cache:optimal_local_storage=0.07
coll_cache:optimal_remote_storage=0
coll_cache:optimal_local_rate=0.478931
coll_cache:optimal_remote_rate=0
coll_cache:optimal_cpu_rate=0.521069
z=25571.6
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=4769989632
config:eval_tsp="2023-08-06 19:19:26"
config:num_worker=4
config:num_intra_size=4
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=com-friendster
config:epochs=4
config:batchsize=2000
config:skip_epoch=2
config:local_step=250
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.07
config:cache_policy=rep
config:omp_thread_num=40
config:unsupervised=False
config:classnum=100
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7f98454a57c0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=4
Rank=0, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.4046330451965332
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 9.775161743164062e-06, transfer latency: 0.3889167308807373
epoch=4 total_steps=500
presamping
presamping takes 7.23775839805603
start training...
[Epoch 0][Step 0], time=1.1751835346221924, ext_time=0.04639887809753418, train_time=1.1215968132019043
[Epoch 0][Step 1], time=0.07110476493835449, ext_time=0.04259061813354492, train_time=0.022484302520751953
[Epoch 0][Step 2], time=0.07041025161743164, ext_time=0.04349207878112793, train_time=0.022008180618286133
[Epoch 0][Step 3], time=0.06831192970275879, ext_time=0.043357133865356445, train_time=0.02004528045654297
[Epoch 0][Step 4], time=0.07011699676513672, ext_time=0.044484615325927734, train_time=0.020633935928344727
[Epoch 0][Step 5], time=0.06885862350463867, ext_time=0.043355703353881836, train_time=0.020564556121826172
[Epoch 0][Step 6], time=0.06899380683898926, ext_time=0.04342842102050781, train_time=0.020671367645263672
[Epoch 0][Step 7], time=0.06979179382324219, ext_time=0.043904781341552734, train_time=0.02096080780029297
[Epoch 0][Step 8], time=0.06911182403564453, ext_time=0.043949127197265625, train_time=0.020224809646606445
[Epoch 0][Step 9], time=0.06809878349304199, ext_time=0.04237222671508789, train_time=0.020911455154418945
[Epoch 0][Step 10], time=0.0680692195892334, ext_time=0.043775081634521484, train_time=0.019365310668945312
[Epoch 0][Step 11], time=0.06971216201782227, ext_time=0.04396557807922363, train_time=0.02079153060913086
[Epoch 0][Step 12], time=0.06734466552734375, ext_time=0.042545318603515625, train_time=0.01998591423034668
[Epoch 0][Step 13], time=0.0692300796508789, ext_time=0.04381084442138672, train_time=0.02046799659729004
[Epoch 0][Step 14], time=0.07172179222106934, ext_time=0.0435335636138916, train_time=0.023287057876586914
[Epoch 0][Step 15], time=0.06935286521911621, ext_time=0.04322981834411621, train_time=0.02121901512145996
[Epoch 0][Step 16], time=0.06939077377319336, ext_time=0.043808937072753906, train_time=0.020619630813598633
[Epoch 0][Step 17], time=0.06993436813354492, ext_time=0.04198431968688965, train_time=0.023192405700683594
[Epoch 0][Step 18], time=0.06983566284179688, ext_time=0.04474163055419922, train_time=0.019922733306884766
[Epoch 0][Step 19], time=0.0684363842010498, ext_time=0.04383134841918945, train_time=0.01966118812561035
[Epoch 0][Step 20], time=0.06932640075683594, ext_time=0.04449152946472168, train_time=0.01984548568725586
[Epoch 0][Step 21], time=0.07150053977966309, ext_time=0.04416036605834961, train_time=0.022375822067260742
[Epoch 0][Step 22], time=0.06975173950195312, ext_time=0.044518470764160156, train_time=0.020224332809448242
[Epoch 0][Step 23], time=0.06755518913269043, ext_time=0.043225765228271484, train_time=0.01947474479675293
[Epoch 0][Step 24], time=0.07048511505126953, ext_time=0.043036699295043945, train_time=0.022490739822387695
[Epoch 0][Step 25], time=0.06893658638000488, ext_time=0.043560028076171875, train_time=0.020467519760131836
[Epoch 0][Step 26], time=0.06920528411865234, ext_time=0.044560909271240234, train_time=0.01956963539123535
[Epoch 0][Step 27], time=0.06992745399475098, ext_time=0.04442119598388672, train_time=0.020470380783081055
[Epoch 0][Step 28], time=0.06810712814331055, ext_time=0.043074607849121094, train_time=0.020148277282714844
[Epoch 0][Step 29], time=0.06861567497253418, ext_time=0.04315304756164551, train_time=0.02058553695678711
[Epoch 0][Step 30], time=0.06886124610900879, ext_time=0.04374980926513672, train_time=0.02003192901611328
[Epoch 0][Step 31], time=0.06899404525756836, ext_time=0.042764902114868164, train_time=0.021339893341064453
[Epoch 0][Step 32], time=0.06879734992980957, ext_time=0.04322648048400879, train_time=0.020672082901000977
[Epoch 0][Step 33], time=0.06939554214477539, ext_time=0.04450678825378418, train_time=0.019873857498168945
[Epoch 0][Step 34], time=0.06861042976379395, ext_time=0.04365420341491699, train_time=0.020061731338500977
[Epoch 0][Step 35], time=0.06940793991088867, ext_time=0.04296112060546875, train_time=0.021494626998901367
[Epoch 0][Step 36], time=0.06884479522705078, ext_time=0.04374575614929199, train_time=0.02010178565979004
[Epoch 0][Step 37], time=0.06969547271728516, ext_time=0.042406320571899414, train_time=0.02240300178527832
[Epoch 0][Step 38], time=0.06807184219360352, ext_time=0.04340672492980957, train_time=0.01975417137145996
[Epoch 0][Step 39], time=0.06895017623901367, ext_time=0.042905569076538086, train_time=0.021193981170654297
[Epoch 0][Step 40], time=0.06911802291870117, ext_time=0.04325461387634277, train_time=0.02097344398498535
[Epoch 0][Step 41], time=0.0692451000213623, ext_time=0.04365682601928711, train_time=0.020665645599365234
[Epoch 0][Step 42], time=0.0695641040802002, ext_time=0.04203081130981445, train_time=0.02257370948791504
[Epoch 0][Step 43], time=0.07146048545837402, ext_time=0.044551849365234375, train_time=0.02184748649597168
[Epoch 0][Step 44], time=0.0690615177154541, ext_time=0.04292798042297363, train_time=0.02126169204711914
[Epoch 0][Step 45], time=0.0702970027923584, ext_time=0.04391121864318848, train_time=0.021404743194580078
[Epoch 0][Step 46], time=0.06880879402160645, ext_time=0.043798208236694336, train_time=0.020087718963623047
[Epoch 0][Step 47], time=0.06921029090881348, ext_time=0.0421144962310791, train_time=0.022174358367919922
[Epoch 0][Step 48], time=0.06874442100524902, ext_time=0.04393911361694336, train_time=0.019861936569213867
[Epoch 0][Step 49], time=0.06881093978881836, ext_time=0.04376673698425293, train_time=0.020141124725341797
[Epoch 0][Step 50], time=0.06891965866088867, ext_time=0.043830156326293945, train_time=0.020151376724243164
[Epoch 0][Step 51], time=0.06877779960632324, ext_time=0.04392600059509277, train_time=0.019910097122192383
[Epoch 0][Step 52], time=0.06982207298278809, ext_time=0.04479718208312988, train_time=0.02001786231994629
[Epoch 0][Step 53], time=0.06879615783691406, ext_time=0.04340958595275879, train_time=0.020473718643188477
[Epoch 0][Step 54], time=0.06876492500305176, ext_time=0.04347991943359375, train_time=0.020355939865112305
[Epoch 0][Step 55], time=0.0687704086303711, ext_time=0.04409384727478027, train_time=0.019724369049072266
[Epoch 0][Step 56], time=0.07017302513122559, ext_time=0.04411125183105469, train_time=0.021060705184936523
[Epoch 0][Step 57], time=0.06845474243164062, ext_time=0.04314255714416504, train_time=0.020436763763427734
[Epoch 0][Step 58], time=0.07167720794677734, ext_time=0.04345703125, train_time=0.02326679229736328
[Epoch 0][Step 59], time=0.06987452507019043, ext_time=0.04340720176696777, train_time=0.02156853675842285
[Epoch 0][Step 60], time=0.07022261619567871, ext_time=0.04433035850524902, train_time=0.020936250686645508
[Epoch 0][Step 61], time=0.06856679916381836, ext_time=0.04379725456237793, train_time=0.01966404914855957
[Epoch 0][Step 62], time=0.06990694999694824, ext_time=0.044228553771972656, train_time=0.02069401741027832
[Epoch 0][Step 63], time=0.06900191307067871, ext_time=0.04431319236755371, train_time=0.019591569900512695
[Epoch 0][Step 64], time=0.0707707405090332, ext_time=0.043756961822509766, train_time=0.02165961265563965
[Epoch 0][Step 65], time=0.06943392753601074, ext_time=0.043890953063964844, train_time=0.020615339279174805
[Epoch 0][Step 66], time=0.06915402412414551, ext_time=0.04365944862365723, train_time=0.020490169525146484
[Epoch 0][Step 67], time=0.06844472885131836, ext_time=0.043790340423583984, train_time=0.01968860626220703
[Epoch 0][Step 68], time=0.06903982162475586, ext_time=0.04396986961364746, train_time=0.020107269287109375
[Epoch 0][Step 69], time=0.07012081146240234, ext_time=0.04304671287536621, train_time=0.022189855575561523
[Epoch 0][Step 70], time=0.07168745994567871, ext_time=0.04512834548950195, train_time=0.021484851837158203
[Epoch 0][Step 71], time=0.06863212585449219, ext_time=0.04399704933166504, train_time=0.01967167854309082
[Epoch 0][Step 72], time=0.06783485412597656, ext_time=0.04351353645324707, train_time=0.01938152313232422
[Epoch 0][Step 73], time=0.06986498832702637, ext_time=0.04317188262939453, train_time=0.021813154220581055
[Epoch 0][Step 74], time=0.06893801689147949, ext_time=0.04356050491333008, train_time=0.02041792869567871
[Epoch 0][Step 75], time=0.06869626045227051, ext_time=0.04290938377380371, train_time=0.020918846130371094
[Epoch 0][Step 76], time=0.06911540031433105, ext_time=0.04299044609069824, train_time=0.021254777908325195
[Epoch 0][Step 77], time=0.06970453262329102, ext_time=0.04374814033508301, train_time=0.021032094955444336
[Epoch 0][Step 78], time=0.06983423233032227, ext_time=0.0447850227355957, train_time=0.019987821578979492
[Epoch 0][Step 79], time=0.06962704658508301, ext_time=0.04413127899169922, train_time=0.020528793334960938
[Epoch 0][Step 80], time=0.0690145492553711, ext_time=0.04285240173339844, train_time=0.02131485939025879
[Epoch 0][Step 81], time=0.06987357139587402, ext_time=0.044731855392456055, train_time=0.020050048828125
[Epoch 0][Step 82], time=0.0683598518371582, ext_time=0.04367351531982422, train_time=0.019741058349609375
[Epoch 0][Step 83], time=0.06944727897644043, ext_time=0.043500423431396484, train_time=0.021054506301879883
[Epoch 0][Step 84], time=0.07006978988647461, ext_time=0.045020341873168945, train_time=0.01990818977355957
[Epoch 0][Step 85], time=0.06826233863830566, ext_time=0.04340767860412598, train_time=0.01994466781616211
[Epoch 0][Step 86], time=0.06919264793395996, ext_time=0.04319477081298828, train_time=0.02110886573791504
[Epoch 0][Step 87], time=0.06809163093566895, ext_time=0.04330921173095703, train_time=0.019865036010742188
[Epoch 0][Step 88], time=0.06873202323913574, ext_time=0.04412841796875, train_time=0.01961970329284668
[Epoch 0][Step 89], time=0.06877255439758301, ext_time=0.044103145599365234, train_time=0.019669771194458008
[Epoch 0][Step 90], time=0.06962323188781738, ext_time=0.04485940933227539, train_time=0.01974964141845703
[Epoch 0][Step 91], time=0.06987476348876953, ext_time=0.04467511177062988, train_time=0.020210981369018555
[Epoch 0][Step 92], time=0.06877970695495605, ext_time=0.04260873794555664, train_time=0.021340608596801758
[Epoch 0][Step 93], time=0.06968903541564941, ext_time=0.044708251953125, train_time=0.019975900650024414
[Epoch 0][Step 94], time=0.06916260719299316, ext_time=0.04364323616027832, train_time=0.02057671546936035
[Epoch 0][Step 95], time=0.06869983673095703, ext_time=0.04406285285949707, train_time=0.0196990966796875
[Epoch 0][Step 96], time=0.0691976547241211, ext_time=0.04233360290527344, train_time=0.022052288055419922
[Epoch 0][Step 97], time=0.06847167015075684, ext_time=0.042864322662353516, train_time=0.020798206329345703
[Epoch 0][Step 98], time=0.06940150260925293, ext_time=0.04424571990966797, train_time=0.020239591598510742
[Epoch 0][Step 99], time=0.06908369064331055, ext_time=0.04341387748718262, train_time=0.020824432373046875
[Epoch 0][Step 100], time=0.0693819522857666, ext_time=0.04458355903625488, train_time=0.019698143005371094
[Epoch 0][Step 101], time=0.0687568187713623, ext_time=0.04334211349487305, train_time=0.020552396774291992
[Epoch 0][Step 102], time=0.06961774826049805, ext_time=0.043251991271972656, train_time=0.02146172523498535
[Epoch 0][Step 103], time=0.06958389282226562, ext_time=0.04349207878112793, train_time=0.021185636520385742
[Epoch 0][Step 104], time=0.06833767890930176, ext_time=0.04392743110656738, train_time=0.019481658935546875
[Epoch 0][Step 105], time=0.06909584999084473, ext_time=0.04366755485534668, train_time=0.020526409149169922
[Epoch 0][Step 106], time=0.07165956497192383, ext_time=0.04510784149169922, train_time=0.021456003189086914
[Epoch 0][Step 107], time=0.06969690322875977, ext_time=0.04458141326904297, train_time=0.020110368728637695
[Epoch 0][Step 108], time=0.06771445274353027, ext_time=0.043096303939819336, train_time=0.019753694534301758
[Epoch 0][Step 109], time=0.0693967342376709, ext_time=0.04457283020019531, train_time=0.0198061466217041
[Epoch 0][Step 110], time=0.06947660446166992, ext_time=0.04209303855895996, train_time=0.022614240646362305
[Epoch 0][Step 111], time=0.07021093368530273, ext_time=0.044579505920410156, train_time=0.020669221878051758
[Epoch 0][Step 112], time=0.06845808029174805, ext_time=0.04394388198852539, train_time=0.0194242000579834
[Epoch 0][Step 113], time=0.06932187080383301, ext_time=0.042801856994628906, train_time=0.02165055274963379
[Epoch 0][Step 114], time=0.0682837963104248, ext_time=0.0437469482421875, train_time=0.0196378231048584
[Epoch 0][Step 115], time=0.06814956665039062, ext_time=0.04385805130004883, train_time=0.01938009262084961
[Epoch 0][Step 116], time=0.1265239715576172, ext_time=0.04580569267272949, train_time=0.07470560073852539
[Epoch 0][Step 117], time=0.06892585754394531, ext_time=0.04363870620727539, train_time=0.01995563507080078
[Epoch 0][Step 118], time=0.0690314769744873, ext_time=0.044297218322753906, train_time=0.019733428955078125
[Epoch 0][Step 119], time=0.06907081604003906, ext_time=0.043894052505493164, train_time=0.02029561996459961
[Epoch 0][Step 120], time=0.0691065788269043, ext_time=0.04342794418334961, train_time=0.02076101303100586
[Epoch 0][Step 121], time=0.06963062286376953, ext_time=0.04380989074707031, train_time=0.020816802978515625
[Epoch 0][Step 122], time=0.07241368293762207, ext_time=0.04521346092224121, train_time=0.022143125534057617
[Epoch 0][Step 123], time=0.06913995742797852, ext_time=0.04374074935913086, train_time=0.020458221435546875
[Epoch 0][Step 124], time=0.06966471672058105, ext_time=0.04375886917114258, train_time=0.020961761474609375
[Epoch 0], time=9.834866285324097, loss=3.874769449234009
[Epoch 1][Step 0], time=0.07192254066467285, ext_time=0.04496884346008301, train_time=0.02191448211669922
[Epoch 1][Step 1], time=0.06930923461914062, ext_time=0.042456865310668945, train_time=0.022040128707885742
[Epoch 1][Step 2], time=0.06980037689208984, ext_time=0.04350709915161133, train_time=0.02140522003173828
[Epoch 1][Step 3], time=0.0687253475189209, ext_time=0.04334282875061035, train_time=0.020523786544799805
[Epoch 1][Step 4], time=0.07029604911804199, ext_time=0.04454779624938965, train_time=0.020696401596069336
[Epoch 1][Step 5], time=0.06859898567199707, ext_time=0.04355287551879883, train_time=0.020153522491455078
[Epoch 1][Step 6], time=0.06877350807189941, ext_time=0.04342842102050781, train_time=0.020439624786376953
[Epoch 1][Step 7], time=0.06977629661560059, ext_time=0.04399585723876953, train_time=0.020816802978515625
[Epoch 1][Step 8], time=0.06854701042175293, ext_time=0.043988943099975586, train_time=0.019594669342041016
[Epoch 1][Step 9], time=0.06827068328857422, ext_time=0.04247236251831055, train_time=0.02097296714782715
[Epoch 1][Step 10], time=0.06823420524597168, ext_time=0.04375934600830078, train_time=0.019543170928955078
[Epoch 1][Step 11], time=0.06945514678955078, ext_time=0.04396176338195801, train_time=0.020543575286865234
[Epoch 1][Step 12], time=0.06776189804077148, ext_time=0.04263710975646973, train_time=0.020328283309936523
[Epoch 1][Step 13], time=0.0682528018951416, ext_time=0.04370737075805664, train_time=0.019571781158447266
[Epoch 1][Step 14], time=0.07107973098754883, ext_time=0.0436403751373291, train_time=0.022546768188476562
[Epoch 1][Step 15], time=0.06910991668701172, ext_time=0.043092966079711914, train_time=0.02111983299255371
[Epoch 1][Step 16], time=0.0686032772064209, ext_time=0.04369688034057617, train_time=0.0199282169342041
[Epoch 1][Step 17], time=0.06975245475769043, ext_time=0.04197835922241211, train_time=0.02301645278930664
[Epoch 1][Step 18], time=0.06923127174377441, ext_time=0.04459428787231445, train_time=0.019649505615234375
[Epoch 1][Step 19], time=0.06921935081481934, ext_time=0.04381251335144043, train_time=0.0198974609375
[Epoch 1][Step 20], time=0.06973791122436523, ext_time=0.04458451271057129, train_time=0.020102739334106445
[Epoch 1][Step 21], time=0.07030344009399414, ext_time=0.04416346549987793, train_time=0.02117443084716797
[Epoch 1][Step 22], time=0.06934738159179688, ext_time=0.04449653625488281, train_time=0.019811391830444336
[Epoch 1][Step 23], time=0.06814432144165039, ext_time=0.043396711349487305, train_time=0.019824504852294922
[Epoch 1][Step 24], time=0.06910157203674316, ext_time=0.043016672134399414, train_time=0.02119135856628418
[Epoch 1][Step 25], time=0.0688638687133789, ext_time=0.043531179428100586, train_time=0.020399093627929688
[Epoch 1][Step 26], time=0.0699162483215332, ext_time=0.0446927547454834, train_time=0.02019190788269043
[Epoch 1][Step 27], time=0.06933975219726562, ext_time=0.04447627067565918, train_time=0.01985931396484375
[Epoch 1][Step 28], time=0.06808352470397949, ext_time=0.043140411376953125, train_time=0.02006244659423828
[Epoch 1][Step 29], time=0.06866598129272461, ext_time=0.04333162307739258, train_time=0.02045726776123047
[Epoch 1][Step 30], time=0.0685122013092041, ext_time=0.043869733810424805, train_time=0.0195767879486084
[Epoch 1][Step 31], time=0.06887102127075195, ext_time=0.04269576072692871, train_time=0.021374225616455078
[Epoch 1][Step 32], time=0.06879568099975586, ext_time=0.04309964179992676, train_time=0.020825624465942383
[Epoch 1][Step 33], time=0.06906700134277344, ext_time=0.04454374313354492, train_time=0.01949000358581543
[Epoch 1][Step 34], time=0.0687248706817627, ext_time=0.043602705001831055, train_time=0.020264625549316406
[Epoch 1][Step 35], time=0.06934952735900879, ext_time=0.04301190376281738, train_time=0.021413326263427734
[Epoch 1][Step 36], time=0.06926608085632324, ext_time=0.04370450973510742, train_time=0.020627737045288086
[Epoch 1][Step 37], time=0.06925129890441895, ext_time=0.04253339767456055, train_time=0.021819591522216797
[Epoch 1][Step 38], time=0.0679776668548584, ext_time=0.0433804988861084, train_time=0.0196988582611084
[Epoch 1][Step 39], time=0.06994414329528809, ext_time=0.043005943298339844, train_time=0.022032976150512695
[Epoch 1][Step 40], time=0.06904196739196777, ext_time=0.0432889461517334, train_time=0.02083587646484375
[Epoch 1][Step 41], time=0.06872367858886719, ext_time=0.04352879524230957, train_time=0.020287513732910156
[Epoch 1][Step 42], time=0.06956315040588379, ext_time=0.041985511779785156, train_time=0.022810697555541992
[Epoch 1][Step 43], time=0.0696253776550293, ext_time=0.04477429389953613, train_time=0.01985931396484375
[Epoch 1][Step 44], time=0.06855583190917969, ext_time=0.04308819770812988, train_time=0.020615100860595703
[Epoch 1][Step 45], time=0.0703284740447998, ext_time=0.043802499771118164, train_time=0.021570682525634766
[Epoch 1][Step 46], time=0.06898140907287598, ext_time=0.0438532829284668, train_time=0.020204544067382812
[Epoch 1][Step 47], time=0.06928253173828125, ext_time=0.04221343994140625, train_time=0.022200345993041992
[Epoch 1][Step 48], time=0.06884026527404785, ext_time=0.04411149024963379, train_time=0.01975107192993164
[Epoch 1][Step 49], time=0.06905794143676758, ext_time=0.04364013671875, train_time=0.020531415939331055
[Epoch 1][Step 50], time=0.06867480278015137, ext_time=0.043745994567871094, train_time=0.020006656646728516
[Epoch 1][Step 51], time=0.06888937950134277, ext_time=0.04388236999511719, train_time=0.02009439468383789
[Epoch 1][Step 52], time=0.07002401351928711, ext_time=0.04472160339355469, train_time=0.0202333927154541
[Epoch 1][Step 53], time=0.06882309913635254, ext_time=0.04330921173095703, train_time=0.020560741424560547
[Epoch 1][Step 54], time=0.06888270378112793, ext_time=0.043433427810668945, train_time=0.020472049713134766
[Epoch 1][Step 55], time=0.06860041618347168, ext_time=0.04417705535888672, train_time=0.019429683685302734
[Epoch 1][Step 56], time=0.07048583030700684, ext_time=0.04404306411743164, train_time=0.021474599838256836
[Epoch 1][Step 57], time=0.06814098358154297, ext_time=0.04326510429382324, train_time=0.020006179809570312
[Epoch 1][Step 58], time=0.14043545722961426, ext_time=0.04347848892211914, train_time=0.09201622009277344
[Epoch 1][Step 59], time=0.06963181495666504, ext_time=0.04334735870361328, train_time=0.02141094207763672
[Epoch 1][Step 60], time=0.06983447074890137, ext_time=0.04380035400390625, train_time=0.021099090576171875
[Epoch 1][Step 61], time=0.06865358352661133, ext_time=0.04385805130004883, train_time=0.019855260848999023
[Epoch 1][Step 62], time=0.07149696350097656, ext_time=0.044486045837402344, train_time=0.022040843963623047
[Epoch 1][Step 63], time=0.06943750381469727, ext_time=0.04442763328552246, train_time=0.019881725311279297
[Epoch 1][Step 64], time=0.07030487060546875, ext_time=0.0438535213470459, train_time=0.021463632583618164
[Epoch 1][Step 65], time=0.0694131851196289, ext_time=0.04397010803222656, train_time=0.020436525344848633
[Epoch 1][Step 66], time=0.0691518783569336, ext_time=0.043729543685913086, train_time=0.020461082458496094
[Epoch 1][Step 67], time=0.06837248802185059, ext_time=0.04378032684326172, train_time=0.019645214080810547
[Epoch 1][Step 68], time=0.06911873817443848, ext_time=0.044029951095581055, train_time=0.02015519142150879
[Epoch 1][Step 69], time=0.0700228214263916, ext_time=0.042978525161743164, train_time=0.02117156982421875
[Epoch 1][Step 70], time=0.07002615928649902, ext_time=0.04500007629394531, train_time=0.01995086669921875
[Epoch 1][Step 71], time=0.06862568855285645, ext_time=0.04393744468688965, train_time=0.019730806350708008
[Epoch 1][Step 72], time=0.06812119483947754, ext_time=0.04360771179199219, train_time=0.01958608627319336
[Epoch 1][Step 73], time=0.06987500190734863, ext_time=0.04316210746765137, train_time=0.021858692169189453
[Epoch 1][Step 74], time=0.06927871704101562, ext_time=0.04342174530029297, train_time=0.020946979522705078
[Epoch 1][Step 75], time=0.06914544105529785, ext_time=0.04311823844909668, train_time=0.02117919921875
[Epoch 1][Step 76], time=0.06910109519958496, ext_time=0.04288172721862793, train_time=0.02131962776184082
[Epoch 1][Step 77], time=0.0699310302734375, ext_time=0.04373788833618164, train_time=0.021271944046020508
[Epoch 1][Step 78], time=0.0697014331817627, ext_time=0.044568777084350586, train_time=0.02011561393737793
[Epoch 1][Step 79], time=0.069427490234375, ext_time=0.044152259826660156, train_time=0.0203399658203125
[Epoch 1][Step 80], time=0.06851387023925781, ext_time=0.04285550117492676, train_time=0.020836830139160156
[Epoch 1][Step 81], time=0.06970739364624023, ext_time=0.044826507568359375, train_time=0.019848108291625977
[Epoch 1][Step 82], time=0.0683143138885498, ext_time=0.04356193542480469, train_time=0.019855499267578125
[Epoch 1][Step 83], time=0.06800031661987305, ext_time=0.0435633659362793, train_time=0.019501924514770508
[Epoch 1][Step 84], time=0.06964802742004395, ext_time=0.04500603675842285, train_time=0.01964116096496582
[Epoch 1][Step 85], time=0.06908178329467773, ext_time=0.043469905853271484, train_time=0.02070927619934082
[Epoch 1][Step 86], time=0.06932592391967773, ext_time=0.04312014579772949, train_time=0.02128434181213379
[Epoch 1][Step 87], time=0.06826591491699219, ext_time=0.043186187744140625, train_time=0.020191431045532227
[Epoch 1][Step 88], time=0.06929969787597656, ext_time=0.04423689842224121, train_time=0.020090579986572266
[Epoch 1][Step 89], time=0.06860494613647461, ext_time=0.04403114318847656, train_time=0.019502639770507812
[Epoch 1][Step 90], time=0.06967043876647949, ext_time=0.04493999481201172, train_time=0.019702672958374023
[Epoch 1][Step 91], time=0.06931400299072266, ext_time=0.044664859771728516, train_time=0.019640684127807617
[Epoch 1][Step 92], time=0.06848287582397461, ext_time=0.042574167251586914, train_time=0.021063566207885742
[Epoch 1][Step 93], time=0.0693047046661377, ext_time=0.044656991958618164, train_time=0.019655227661132812
[Epoch 1][Step 94], time=0.06949996948242188, ext_time=0.04356074333190918, train_time=0.02105236053466797
[Epoch 1][Step 95], time=0.06851792335510254, ext_time=0.0440061092376709, train_time=0.019588947296142578
[Epoch 1][Step 96], time=0.06916189193725586, ext_time=0.04234719276428223, train_time=0.021984338760375977
[Epoch 1][Step 97], time=0.06857800483703613, ext_time=0.042711734771728516, train_time=0.021050691604614258
[Epoch 1][Step 98], time=0.06881093978881836, ext_time=0.044194698333740234, train_time=0.01967620849609375
[Epoch 1][Step 99], time=0.06902956962585449, ext_time=0.04337930679321289, train_time=0.020791053771972656
[Epoch 1][Step 100], time=0.06909537315368652, ext_time=0.044481515884399414, train_time=0.019658565521240234
[Epoch 1][Step 101], time=0.06885361671447754, ext_time=0.04342532157897949, train_time=0.02053689956665039
[Epoch 1][Step 102], time=0.06936264038085938, ext_time=0.04341936111450195, train_time=0.020989656448364258
[Epoch 1][Step 103], time=0.06906509399414062, ext_time=0.04354047775268555, train_time=0.020519495010375977
[Epoch 1][Step 104], time=0.06839489936828613, ext_time=0.044098615646362305, train_time=0.019384384155273438
[Epoch 1][Step 105], time=0.06906366348266602, ext_time=0.04386091232299805, train_time=0.020274877548217773
[Epoch 1][Step 106], time=0.07025289535522461, ext_time=0.045181989669799805, train_time=0.02004075050354004
[Epoch 1][Step 107], time=0.06943392753601074, ext_time=0.044641733169555664, train_time=0.019756793975830078
[Epoch 1][Step 108], time=0.06734585762023926, ext_time=0.04300260543823242, train_time=0.019484519958496094
[Epoch 1][Step 109], time=0.06939029693603516, ext_time=0.04458212852478027, train_time=0.019804000854492188
[Epoch 1][Step 110], time=0.06929874420166016, ext_time=0.04207205772399902, train_time=0.022459745407104492
[Epoch 1][Step 111], time=0.06881856918334961, ext_time=0.044214487075805664, train_time=0.019619464874267578
[Epoch 1][Step 112], time=0.06834578514099121, ext_time=0.043831825256347656, train_time=0.019588470458984375
[Epoch 1][Step 113], time=0.06911206245422363, ext_time=0.04289507865905762, train_time=0.021353721618652344
[Epoch 1][Step 114], time=0.06837916374206543, ext_time=0.04361319541931152, train_time=0.019890785217285156
[Epoch 1][Step 115], time=0.06787705421447754, ext_time=0.04396486282348633, train_time=0.019002199172973633
[Epoch 1][Step 116], time=0.07411670684814453, ext_time=0.04610252380371094, train_time=0.022885799407958984
[Epoch 1][Step 117], time=0.06849360466003418, ext_time=0.04369831085205078, train_time=0.019720077514648438
[Epoch 1][Step 118], time=0.0690910816192627, ext_time=0.0443112850189209, train_time=0.019790172576904297
[Epoch 1][Step 119], time=0.0691843032836914, ext_time=0.04405927658081055, train_time=0.0202181339263916
[Epoch 1][Step 120], time=0.06886672973632812, ext_time=0.04345059394836426, train_time=0.020532846450805664
[Epoch 1][Step 121], time=0.0692746639251709, ext_time=0.04392528533935547, train_time=0.02038264274597168
[Epoch 1][Step 122], time=0.07008790969848633, ext_time=0.0451200008392334, train_time=0.019919157028198242
[Epoch 1][Step 123], time=0.06941437721252441, ext_time=0.043581247329711914, train_time=0.020954370498657227
[Epoch 1][Step 124], time=0.06974315643310547, ext_time=0.04388070106506348, train_time=0.020936012268066406
[Epoch 1], time=8.728489398956299, loss=3.159688949584961
[Epoch 2][Step 0], time=0.07182574272155762, ext_time=0.04501008987426758, train_time=0.021689653396606445
[Epoch 2][Step 1], time=0.06941652297973633, ext_time=0.04251575469970703, train_time=0.02207779884338379
[Epoch 2][Step 2], time=0.06985664367675781, ext_time=0.043474674224853516, train_time=0.021483421325683594
[Epoch 2][Step 3], time=0.0685124397277832, ext_time=0.04338645935058594, train_time=0.0202639102935791
[Epoch 2][Step 4], time=0.0700688362121582, ext_time=0.04460263252258301, train_time=0.02046346664428711
[Epoch 2][Step 5], time=0.0684206485748291, ext_time=0.04337430000305176, train_time=0.020117759704589844
[Epoch 2][Step 6], time=0.06860804557800293, ext_time=0.04340505599975586, train_time=0.020328283309936523
[Epoch 2][Step 7], time=0.06979823112487793, ext_time=0.04402446746826172, train_time=0.020834922790527344
[Epoch 2][Step 8], time=0.06882238388061523, ext_time=0.04395699501037598, train_time=0.019899845123291016
[Epoch 2][Step 9], time=0.06825709342956543, ext_time=0.042398929595947266, train_time=0.02106952667236328
[Epoch 2][Step 10], time=0.06842041015625, ext_time=0.04374527931213379, train_time=0.019758224487304688
[Epoch 2][Step 11], time=0.06952667236328125, ext_time=0.04396820068359375, train_time=0.020570993423461914
[Epoch 2][Step 12], time=0.0674126148223877, ext_time=0.04265093803405762, train_time=0.019994497299194336
[Epoch 2][Step 13], time=0.06812667846679688, ext_time=0.043587446212768555, train_time=0.019609689712524414
[Epoch 2][Step 14], time=0.06909751892089844, ext_time=0.04358530044555664, train_time=0.02064037322998047
[Epoch 2][Step 15], time=0.06916451454162598, ext_time=0.0431821346282959, train_time=0.021094560623168945
[Epoch 2][Step 16], time=0.06909918785095215, ext_time=0.04392862319946289, train_time=0.020244836807250977
[Epoch 2][Step 17], time=0.06952238082885742, ext_time=0.042009592056274414, train_time=0.022734642028808594
[Epoch 2][Step 18], time=0.0711524486541748, ext_time=0.044606685638427734, train_time=0.021549463272094727
[Epoch 2][Step 19], time=0.06837844848632812, ext_time=0.04387617111206055, train_time=0.019562244415283203
[Epoch 2][Step 20], time=0.06927919387817383, ext_time=0.044501543045043945, train_time=0.019761085510253906
[Epoch 2][Step 21], time=0.07008004188537598, ext_time=0.044147491455078125, train_time=0.020940780639648438
[Epoch 2][Step 22], time=0.06925535202026367, ext_time=0.04454994201660156, train_time=0.01969122886657715
[Epoch 2][Step 23], time=0.06734919548034668, ext_time=0.04315590858459473, train_time=0.01934218406677246
[Epoch 2][Step 24], time=0.06922411918640137, ext_time=0.04304146766662598, train_time=0.021239519119262695
[Epoch 2][Step 25], time=0.0687246322631836, ext_time=0.043593406677246094, train_time=0.020233869552612305
[Epoch 2][Step 26], time=0.06989455223083496, ext_time=0.044724225997924805, train_time=0.020158767700195312
[Epoch 2][Step 27], time=0.06914830207824707, ext_time=0.04441428184509277, train_time=0.019727706909179688
[Epoch 2][Step 28], time=0.06814289093017578, ext_time=0.0430912971496582, train_time=0.020159482955932617
[Epoch 2][Step 29], time=0.06837344169616699, ext_time=0.04306793212890625, train_time=0.020435333251953125
[Epoch 2][Step 30], time=0.06927967071533203, ext_time=0.04376411437988281, train_time=0.020566701889038086
[Epoch 2][Step 31], time=0.06905579566955566, ext_time=0.042700767517089844, train_time=0.02151632308959961
[Epoch 2][Step 32], time=0.06884503364562988, ext_time=0.043306589126586914, train_time=0.020674943923950195
[Epoch 2][Step 33], time=0.06938672065734863, ext_time=0.04448986053466797, train_time=0.01987004280090332
[Epoch 2][Step 34], time=0.06825685501098633, ext_time=0.04355168342590332, train_time=0.019815921783447266
[Epoch 2][Step 35], time=0.06891298294067383, ext_time=0.04290270805358887, train_time=0.021158456802368164
[Epoch 2][Step 36], time=0.06889033317565918, ext_time=0.04371523857116699, train_time=0.020279884338378906
[Epoch 2][Step 37], time=0.06957077980041504, ext_time=0.04248857498168945, train_time=0.022246122360229492
[Epoch 2][Step 38], time=0.06801247596740723, ext_time=0.04341459274291992, train_time=0.019732952117919922
[Epoch 2][Step 39], time=0.0689537525177002, ext_time=0.042905330657958984, train_time=0.021222352981567383
[Epoch 2][Step 40], time=0.06856656074523926, ext_time=0.043129682540893555, train_time=0.020536422729492188
[Epoch 2][Step 41], time=0.06859374046325684, ext_time=0.04340934753417969, train_time=0.020285367965698242
[Epoch 2][Step 42], time=0.06930041313171387, ext_time=0.04198646545410156, train_time=0.022502422332763672
[Epoch 2][Step 43], time=0.06959128379821777, ext_time=0.04469132423400879, train_time=0.01989579200744629
[Epoch 2][Step 44], time=0.06919264793395996, ext_time=0.04302072525024414, train_time=0.021314144134521484
[Epoch 2][Step 45], time=0.07016634941101074, ext_time=0.04388141632080078, train_time=0.02133774757385254
[Epoch 2][Step 46], time=0.06887555122375488, ext_time=0.04367947578430176, train_time=0.019983768463134766
[Epoch 2][Step 47], time=0.06961870193481445, ext_time=0.04218554496765137, train_time=0.02262115478515625
[Epoch 2][Step 48], time=0.06954503059387207, ext_time=0.04414939880371094, train_time=0.019949913024902344
[Epoch 2][Step 49], time=0.06897711753845215, ext_time=0.04387688636779785, train_time=0.020180225372314453
[Epoch 2][Step 50], time=0.06809687614440918, ext_time=0.04382443428039551, train_time=0.019366979598999023
[Epoch 2][Step 51], time=0.06848359107971191, ext_time=0.04393339157104492, train_time=0.019638776779174805
[Epoch 2][Step 52], time=0.06958174705505371, ext_time=0.044812679290771484, train_time=0.019766807556152344
[Epoch 2][Step 53], time=0.06845355033874512, ext_time=0.04339885711669922, train_time=0.02016448974609375
[Epoch 2][Step 54], time=0.06853604316711426, ext_time=0.04346823692321777, train_time=0.020164012908935547
[Epoch 2][Step 55], time=0.0682528018951416, ext_time=0.04407334327697754, train_time=0.019266128540039062
[Epoch 2][Step 56], time=0.0701446533203125, ext_time=0.04400229454040527, train_time=0.021181344985961914
[Epoch 2][Step 57], time=0.06824970245361328, ext_time=0.04331374168395996, train_time=0.020059823989868164
[Epoch 2][Step 58], time=0.07000970840454102, ext_time=0.04337811470031738, train_time=0.02169036865234375
[Epoch 2][Step 59], time=0.06980490684509277, ext_time=0.0432286262512207, train_time=0.021723031997680664
[Epoch 2][Step 60], time=0.07006692886352539, ext_time=0.043784141540527344, train_time=0.021349668502807617
[Epoch 2][Step 61], time=0.0684199333190918, ext_time=0.04381608963012695, train_time=0.01965928077697754
[Epoch 2][Step 62], time=0.06988191604614258, ext_time=0.04416632652282715, train_time=0.020777463912963867
[Epoch 2][Step 63], time=0.0692758560180664, ext_time=0.04450821876525879, train_time=0.019762039184570312
[Epoch 2][Step 64], time=0.07047462463378906, ext_time=0.04384660720825195, train_time=0.021702051162719727
[Epoch 2][Step 65], time=0.06935596466064453, ext_time=0.04388308525085449, train_time=0.02053666114807129
[Epoch 2][Step 66], time=0.06932544708251953, ext_time=0.0437774658203125, train_time=0.0205838680267334
[Epoch 2][Step 67], time=0.06843090057373047, ext_time=0.04392743110656738, train_time=0.019589900970458984
[Epoch 2][Step 68], time=0.06905198097229004, ext_time=0.04405713081359863, train_time=0.020021677017211914
[Epoch 2][Step 69], time=0.06989312171936035, ext_time=0.04303765296936035, train_time=0.02200603485107422
[Epoch 2][Step 70], time=0.07043719291687012, ext_time=0.04500150680541992, train_time=0.02039027214050293
[Epoch 2][Step 71], time=0.0685434341430664, ext_time=0.04407620429992676, train_time=0.019541025161743164
[Epoch 2][Step 72], time=0.06806325912475586, ext_time=0.0436248779296875, train_time=0.019530057907104492
[Epoch 2][Step 73], time=0.06956863403320312, ext_time=0.0431058406829834, train_time=0.021605253219604492
[Epoch 2][Step 74], time=0.06897902488708496, ext_time=0.04352450370788574, train_time=0.0204923152923584
[Epoch 2][Step 75], time=0.0688328742980957, ext_time=0.043073415756225586, train_time=0.02087259292602539
[Epoch 2][Step 76], time=0.06914114952087402, ext_time=0.04295992851257324, train_time=0.021333932876586914
[Epoch 2][Step 77], time=0.0692603588104248, ext_time=0.04375934600830078, train_time=0.020537614822387695
[Epoch 2][Step 78], time=0.06978750228881836, ext_time=0.04466080665588379, train_time=0.020125865936279297
[Epoch 2][Step 79], time=0.06946110725402832, ext_time=0.04410982131958008, train_time=0.020379066467285156
[Epoch 2][Step 80], time=0.06904292106628418, ext_time=0.04290771484375, train_time=0.021320819854736328
[Epoch 2][Step 81], time=0.0699465274810791, ext_time=0.04484748840332031, train_time=0.02008199691772461
[Epoch 2][Step 82], time=0.06793594360351562, ext_time=0.04352760314941406, train_time=0.019527196884155273
[Epoch 2][Step 83], time=0.06792116165161133, ext_time=0.043556928634643555, train_time=0.01946401596069336
[Epoch 2][Step 84], time=0.06969165802001953, ext_time=0.04493093490600586, train_time=0.01978898048400879
[Epoch 2][Step 85], time=0.06882047653198242, ext_time=0.04337811470031738, train_time=0.020520448684692383
[Epoch 2][Step 86], time=0.06917500495910645, ext_time=0.04310274124145508, train_time=0.021187305450439453
[Epoch 2][Step 87], time=0.06793379783630371, ext_time=0.04325985908508301, train_time=0.01973748207092285
[Epoch 2][Step 88], time=0.06883621215820312, ext_time=0.044213294982910156, train_time=0.019674062728881836
[Epoch 2][Step 89], time=0.06860828399658203, ext_time=0.04409503936767578, train_time=0.01957225799560547
[Epoch 2][Step 90], time=0.06986689567565918, ext_time=0.04497170448303223, train_time=0.019861936569213867
[Epoch 2][Step 91], time=0.06933450698852539, ext_time=0.044567108154296875, train_time=0.019782543182373047
[Epoch 2][Step 92], time=0.06870603561401367, ext_time=0.0425875186920166, train_time=0.021314382553100586
[Epoch 2][Step 93], time=0.06939578056335449, ext_time=0.04474616050720215, train_time=0.019600391387939453
[Epoch 2][Step 94], time=0.06916046142578125, ext_time=0.04364347457885742, train_time=0.020625591278076172
[Epoch 2][Step 95], time=0.06829333305358887, ext_time=0.044042110443115234, train_time=0.019273996353149414
[Epoch 2][Step 96], time=0.0691380500793457, ext_time=0.04240822792053223, train_time=0.0219113826751709
[Epoch 2][Step 97], time=0.06843900680541992, ext_time=0.04281473159790039, train_time=0.020802974700927734
[Epoch 2][Step 98], time=0.06885170936584473, ext_time=0.04430031776428223, train_time=0.019648313522338867
[Epoch 2][Step 99], time=0.06886434555053711, ext_time=0.04331684112548828, train_time=0.02072310447692871
[Epoch 2][Step 100], time=0.06934237480163574, ext_time=0.044373273849487305, train_time=0.02001214027404785
[Epoch 2][Step 101], time=0.06876182556152344, ext_time=0.04324221611022949, train_time=0.02067112922668457
[Epoch 2][Step 102], time=0.0694437026977539, ext_time=0.04333829879760742, train_time=0.021221637725830078
[Epoch 2][Step 103], time=0.06939244270324707, ext_time=0.04355168342590332, train_time=0.020917892456054688
[Epoch 2][Step 104], time=0.06847143173217773, ext_time=0.04397225379943848, train_time=0.019596576690673828
[Epoch 2][Step 105], time=0.06877779960632324, ext_time=0.043668270111083984, train_time=0.020184993743896484
[Epoch 2][Step 106], time=0.07013320922851562, ext_time=0.04512500762939453, train_time=0.01995849609375
[Epoch 2][Step 107], time=0.06951594352722168, ext_time=0.04447031021118164, train_time=0.020034074783325195
[Epoch 2][Step 108], time=0.06734132766723633, ext_time=0.042995452880859375, train_time=0.019472360610961914
[Epoch 2][Step 109], time=0.06938886642456055, ext_time=0.04454779624938965, train_time=0.019819259643554688
[Epoch 2][Step 110], time=0.07004714012145996, ext_time=0.04214906692504883, train_time=0.02315378189086914
[Epoch 2][Step 111], time=0.06868481636047363, ext_time=0.044220685958862305, train_time=0.019493818283081055
[Epoch 2][Step 112], time=0.06842160224914551, ext_time=0.044046878814697266, train_time=0.019443750381469727
[Epoch 2][Step 113], time=0.06943655014038086, ext_time=0.042812347412109375, train_time=0.021779537200927734
[Epoch 2][Step 114], time=0.06837940216064453, ext_time=0.04379987716674805, train_time=0.019671201705932617
[Epoch 2][Step 115], time=0.06837582588195801, ext_time=0.043750762939453125, train_time=0.019668102264404297
[Epoch 2][Step 116], time=0.07419347763061523, ext_time=0.045943498611450195, train_time=0.0213625431060791
[Epoch 2][Step 117], time=0.06852912902832031, ext_time=0.04363417625427246, train_time=0.01985478401184082
[Epoch 2][Step 118], time=0.06889963150024414, ext_time=0.04439425468444824, train_time=0.01951313018798828
[Epoch 2][Step 119], time=0.06927156448364258, ext_time=0.04401707649230957, train_time=0.020359277725219727
[Epoch 2][Step 120], time=0.06920242309570312, ext_time=0.04335927963256836, train_time=0.020978450775146484
[Epoch 2][Step 121], time=0.06930422782897949, ext_time=0.04400014877319336, train_time=0.02035832405090332
[Epoch 2][Step 122], time=0.07045722007751465, ext_time=0.0452272891998291, train_time=0.020183324813842773
[Epoch 2][Step 123], time=0.06927680969238281, ext_time=0.04368185997009277, train_time=0.02069234848022461
[Epoch 2][Step 124], time=0.06973409652709961, ext_time=0.0437467098236084, train_time=0.021058082580566406
[Epoch 2], time=8.651025772094727, loss=2.483710527420044
[Epoch 3][Step 0], time=0.07350730895996094, ext_time=0.045064449310302734, train_time=0.02341151237487793
[Epoch 3][Step 1], time=0.06908583641052246, ext_time=0.04239916801452637, train_time=0.021840810775756836
[Epoch 3][Step 2], time=0.0696420669555664, ext_time=0.04358959197998047, train_time=0.021166324615478516
[Epoch 3][Step 3], time=0.0687408447265625, ext_time=0.04344820976257324, train_time=0.020417213439941406
[Epoch 3][Step 4], time=0.07003283500671387, ext_time=0.04458498954772949, train_time=0.020463228225708008
[Epoch 3][Step 5], time=0.06859683990478516, ext_time=0.043450355529785156, train_time=0.020302772521972656
[Epoch 3][Step 6], time=0.06834077835083008, ext_time=0.04337739944458008, train_time=0.020059823989868164
[Epoch 3][Step 7], time=0.07004690170288086, ext_time=0.04389524459838867, train_time=0.021195411682128906
[Epoch 3][Step 8], time=0.06870508193969727, ext_time=0.044016361236572266, train_time=0.01975560188293457
[Epoch 3][Step 9], time=0.06831645965576172, ext_time=0.04243922233581543, train_time=0.02108287811279297
[Epoch 3][Step 10], time=0.06829500198364258, ext_time=0.04384946823120117, train_time=0.019533395767211914
[Epoch 3][Step 11], time=0.0696556568145752, ext_time=0.04409527778625488, train_time=0.02059650421142578
[Epoch 3][Step 12], time=0.06750321388244629, ext_time=0.04272150993347168, train_time=0.01997661590576172
[Epoch 3][Step 13], time=0.06824612617492676, ext_time=0.04367637634277344, train_time=0.019616365432739258
[Epoch 3][Step 14], time=0.06918668746948242, ext_time=0.04356956481933594, train_time=0.02071356773376465
[Epoch 3][Step 15], time=0.06914949417114258, ext_time=0.04317116737365723, train_time=0.021126270294189453
[Epoch 3][Step 16], time=0.0689859390258789, ext_time=0.04377412796020508, train_time=0.020269155502319336
[Epoch 3][Step 17], time=0.0698235034942627, ext_time=0.04194378852844238, train_time=0.023150205612182617
[Epoch 3][Step 18], time=0.06926345825195312, ext_time=0.04464221000671387, train_time=0.019613027572631836
[Epoch 3][Step 19], time=0.06806206703186035, ext_time=0.04374051094055176, train_time=0.01942586898803711
[Epoch 3][Step 20], time=0.06986355781555176, ext_time=0.044592857360839844, train_time=0.020285367965698242
[Epoch 3][Step 21], time=0.07124876976013184, ext_time=0.044263601303100586, train_time=0.022046566009521484
[Epoch 3][Step 22], time=0.0692756175994873, ext_time=0.044470787048339844, train_time=0.019796371459960938
[Epoch 3][Step 23], time=0.0675969123840332, ext_time=0.04323124885559082, train_time=0.01947331428527832
[Epoch 3][Step 24], time=0.06912374496459961, ext_time=0.042962074279785156, train_time=0.021271705627441406
[Epoch 3][Step 25], time=0.06897687911987305, ext_time=0.04349040985107422, train_time=0.020603179931640625
[Epoch 3][Step 26], time=0.06942915916442871, ext_time=0.04466509819030762, train_time=0.019783496856689453
[Epoch 3][Step 27], time=0.06938552856445312, ext_time=0.04444718360900879, train_time=0.019918441772460938
[Epoch 3][Step 28], time=0.06810522079467773, ext_time=0.04312419891357422, train_time=0.020132780075073242
[Epoch 3][Step 29], time=0.06807684898376465, ext_time=0.04308032989501953, train_time=0.020134687423706055
[Epoch 3][Step 30], time=0.06976580619812012, ext_time=0.043802499771118164, train_time=0.02104496955871582
[Epoch 3][Step 31], time=0.06887578964233398, ext_time=0.04265713691711426, train_time=0.021316051483154297
[Epoch 3][Step 32], time=0.06863999366760254, ext_time=0.04314565658569336, train_time=0.020627498626708984
[Epoch 3][Step 33], time=0.06947731971740723, ext_time=0.04452395439147949, train_time=0.019952058792114258
[Epoch 3][Step 34], time=0.06841444969177246, ext_time=0.04348897933959961, train_time=0.020062685012817383
[Epoch 3][Step 35], time=0.06940197944641113, ext_time=0.04310774803161621, train_time=0.021399974822998047
[Epoch 3][Step 36], time=0.06895756721496582, ext_time=0.043710947036743164, train_time=0.020321369171142578
[Epoch 3][Step 37], time=0.0692746639251709, ext_time=0.042532920837402344, train_time=0.021940231323242188
[Epoch 3][Step 38], time=0.06803655624389648, ext_time=0.043469905853271484, train_time=0.019686222076416016
[Epoch 3][Step 39], time=0.06882619857788086, ext_time=0.04294109344482422, train_time=0.021001815795898438
[Epoch 3][Step 40], time=0.06921982765197754, ext_time=0.043321847915649414, train_time=0.021027565002441406
[Epoch 3][Step 41], time=0.06870436668395996, ext_time=0.043395042419433594, train_time=0.020403146743774414
[Epoch 3][Step 42], time=0.06949710845947266, ext_time=0.04195904731750488, train_time=0.022777557373046875
[Epoch 3][Step 43], time=0.06972193717956543, ext_time=0.04465985298156738, train_time=0.0200655460357666
[Epoch 3][Step 44], time=0.06928086280822754, ext_time=0.04284477233886719, train_time=0.02161717414855957
[Epoch 3][Step 45], time=0.07013797760009766, ext_time=0.04390239715576172, train_time=0.02125382423400879
[Epoch 3][Step 46], time=0.0684654712677002, ext_time=0.043764591217041016, train_time=0.019745349884033203
[Epoch 3][Step 47], time=0.06922531127929688, ext_time=0.04218769073486328, train_time=0.02225661277770996
[Epoch 3][Step 48], time=0.06852436065673828, ext_time=0.04400038719177246, train_time=0.019553184509277344
[Epoch 3][Step 49], time=0.0688621997833252, ext_time=0.043740272521972656, train_time=0.02021026611328125
[Epoch 3][Step 50], time=0.06876349449157715, ext_time=0.04377126693725586, train_time=0.020074129104614258
[Epoch 3][Step 51], time=0.06881332397460938, ext_time=0.04407644271850586, train_time=0.01983952522277832
[Epoch 3][Step 52], time=0.06938600540161133, ext_time=0.044767141342163086, train_time=0.01961660385131836
[Epoch 3][Step 53], time=0.06864643096923828, ext_time=0.04343533515930176, train_time=0.020324230194091797
[Epoch 3][Step 54], time=0.06847977638244629, ext_time=0.04346513748168945, train_time=0.02012801170349121
[Epoch 3][Step 55], time=0.06914687156677246, ext_time=0.044028520584106445, train_time=0.02019190788269043
[Epoch 3][Step 56], time=0.07011198997497559, ext_time=0.04410982131958008, train_time=0.02104043960571289
[Epoch 3][Step 57], time=0.06829237937927246, ext_time=0.04319000244140625, train_time=0.020238876342773438
[Epoch 3][Step 58], time=0.07012367248535156, ext_time=0.043439626693725586, train_time=0.02176952362060547
[Epoch 3][Step 59], time=0.06962966918945312, ext_time=0.043399810791015625, train_time=0.021351099014282227
[Epoch 3][Step 60], time=0.0700523853302002, ext_time=0.0441136360168457, train_time=0.021009206771850586
[Epoch 3][Step 61], time=0.0685415267944336, ext_time=0.043849945068359375, train_time=0.019582509994506836
[Epoch 3][Step 62], time=0.06969833374023438, ext_time=0.044083356857299805, train_time=0.020667314529418945
[Epoch 3][Step 63], time=0.06943273544311523, ext_time=0.04431509971618652, train_time=0.020145416259765625
[Epoch 3][Step 64], time=0.07047271728515625, ext_time=0.04384970664978027, train_time=0.021718740463256836
[Epoch 3][Step 65], time=0.06938409805297852, ext_time=0.04384636878967285, train_time=0.020623207092285156
[Epoch 3][Step 66], time=0.06870269775390625, ext_time=0.043662071228027344, train_time=0.02008819580078125
[Epoch 3][Step 67], time=0.06850862503051758, ext_time=0.04380631446838379, train_time=0.01975703239440918
[Epoch 3][Step 68], time=0.0687415599822998, ext_time=0.04391336441040039, train_time=0.019846439361572266
[Epoch 3][Step 69], time=0.06981492042541504, ext_time=0.04310965538024902, train_time=0.02184605598449707
[Epoch 3][Step 70], time=0.07144403457641602, ext_time=0.04505515098571777, train_time=0.020163536071777344
[Epoch 3][Step 71], time=0.06861376762390137, ext_time=0.0440213680267334, train_time=0.01967167854309082
[Epoch 3][Step 72], time=0.0677022933959961, ext_time=0.0436251163482666, train_time=0.019143104553222656
[Epoch 3][Step 73], time=0.06965303421020508, ext_time=0.043257713317871094, train_time=0.021511077880859375
[Epoch 3][Step 74], time=0.06949234008789062, ext_time=0.04350900650024414, train_time=0.02111077308654785
[Epoch 3][Step 75], time=0.06912398338317871, ext_time=0.04300642013549805, train_time=0.021206378936767578
[Epoch 3][Step 76], time=0.06909298896789551, ext_time=0.043020009994506836, train_time=0.021211862564086914
[Epoch 3][Step 77], time=0.07000398635864258, ext_time=0.04381203651428223, train_time=0.021244049072265625
[Epoch 3][Step 78], time=0.06973695755004883, ext_time=0.04467582702636719, train_time=0.02005743980407715
[Epoch 3][Step 79], time=0.06933450698852539, ext_time=0.04399514198303223, train_time=0.02037811279296875
[Epoch 3][Step 80], time=0.06879925727844238, ext_time=0.042992353439331055, train_time=0.02098870277404785
[Epoch 3][Step 81], time=0.06976938247680664, ext_time=0.04475045204162598, train_time=0.019995450973510742
[Epoch 3][Step 82], time=0.06813359260559082, ext_time=0.04368162155151367, train_time=0.01954793930053711
[Epoch 3][Step 83], time=0.06838536262512207, ext_time=0.04345512390136719, train_time=0.02002263069152832
[Epoch 3][Step 84], time=0.06987762451171875, ext_time=0.04495716094970703, train_time=0.019904613494873047
[Epoch 3][Step 85], time=0.06809186935424805, ext_time=0.04334521293640137, train_time=0.019807100296020508
[Epoch 3][Step 86], time=0.06914663314819336, ext_time=0.04319000244140625, train_time=0.021024227142333984
[Epoch 3][Step 87], time=0.06837272644042969, ext_time=0.04317522048950195, train_time=0.02034473419189453
[Epoch 3][Step 88], time=0.0689535140991211, ext_time=0.04419898986816406, train_time=0.019817352294921875
[Epoch 3][Step 89], time=0.06910037994384766, ext_time=0.044112205505371094, train_time=0.020052194595336914
[Epoch 3][Step 90], time=0.07011604309082031, ext_time=0.04495954513549805, train_time=0.020117998123168945
[Epoch 3][Step 91], time=0.06922054290771484, ext_time=0.04470419883728027, train_time=0.019494295120239258
[Epoch 3][Step 92], time=0.0686647891998291, ext_time=0.042528390884399414, train_time=0.021334171295166016
[Epoch 3][Step 93], time=0.06973958015441895, ext_time=0.04468131065368652, train_time=0.02004837989807129
[Epoch 3][Step 94], time=0.06907391548156738, ext_time=0.04369544982910156, train_time=0.020424604415893555
[Epoch 3][Step 95], time=0.06839513778686523, ext_time=0.04400348663330078, train_time=0.019483089447021484
[Epoch 3][Step 96], time=0.06927371025085449, ext_time=0.042464494705200195, train_time=0.02198004722595215
[Epoch 3][Step 97], time=0.06856608390808105, ext_time=0.04289364814758301, train_time=0.020859479904174805
[Epoch 3][Step 98], time=0.06884241104125977, ext_time=0.04422497749328613, train_time=0.019722938537597656
[Epoch 3][Step 99], time=0.06851482391357422, ext_time=0.04345440864562988, train_time=0.02022266387939453
[Epoch 3][Step 100], time=0.06907534599304199, ext_time=0.04450058937072754, train_time=0.019591331481933594
[Epoch 3][Step 101], time=0.06873512268066406, ext_time=0.04331350326538086, train_time=0.020525693893432617
[Epoch 3][Step 102], time=0.06958794593811035, ext_time=0.04333925247192383, train_time=0.02133917808532715
[Epoch 3][Step 103], time=0.06937074661254883, ext_time=0.043630361557006836, train_time=0.020818710327148438
[Epoch 3][Step 104], time=0.0684821605682373, ext_time=0.044037818908691406, train_time=0.01949787139892578
[Epoch 3][Step 105], time=0.0688941478729248, ext_time=0.04363393783569336, train_time=0.020345211029052734
[Epoch 3][Step 106], time=0.0699317455291748, ext_time=0.044973134994506836, train_time=0.019919395446777344
[Epoch 3][Step 107], time=0.0695197582244873, ext_time=0.04454374313354492, train_time=0.019986629486083984
[Epoch 3][Step 108], time=0.06756353378295898, ext_time=0.04304909706115723, train_time=0.019642353057861328
[Epoch 3][Step 109], time=0.06937384605407715, ext_time=0.04454755783081055, train_time=0.019794702529907227
[Epoch 3][Step 110], time=0.06972813606262207, ext_time=0.04209709167480469, train_time=0.022888898849487305
[Epoch 3][Step 111], time=0.06866621971130371, ext_time=0.044263601303100586, train_time=0.019431352615356445
[Epoch 3][Step 112], time=0.06833386421203613, ext_time=0.04398918151855469, train_time=0.019437551498413086
[Epoch 3][Step 113], time=0.06914186477661133, ext_time=0.04299473762512207, train_time=0.021294116973876953
[Epoch 3][Step 114], time=0.06844258308410645, ext_time=0.04384422302246094, train_time=0.01970219612121582
[Epoch 3][Step 115], time=0.06787657737731934, ext_time=0.04382896423339844, train_time=0.019144296646118164
[Epoch 3][Step 116], time=0.07069230079650879, ext_time=0.04592251777648926, train_time=0.019640445709228516
[Epoch 3][Step 117], time=0.0690011978149414, ext_time=0.043752431869506836, train_time=0.020355224609375
[Epoch 3][Step 118], time=0.06889986991882324, ext_time=0.04427075386047363, train_time=0.019666194915771484
[Epoch 3][Step 119], time=0.0692586898803711, ext_time=0.043973445892333984, train_time=0.020405292510986328
[Epoch 3][Step 120], time=0.06912541389465332, ext_time=0.04342484474182129, train_time=0.020830869674682617
[Epoch 3][Step 121], time=0.06951212882995605, ext_time=0.043886423110961914, train_time=0.02069687843322754
[Epoch 3][Step 122], time=0.07021355628967285, ext_time=0.04510688781738281, train_time=0.019997835159301758
[Epoch 3][Step 123], time=0.0693364143371582, ext_time=0.04368114471435547, train_time=0.020718812942504883
[Epoch 3][Step 124], time=0.06982207298278809, ext_time=0.04370832443237305, train_time=0.02117609977722168
[Epoch 3], time=8.651486873626709, loss=1.8749489784240723
    [Step(average) Profiler Level 1 E3 S499]
        L1  sample           0.004957 | send           0.000000
        L1  recv             0.000000 | copy           0.043810 | convert time 0.000000 | train  0.020586
        L1  feature nbytes  960.41 MB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes    0.00 Bytes | remote nbytes 0.00 Bytes
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S499]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.043810
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S499]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000612 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.000000 | cache combine cache 0.043160 | cache combine remote 0.000000
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S499]
        p50.00_tail_logl2featcopy=0.043796
        p90.00_tail_logl2featcopy=0.044756
        p95.00_tail_logl2featcopy=0.045009
        p99.00_tail_logl2featcopy=0.045586
        p99.90_tail_logl2featcopy=0.047876
[CUDA] cuda: usage: 13.83 GB
Rank=3, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.4211604595184326
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 1.3589859008789062e-05, transfer latency: 0.39327168464660645
presamping
presamping takes 7.2484564781188965
Rank=2, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.41107892990112305
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 1.4543533325195312e-05, transfer latency: 0.3900794982910156
presamping
presamping takes 7.997956037521362
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   74567 KB |    1721 MB |    2116 GB |    2116 GB |
|       from large pool |   62742 KB |    1710 MB |    2097 GB |    2097 GB |
|       from small pool |   11825 KB |      19 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Active memory         |   74567 KB |    1721 MB |    2116 GB |    2116 GB |
|       from large pool |   62742 KB |    1710 MB |    2097 GB |    2097 GB |
|       from small pool |   11825 KB |      19 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2770 MB |    3780 MB |    6436 MB |    3666 MB |
|       from large pool |    2740 MB |    3752 MB |    6400 MB |    3660 MB |
|       from small pool |      30 MB |      30 MB |      36 MB |       6 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   56504 KB |    1602 MB |    2645 GB |    2645 GB |
|       from large pool |   43754 KB |    1590 MB |    2625 GB |    2625 GB |
|       from small pool |   12750 KB |      17 MB |      19 GB |      19 GB |
|---------------------------------------------------------------------------|
| Allocations           |      66    |      95    |  136054    |  135988    |
|       from large pool |      12    |      23    |   44762    |   44750    |
|       from small pool |      54    |      73    |   91292    |   91238    |
|---------------------------------------------------------------------------|
| Active allocs         |      66    |      95    |  136054    |  135988    |
|       from large pool |      12    |      23    |   44762    |   44750    |
|       from small pool |      54    |      73    |   91292    |   91238    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      26    |      26    |      36    |      10    |
|       from large pool |      11    |      12    |      18    |       7    |
|       from small pool |      15    |      15    |      18    |       3    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      35    |      51    |   49609    |   49574    |
|       from large pool |       6    |      16    |   23624    |   23618    |
|       from small pool |      29    |      40    |   25985    |   25956    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 35.875924 seconds
[EPOCH_TIME] 8.968981 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 8.651371 seconds
Rank=1, Graph loaded.
!!!!Train_dataloader(with 125 items) enumerate latency: 0.4076218605041504
torch.Size([2000]) torch.Size([2000])
torch.Size([2104]) torch.Size([2104])
!!!!Train_data_list(with 125 items) enumerate latency: 1.0728836059570312e-05, transfer latency: 0.37479162216186523
presamping
presamping takes 5.8631017208099365

