succeed=True
[CUDA] cuda: usage: 5.46 GB
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
coll_cache:optimal_rep_storage=0
coll_cache:optimal_part_storage=0.28
coll_cache:optimal_cpu_storage=0.72
coll_cache:optimal_local_storage=0.07
coll_cache:optimal_remote_storage=0.21
coll_cache:optimal_local_rate=0.248115
coll_cache:optimal_remote_rate=0.744346
coll_cache:optimal_cpu_rate=0.00753916
z=5817.27,5817.27,5817.27,5817.27,
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
config:eval_tsp="2023-08-04 19:13:36"
config:num_worker=4
config:num_intra_size=4
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=ogbn-papers100M
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=250
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.07
config:cache_policy=clique_part
config:omp_thread_num=40
config:unsupervised=False
config:classnum=172
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7fd5ad8cd7f0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=4
Rank=0, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.3637182712554932
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 6.67572021484375e-06, transfer latency: 1.366056203842163
epoch=4 total_steps=300
presamping
presamping takes 6.055286884307861
start training...
[Epoch 0][Step 0], time=1.1367154121398926, ext_time=0.008996725082397461, train_time=1.115164041519165
[Epoch 0][Step 1], time=0.04986166954040527, ext_time=0.0067369937896728516, train_time=0.03292536735534668
[Epoch 0][Step 2], time=0.04790329933166504, ext_time=0.006694793701171875, train_time=0.03157305717468262
[Epoch 0][Step 3], time=0.04811835289001465, ext_time=0.006608009338378906, train_time=0.03198981285095215
[Epoch 0][Step 4], time=0.04847073554992676, ext_time=0.0071680545806884766, train_time=0.03151440620422363
[Epoch 0][Step 5], time=0.04821419715881348, ext_time=0.006769895553588867, train_time=0.031807661056518555
[Epoch 0][Step 6], time=0.04807233810424805, ext_time=0.006631135940551758, train_time=0.03181815147399902
[Epoch 0][Step 7], time=0.04830431938171387, ext_time=0.0066986083984375, train_time=0.032006025314331055
[Epoch 0][Step 8], time=0.047692060470581055, ext_time=0.006556510925292969, train_time=0.03161191940307617
[Epoch 0][Step 9], time=0.047945499420166016, ext_time=0.006685495376586914, train_time=0.03180289268493652
[Epoch 0][Step 10], time=0.04808759689331055, ext_time=0.006609201431274414, train_time=0.03203940391540527
[Epoch 0][Step 11], time=0.048171281814575195, ext_time=0.0065915584564208984, train_time=0.03206515312194824
[Epoch 0][Step 12], time=0.04820108413696289, ext_time=0.006573200225830078, train_time=0.03209185600280762
[Epoch 0][Step 13], time=0.04828524589538574, ext_time=0.0066242218017578125, train_time=0.03213167190551758
[Epoch 0][Step 14], time=0.04796886444091797, ext_time=0.0064961910247802734, train_time=0.031949520111083984
[Epoch 0][Step 15], time=0.04803800582885742, ext_time=0.006576061248779297, train_time=0.03206181526184082
[Epoch 0][Step 16], time=0.047677040100097656, ext_time=0.006558656692504883, train_time=0.03157687187194824
[Epoch 0][Step 17], time=0.0476226806640625, ext_time=0.0065610408782958984, train_time=0.03150439262390137
[Epoch 0][Step 18], time=0.04811835289001465, ext_time=0.006549358367919922, train_time=0.031922101974487305
[Epoch 0][Step 19], time=0.047718048095703125, ext_time=0.0066683292388916016, train_time=0.03147602081298828
[Epoch 0][Step 20], time=0.04793095588684082, ext_time=0.006789445877075195, train_time=0.03160977363586426
[Epoch 0][Step 21], time=0.048241376876831055, ext_time=0.006809711456298828, train_time=0.031809329986572266
[Epoch 0][Step 22], time=0.04800844192504883, ext_time=0.006736278533935547, train_time=0.031793832778930664
[Epoch 0][Step 23], time=0.04797983169555664, ext_time=0.00674891471862793, train_time=0.031569719314575195
[Epoch 0][Step 24], time=0.04830336570739746, ext_time=0.006640911102294922, train_time=0.03203105926513672
[Epoch 0][Step 25], time=0.05091452598571777, ext_time=0.006827831268310547, train_time=0.034540414810180664
[Epoch 0][Step 26], time=0.0497288703918457, ext_time=0.006907224655151367, train_time=0.033257246017456055
[Epoch 0][Step 27], time=0.04769301414489746, ext_time=0.0065653324127197266, train_time=0.03155207633972168
[Epoch 0][Step 28], time=0.048161983489990234, ext_time=0.006497383117675781, train_time=0.03211021423339844
[Epoch 0][Step 29], time=0.04786109924316406, ext_time=0.006610870361328125, train_time=0.031586647033691406
[Epoch 0][Step 30], time=0.04819989204406738, ext_time=0.006673097610473633, train_time=0.031920433044433594
[Epoch 0][Step 31], time=0.05052018165588379, ext_time=0.0065670013427734375, train_time=0.03435206413269043
[Epoch 0][Step 32], time=0.048089027404785156, ext_time=0.006807088851928711, train_time=0.03157520294189453
[Epoch 0][Step 33], time=0.048563241958618164, ext_time=0.006632328033447266, train_time=0.03237485885620117
[Epoch 0][Step 34], time=0.04806995391845703, ext_time=0.006447792053222656, train_time=0.03201436996459961
[Epoch 0][Step 35], time=0.048070669174194336, ext_time=0.0065915584564208984, train_time=0.03179788589477539
[Epoch 0][Step 36], time=0.04777646064758301, ext_time=0.006686687469482422, train_time=0.03138232231140137
[Epoch 0][Step 37], time=0.04881906509399414, ext_time=0.006516456604003906, train_time=0.03260922431945801
[Epoch 0][Step 38], time=0.05330371856689453, ext_time=0.006568193435668945, train_time=0.037130117416381836
[Epoch 0][Step 39], time=0.04797768592834473, ext_time=0.006659746170043945, train_time=0.03175544738769531
[Epoch 0][Step 40], time=0.04794573783874512, ext_time=0.006695747375488281, train_time=0.031708478927612305
[Epoch 0][Step 41], time=0.04793357849121094, ext_time=0.00662684440612793, train_time=0.031853675842285156
[Epoch 0][Step 42], time=0.04820370674133301, ext_time=0.0067615509033203125, train_time=0.03183627128601074
[Epoch 0][Step 43], time=0.04752492904663086, ext_time=0.006654024124145508, train_time=0.03131890296936035
[Epoch 0][Step 44], time=0.047972917556762695, ext_time=0.006596088409423828, train_time=0.03193092346191406
[Epoch 0][Step 45], time=0.048105478286743164, ext_time=0.006716728210449219, train_time=0.03182721138000488
[Epoch 0][Step 46], time=0.048017263412475586, ext_time=0.006893157958984375, train_time=0.03166079521179199
[Epoch 0][Step 47], time=0.04748845100402832, ext_time=0.0065097808837890625, train_time=0.03141164779663086
[Epoch 0][Step 48], time=0.04878878593444824, ext_time=0.006799936294555664, train_time=0.03251075744628906
[Epoch 0][Step 49], time=0.04783511161804199, ext_time=0.00657200813293457, train_time=0.03169679641723633
[Epoch 0][Step 50], time=0.04791879653930664, ext_time=0.006562948226928711, train_time=0.03179597854614258
[Epoch 0][Step 51], time=0.04804539680480957, ext_time=0.00652313232421875, train_time=0.03201580047607422
[Epoch 0][Step 52], time=0.047800540924072266, ext_time=0.006602764129638672, train_time=0.03168940544128418
[Epoch 0][Step 53], time=0.04787564277648926, ext_time=0.0068569183349609375, train_time=0.03130340576171875
[Epoch 0][Step 54], time=0.047713279724121094, ext_time=0.006734132766723633, train_time=0.0313565731048584
[Epoch 0][Step 55], time=0.04911088943481445, ext_time=0.0069942474365234375, train_time=0.0325167179107666
[Epoch 0][Step 56], time=0.048491716384887695, ext_time=0.006638765335083008, train_time=0.03231930732727051
[Epoch 0][Step 57], time=0.0488126277923584, ext_time=0.0068852901458740234, train_time=0.03223419189453125
[Epoch 0][Step 58], time=0.04773712158203125, ext_time=0.006507396697998047, train_time=0.03173565864562988
[Epoch 0][Step 59], time=0.04758119583129883, ext_time=0.006679058074951172, train_time=0.031343936920166016
[Epoch 0][Step 60], time=0.04956865310668945, ext_time=0.006855487823486328, train_time=0.03313183784484863
[Epoch 0][Step 61], time=0.048473358154296875, ext_time=0.006819486618041992, train_time=0.03208422660827637
[Epoch 0][Step 62], time=0.04803633689880371, ext_time=0.006510496139526367, train_time=0.031903982162475586
[Epoch 0][Step 63], time=0.0489044189453125, ext_time=0.0070683956146240234, train_time=0.0322265625
[Epoch 0][Step 64], time=0.049175262451171875, ext_time=0.006818294525146484, train_time=0.03280329704284668
[Epoch 0][Step 65], time=0.05060696601867676, ext_time=0.0067293643951416016, train_time=0.03425192832946777
[Epoch 0][Step 66], time=0.04779386520385742, ext_time=0.006637096405029297, train_time=0.03154349327087402
[Epoch 0][Step 67], time=0.048249244689941406, ext_time=0.0068209171295166016, train_time=0.03175663948059082
[Epoch 0][Step 68], time=0.053571462631225586, ext_time=0.006948947906494141, train_time=0.03706097602844238
[Epoch 0][Step 69], time=0.052315473556518555, ext_time=0.006568431854248047, train_time=0.03613877296447754
[Epoch 0][Step 70], time=0.04808402061462402, ext_time=0.006587982177734375, train_time=0.031983375549316406
[Epoch 0][Step 71], time=0.050554513931274414, ext_time=0.006808757781982422, train_time=0.03391075134277344
[Epoch 0][Step 72], time=0.04842114448547363, ext_time=0.0065937042236328125, train_time=0.03221774101257324
[Epoch 0][Step 73], time=0.04719996452331543, ext_time=0.006612300872802734, train_time=0.03118753433227539
[Epoch 0][Step 74], time=0.04874897003173828, ext_time=0.006739377975463867, train_time=0.03269052505493164
[Epoch 0], time=4.733697175979614, loss=4.945679664611816
[Epoch 1][Step 0], time=0.0534367561340332, ext_time=0.00687098503112793, train_time=0.036849260330200195
[Epoch 1][Step 1], time=0.05179190635681152, ext_time=0.006639242172241211, train_time=0.035845041275024414
[Epoch 1][Step 2], time=0.049655914306640625, ext_time=0.006916046142578125, train_time=0.03329586982727051
[Epoch 1][Step 3], time=0.049765825271606445, ext_time=0.006653547286987305, train_time=0.03378939628601074
[Epoch 1][Step 4], time=0.049872636795043945, ext_time=0.007013797760009766, train_time=0.03326845169067383
[Epoch 1][Step 5], time=0.04972505569458008, ext_time=0.0069026947021484375, train_time=0.03336310386657715
[Epoch 1][Step 6], time=0.04975080490112305, ext_time=0.006761789321899414, train_time=0.03350329399108887
[Epoch 1][Step 7], time=0.04976010322570801, ext_time=0.0068094730377197266, train_time=0.03354907035827637
[Epoch 1][Step 8], time=0.048925161361694336, ext_time=0.006621837615966797, train_time=0.032933712005615234
[Epoch 1][Step 9], time=0.04923129081726074, ext_time=0.006632089614868164, train_time=0.0332791805267334
[Epoch 1][Step 10], time=0.04837632179260254, ext_time=0.006579875946044922, train_time=0.032540082931518555
[Epoch 1][Step 11], time=0.04866147041320801, ext_time=0.006510734558105469, train_time=0.03277778625488281
[Epoch 1][Step 12], time=0.048491477966308594, ext_time=0.006573200225830078, train_time=0.03251194953918457
[Epoch 1][Step 13], time=0.04885077476501465, ext_time=0.006613016128540039, train_time=0.03283977508544922
[Epoch 1][Step 14], time=0.048778533935546875, ext_time=0.006487607955932617, train_time=0.03293609619140625
[Epoch 1][Step 15], time=0.04829144477844238, ext_time=0.0064580440521240234, train_time=0.03263235092163086
[Epoch 1][Step 16], time=0.04741477966308594, ext_time=0.006506443023681641, train_time=0.031522512435913086
[Epoch 1][Step 17], time=0.047518014907836914, ext_time=0.006458282470703125, train_time=0.03168797492980957
[Epoch 1][Step 18], time=0.04830288887023926, ext_time=0.0065538883209228516, train_time=0.03224682807922363
[Epoch 1][Step 19], time=0.04766368865966797, ext_time=0.006586313247680664, train_time=0.031646728515625
[Epoch 1][Step 20], time=0.04794740676879883, ext_time=0.00680232048034668, train_time=0.031136512756347656
[Epoch 1][Step 21], time=0.04850482940673828, ext_time=0.006801605224609375, train_time=0.0321810245513916
[Epoch 1][Step 22], time=0.048087358474731445, ext_time=0.006708621978759766, train_time=0.03201556205749512
[Epoch 1][Step 23], time=0.048313140869140625, ext_time=0.006880521774291992, train_time=0.031920671463012695
[Epoch 1][Step 24], time=0.049597978591918945, ext_time=0.006555795669555664, train_time=0.03357887268066406
[Epoch 1][Step 25], time=0.048357486724853516, ext_time=0.006613016128540039, train_time=0.032362937927246094
[Epoch 1][Step 26], time=0.04826188087463379, ext_time=0.006535053253173828, train_time=0.03236985206604004
[Epoch 1][Step 27], time=0.05063986778259277, ext_time=0.006880283355712891, train_time=0.03432488441467285
[Epoch 1][Step 28], time=0.04976940155029297, ext_time=0.006597995758056641, train_time=0.03380227088928223
[Epoch 1][Step 29], time=0.04874086380004883, ext_time=0.0067501068115234375, train_time=0.03249549865722656
[Epoch 1][Step 30], time=0.0491182804107666, ext_time=0.006716251373291016, train_time=0.03298234939575195
[Epoch 1][Step 31], time=0.04840493202209473, ext_time=0.006647348403930664, train_time=0.0323789119720459
[Epoch 1][Step 32], time=0.04919838905334473, ext_time=0.007013559341430664, train_time=0.03267240524291992
[Epoch 1][Step 33], time=0.04949831962585449, ext_time=0.0067136287689208984, train_time=0.03339123725891113
[Epoch 1][Step 34], time=0.04888558387756348, ext_time=0.006572723388671875, train_time=0.032843828201293945
[Epoch 1][Step 35], time=0.04906606674194336, ext_time=0.006705284118652344, train_time=0.03288674354553223
[Epoch 1][Step 36], time=0.048616647720336914, ext_time=0.006991863250732422, train_time=0.03211808204650879
[Epoch 1][Step 37], time=0.04942512512207031, ext_time=0.0066242218017578125, train_time=0.03341412544250488
[Epoch 1][Step 38], time=0.049265384674072266, ext_time=0.006691932678222656, train_time=0.03322172164916992
[Epoch 1][Step 39], time=0.049036502838134766, ext_time=0.006765842437744141, train_time=0.032904624938964844
[Epoch 1][Step 40], time=0.04938364028930664, ext_time=0.0068836212158203125, train_time=0.033185720443725586
[Epoch 1][Step 41], time=0.049101829528808594, ext_time=0.006717205047607422, train_time=0.03310656547546387
[Epoch 1][Step 42], time=0.04871416091918945, ext_time=0.006740570068359375, train_time=0.03245902061462402
[Epoch 1][Step 43], time=0.04903054237365723, ext_time=0.006749868392944336, train_time=0.03295278549194336
[Epoch 1][Step 44], time=0.04845595359802246, ext_time=0.00674891471862793, train_time=0.032327890396118164
[Epoch 1][Step 45], time=0.04884672164916992, ext_time=0.006650209426879883, train_time=0.03286004066467285
[Epoch 1][Step 46], time=0.048787832260131836, ext_time=0.0067713260650634766, train_time=0.032694101333618164
[Epoch 1][Step 47], time=0.048479557037353516, ext_time=0.006593942642211914, train_time=0.03249382972717285
[Epoch 1][Step 48], time=0.04920458793640137, ext_time=0.00665283203125, train_time=0.031410932540893555
[Epoch 1][Step 49], time=0.04873299598693848, ext_time=0.006773471832275391, train_time=0.03250479698181152
[Epoch 1][Step 50], time=0.04856109619140625, ext_time=0.006533384323120117, train_time=0.032648324966430664
[Epoch 1][Step 51], time=0.04976654052734375, ext_time=0.006629228591918945, train_time=0.03377819061279297
[Epoch 1][Step 52], time=0.04883742332458496, ext_time=0.006728649139404297, train_time=0.03271889686584473
[Epoch 1][Step 53], time=0.04886150360107422, ext_time=0.006974697113037109, train_time=0.03239917755126953
[Epoch 1][Step 54], time=0.04832196235656738, ext_time=0.0068285465240478516, train_time=0.031884193420410156
[Epoch 1][Step 55], time=0.048856258392333984, ext_time=0.006929159164428711, train_time=0.032503604888916016
[Epoch 1][Step 56], time=0.049757957458496094, ext_time=0.006675004959106445, train_time=0.03378653526306152
[Epoch 1][Step 57], time=0.04889202117919922, ext_time=0.006932735443115234, train_time=0.032441139221191406
[Epoch 1][Step 58], time=0.04845857620239258, ext_time=0.006600141525268555, train_time=0.0325312614440918
[Epoch 1][Step 59], time=0.04808402061462402, ext_time=0.006691694259643555, train_time=0.03199934959411621
[Epoch 1][Step 60], time=0.04896402359008789, ext_time=0.0066874027252197266, train_time=0.03289198875427246
[Epoch 1][Step 61], time=0.04972529411315918, ext_time=0.0070171356201171875, train_time=0.0332186222076416
[Epoch 1][Step 62], time=0.04895973205566406, ext_time=0.0066111087799072266, train_time=0.03284096717834473
[Epoch 1][Step 63], time=0.04866290092468262, ext_time=0.006884098052978516, train_time=0.032303810119628906
[Epoch 1][Step 64], time=0.04932904243469238, ext_time=0.006725788116455078, train_time=0.03320717811584473
[Epoch 1][Step 65], time=0.048792362213134766, ext_time=0.006803989410400391, train_time=0.03236699104309082
[Epoch 1][Step 66], time=0.04849720001220703, ext_time=0.006710052490234375, train_time=0.03236556053161621
[Epoch 1][Step 67], time=0.04929709434509277, ext_time=0.00700688362121582, train_time=0.032807111740112305
[Epoch 1][Step 68], time=0.04929089546203613, ext_time=0.0067942142486572266, train_time=0.03313302993774414
[Epoch 1][Step 69], time=0.04907035827636719, ext_time=0.006707429885864258, train_time=0.03299093246459961
[Epoch 1][Step 70], time=0.048711538314819336, ext_time=0.006700277328491211, train_time=0.03267312049865723
[Epoch 1][Step 71], time=0.04871630668640137, ext_time=0.0067098140716552734, train_time=0.032480478286743164
[Epoch 1][Step 72], time=0.04896116256713867, ext_time=0.006748676300048828, train_time=0.032804250717163086
[Epoch 1][Step 73], time=0.04862213134765625, ext_time=0.006630420684814453, train_time=0.032601356506347656
[Epoch 1][Step 74], time=0.04889655113220215, ext_time=0.0067255496978759766, train_time=0.032845258712768555
[Epoch 1], time=3.680438280105591, loss=4.785239219665527
[Epoch 2][Step 0], time=0.052220821380615234, ext_time=0.00670170783996582, train_time=0.03588271141052246
[Epoch 2][Step 1], time=0.04886221885681152, ext_time=0.006628751754760742, train_time=0.03285694122314453
[Epoch 2][Step 2], time=0.048609256744384766, ext_time=0.006812572479248047, train_time=0.032372474670410156
[Epoch 2][Step 3], time=0.04885673522949219, ext_time=0.006661415100097656, train_time=0.03286242485046387
[Epoch 2][Step 4], time=0.04950714111328125, ext_time=0.006961822509765625, train_time=0.03296947479248047
[Epoch 2][Step 5], time=0.04911446571350098, ext_time=0.006918430328369141, train_time=0.03271079063415527
[Epoch 2][Step 6], time=0.048799753189086914, ext_time=0.006768941879272461, train_time=0.032595157623291016
[Epoch 2][Step 7], time=0.04900407791137695, ext_time=0.006809234619140625, train_time=0.032765865325927734
[Epoch 2][Step 8], time=0.04842782020568848, ext_time=0.006592512130737305, train_time=0.032502174377441406
[Epoch 2][Step 9], time=0.04987835884094238, ext_time=0.0067005157470703125, train_time=0.033850908279418945
[Epoch 2][Step 10], time=0.04919838905334473, ext_time=0.006758928298950195, train_time=0.03318500518798828
[Epoch 2][Step 11], time=0.049108028411865234, ext_time=0.006701469421386719, train_time=0.03306984901428223
[Epoch 2][Step 12], time=0.04916095733642578, ext_time=0.006960391998291016, train_time=0.0327603816986084
[Epoch 2][Step 13], time=0.048949480056762695, ext_time=0.006780147552490234, train_time=0.03276419639587402
[Epoch 2][Step 14], time=0.04887199401855469, ext_time=0.006515979766845703, train_time=0.03297734260559082
[Epoch 2][Step 15], time=0.050476789474487305, ext_time=0.0068302154541015625, train_time=0.03436088562011719
[Epoch 2][Step 16], time=0.048860788345336914, ext_time=0.006642341613769531, train_time=0.03281426429748535
[Epoch 2][Step 17], time=0.04866790771484375, ext_time=0.006675004959106445, train_time=0.03255796432495117
[Epoch 2][Step 18], time=0.04884028434753418, ext_time=0.006697893142700195, train_time=0.03265571594238281
[Epoch 2][Step 19], time=0.04860329627990723, ext_time=0.00678253173828125, train_time=0.03149819374084473
[Epoch 2][Step 20], time=0.048978328704833984, ext_time=0.006765604019165039, train_time=0.032872915267944336
[Epoch 2][Step 21], time=0.04917550086975098, ext_time=0.0069315433502197266, train_time=0.032781124114990234
[Epoch 2][Step 22], time=0.04875326156616211, ext_time=0.0067861080169677734, train_time=0.03263282775878906
[Epoch 2][Step 23], time=0.04889249801635742, ext_time=0.006873130798339844, train_time=0.032534122467041016
[Epoch 2][Step 24], time=0.0490880012512207, ext_time=0.006704092025756836, train_time=0.03294777870178223
[Epoch 2][Step 25], time=0.04913520812988281, ext_time=0.006726741790771484, train_time=0.03304457664489746
[Epoch 2][Step 26], time=0.049027204513549805, ext_time=0.0067081451416015625, train_time=0.032982826232910156
[Epoch 2][Step 27], time=0.04896259307861328, ext_time=0.006651401519775391, train_time=0.03282332420349121
[Epoch 2][Step 28], time=0.048738956451416016, ext_time=0.006634235382080078, train_time=0.032794952392578125
[Epoch 2][Step 29], time=0.048325538635253906, ext_time=0.00662541389465332, train_time=0.032224416732788086
[Epoch 2][Step 30], time=0.04901742935180664, ext_time=0.0067424774169921875, train_time=0.03286385536193848
[Epoch 2][Step 31], time=0.04859161376953125, ext_time=0.0067365169525146484, train_time=0.03241682052612305
[Epoch 2][Step 32], time=0.049401044845581055, ext_time=0.0070192813873291016, train_time=0.03284883499145508
[Epoch 2][Step 33], time=0.049399614334106445, ext_time=0.006749629974365234, train_time=0.03325676918029785
[Epoch 2][Step 34], time=0.04889941215515137, ext_time=0.006613731384277344, train_time=0.03292274475097656
[Epoch 2][Step 35], time=0.04920339584350586, ext_time=0.006738901138305664, train_time=0.03298664093017578
[Epoch 2][Step 36], time=0.04795360565185547, ext_time=0.006902217864990234, train_time=0.03156304359436035
[Epoch 2][Step 37], time=0.049279212951660156, ext_time=0.0066683292388916016, train_time=0.03323483467102051
[Epoch 2][Step 38], time=0.049204349517822266, ext_time=0.006613731384277344, train_time=0.03320598602294922
[Epoch 2][Step 39], time=0.04921245574951172, ext_time=0.006740093231201172, train_time=0.03291440010070801
[Epoch 2][Step 40], time=0.04904532432556152, ext_time=0.00683903694152832, train_time=0.03287863731384277
[Epoch 2][Step 41], time=0.04879903793334961, ext_time=0.0067980289459228516, train_time=0.0326848030090332
[Epoch 2][Step 42], time=0.0487823486328125, ext_time=0.006849050521850586, train_time=0.03248453140258789
[Epoch 2][Step 43], time=0.04848623275756836, ext_time=0.006781339645385742, train_time=0.0323488712310791
[Epoch 2][Step 44], time=0.04883861541748047, ext_time=0.0067272186279296875, train_time=0.03281807899475098
[Epoch 2][Step 45], time=0.049031734466552734, ext_time=0.006652355194091797, train_time=0.033030033111572266
[Epoch 2][Step 46], time=0.04895973205566406, ext_time=0.0068089962005615234, train_time=0.03282022476196289
[Epoch 2][Step 47], time=0.04866909980773926, ext_time=0.006500959396362305, train_time=0.03275108337402344
[Epoch 2][Step 48], time=0.049652814865112305, ext_time=0.006890296936035156, train_time=0.03340864181518555
[Epoch 2][Step 49], time=0.048998355865478516, ext_time=0.0068471431732177734, train_time=0.03268694877624512
[Epoch 2][Step 50], time=0.04850506782531738, ext_time=0.0065402984619140625, train_time=0.03253984451293945
[Epoch 2][Step 51], time=0.048998117446899414, ext_time=0.006657838821411133, train_time=0.03296518325805664
[Epoch 2][Step 52], time=0.048525333404541016, ext_time=0.006750822067260742, train_time=0.03241991996765137
[Epoch 2][Step 53], time=0.048543691635131836, ext_time=0.006916522979736328, train_time=0.032063961029052734
[Epoch 2][Step 54], time=0.048569679260253906, ext_time=0.006855487823486328, train_time=0.03224611282348633
[Epoch 2][Step 55], time=0.05023789405822754, ext_time=0.0070302486419677734, train_time=0.0337526798248291
[Epoch 2][Step 56], time=0.04894113540649414, ext_time=0.006669282913208008, train_time=0.032984018325805664
[Epoch 2][Step 57], time=0.04871010780334473, ext_time=0.006911277770996094, train_time=0.03232407569885254
[Epoch 2][Step 58], time=0.0483701229095459, ext_time=0.0066375732421875, train_time=0.03238844871520996
[Epoch 2][Step 59], time=0.048108816146850586, ext_time=0.0066645145416259766, train_time=0.032034873962402344
[Epoch 2][Step 60], time=0.0487515926361084, ext_time=0.006701231002807617, train_time=0.03264164924621582
[Epoch 2][Step 61], time=0.04948997497558594, ext_time=0.006970882415771484, train_time=0.033025264739990234
[Epoch 2][Step 62], time=0.04905223846435547, ext_time=0.006643056869506836, train_time=0.03291893005371094
[Epoch 2][Step 63], time=0.048830509185791016, ext_time=0.006898641586303711, train_time=0.032456159591674805
[Epoch 2][Step 64], time=0.04911375045776367, ext_time=0.006751298904418945, train_time=0.03301882743835449
[Epoch 2][Step 65], time=0.04907059669494629, ext_time=0.006860494613647461, train_time=0.0327608585357666
[Epoch 2][Step 66], time=0.04851508140563965, ext_time=0.0067441463470458984, train_time=0.03235602378845215
[Epoch 2][Step 67], time=0.049327850341796875, ext_time=0.006766080856323242, train_time=0.033071041107177734
[Epoch 2][Step 68], time=0.049329280853271484, ext_time=0.006772756576538086, train_time=0.033145904541015625
[Epoch 2][Step 69], time=0.0490264892578125, ext_time=0.006846427917480469, train_time=0.03283190727233887
[Epoch 2][Step 70], time=0.04847097396850586, ext_time=0.006690025329589844, train_time=0.03245234489440918
[Epoch 2][Step 71], time=0.04867720603942871, ext_time=0.006699323654174805, train_time=0.032463788986206055
[Epoch 2][Step 72], time=0.04902219772338867, ext_time=0.006787300109863281, train_time=0.03282594680786133
[Epoch 2][Step 73], time=0.04859757423400879, ext_time=0.00671076774597168, train_time=0.03252744674682617
[Epoch 2][Step 74], time=0.04893207550048828, ext_time=0.006738901138305664, train_time=0.03286600112915039
[Epoch 2], time=3.6800737380981445, loss=4.6624436378479
[Epoch 3][Step 0], time=0.050867319107055664, ext_time=0.0066220760345458984, train_time=0.0346677303314209
[Epoch 3][Step 1], time=0.04878354072570801, ext_time=0.006678342819213867, train_time=0.032758474349975586
[Epoch 3][Step 2], time=0.0485539436340332, ext_time=0.006815671920776367, train_time=0.03233957290649414
[Epoch 3][Step 3], time=0.04895615577697754, ext_time=0.006708860397338867, train_time=0.03288388252258301
[Epoch 3][Step 4], time=0.04886150360107422, ext_time=0.007117271423339844, train_time=0.032205820083618164
[Epoch 3][Step 5], time=0.04965782165527344, ext_time=0.0070531368255615234, train_time=0.033139944076538086
[Epoch 3][Step 6], time=0.048828125, ext_time=0.006723880767822266, train_time=0.03260397911071777
[Epoch 3][Step 7], time=0.049184322357177734, ext_time=0.006847381591796875, train_time=0.03287458419799805
[Epoch 3][Step 8], time=0.048542022705078125, ext_time=0.006644010543823242, train_time=0.03252434730529785
[Epoch 3][Step 9], time=0.0486292839050293, ext_time=0.0066754817962646484, train_time=0.03258872032165527
[Epoch 3][Step 10], time=0.04884004592895508, ext_time=0.0066988468170166016, train_time=0.03283262252807617
[Epoch 3][Step 11], time=0.048944711685180664, ext_time=0.006637096405029297, train_time=0.03293108940124512
[Epoch 3][Step 12], time=0.04893660545349121, ext_time=0.006647586822509766, train_time=0.03293728828430176
[Epoch 3][Step 13], time=0.049454450607299805, ext_time=0.00674748420715332, train_time=0.03332877159118652
[Epoch 3][Step 14], time=0.048998117446899414, ext_time=0.0066373348236083984, train_time=0.03298044204711914
[Epoch 3][Step 15], time=0.04888153076171875, ext_time=0.00665283203125, train_time=0.03298068046569824
[Epoch 3][Step 16], time=0.04862570762634277, ext_time=0.006638765335083008, train_time=0.03261542320251465
[Epoch 3][Step 17], time=0.0484311580657959, ext_time=0.006638050079345703, train_time=0.03238224983215332
[Epoch 3][Step 18], time=0.048858642578125, ext_time=0.006756782531738281, train_time=0.03266096115112305
[Epoch 3][Step 19], time=0.04841208457946777, ext_time=0.006764888763427734, train_time=0.03222012519836426
[Epoch 3][Step 20], time=0.04962491989135742, ext_time=0.006817817687988281, train_time=0.03343367576599121
[Epoch 3][Step 21], time=0.04942154884338379, ext_time=0.007000923156738281, train_time=0.03293251991271973
[Epoch 3][Step 22], time=0.04878664016723633, ext_time=0.006886005401611328, train_time=0.032544851303100586
[Epoch 3][Step 23], time=0.04874300956726074, ext_time=0.006775856018066406, train_time=0.03248143196105957
[Epoch 3][Step 24], time=0.0494389533996582, ext_time=0.006745100021362305, train_time=0.03318357467651367
[Epoch 3][Step 25], time=0.04896974563598633, ext_time=0.006753206253051758, train_time=0.03276777267456055
[Epoch 3][Step 26], time=0.049025535583496094, ext_time=0.006769657135009766, train_time=0.03286385536193848
[Epoch 3][Step 27], time=0.048813819885253906, ext_time=0.006666898727416992, train_time=0.032671451568603516
[Epoch 3][Step 28], time=0.04946565628051758, ext_time=0.00668644905090332, train_time=0.033452510833740234
[Epoch 3][Step 29], time=0.048763275146484375, ext_time=0.006775856018066406, train_time=0.032454729080200195
[Epoch 3][Step 30], time=0.04912924766540527, ext_time=0.006735801696777344, train_time=0.032924652099609375
[Epoch 3][Step 31], time=0.04849433898925781, ext_time=0.006788015365600586, train_time=0.03229808807373047
[Epoch 3][Step 32], time=0.04904603958129883, ext_time=0.0069446563720703125, train_time=0.03258538246154785
[Epoch 3][Step 33], time=0.04945635795593262, ext_time=0.006679534912109375, train_time=0.03335118293762207
[Epoch 3][Step 34], time=0.04893612861633301, ext_time=0.006569862365722656, train_time=0.033052921295166016
[Epoch 3][Step 35], time=0.04925537109375, ext_time=0.006786823272705078, train_time=0.033034563064575195
[Epoch 3][Step 36], time=0.04822134971618652, ext_time=0.006910562515258789, train_time=0.031806230545043945
[Epoch 3][Step 37], time=0.04950833320617676, ext_time=0.006734132766723633, train_time=0.033388376235961914
[Epoch 3][Step 38], time=0.04924726486206055, ext_time=0.006726980209350586, train_time=0.033162832260131836
[Epoch 3][Step 39], time=0.048833608627319336, ext_time=0.006737470626831055, train_time=0.032746315002441406
[Epoch 3][Step 40], time=0.049398183822631836, ext_time=0.006844520568847656, train_time=0.03322196006774902
[Epoch 3][Step 41], time=0.04817509651184082, ext_time=0.006670236587524414, train_time=0.03169751167297363
[Epoch 3][Step 42], time=0.04857039451599121, ext_time=0.0068302154541015625, train_time=0.03222346305847168
[Epoch 3][Step 43], time=0.04834413528442383, ext_time=0.0066986083984375, train_time=0.03229999542236328
[Epoch 3][Step 44], time=0.04925370216369629, ext_time=0.00672602653503418, train_time=0.03321099281311035
[Epoch 3][Step 45], time=0.04897594451904297, ext_time=0.006707429885864258, train_time=0.03288769721984863
[Epoch 3][Step 46], time=0.04899907112121582, ext_time=0.006810903549194336, train_time=0.032845258712768555
[Epoch 3][Step 47], time=0.04849648475646973, ext_time=0.006578207015991211, train_time=0.032558441162109375
[Epoch 3][Step 48], time=0.04896187782287598, ext_time=0.006874799728393555, train_time=0.03277921676635742
[Epoch 3][Step 49], time=0.048743247985839844, ext_time=0.006709098815917969, train_time=0.03254270553588867
[Epoch 3][Step 50], time=0.04864692687988281, ext_time=0.006582736968994141, train_time=0.03266572952270508
[Epoch 3][Step 51], time=0.04889225959777832, ext_time=0.0066204071044921875, train_time=0.03285861015319824
[Epoch 3][Step 52], time=0.04889249801635742, ext_time=0.006727933883666992, train_time=0.03282451629638672
[Epoch 3][Step 53], time=0.04907560348510742, ext_time=0.006941080093383789, train_time=0.03262639045715332
[Epoch 3][Step 54], time=0.048464298248291016, ext_time=0.006800174713134766, train_time=0.032195329666137695
[Epoch 3][Step 55], time=0.048978328704833984, ext_time=0.006903886795043945, train_time=0.032622575759887695
[Epoch 3][Step 56], time=0.048880577087402344, ext_time=0.006646633148193359, train_time=0.03294539451599121
[Epoch 3][Step 57], time=0.048851966857910156, ext_time=0.006825447082519531, train_time=0.0325469970703125
[Epoch 3][Step 58], time=0.04857492446899414, ext_time=0.00653529167175293, train_time=0.03267312049865723
[Epoch 3][Step 59], time=0.048049211502075195, ext_time=0.006651639938354492, train_time=0.03197121620178223
[Epoch 3][Step 60], time=0.04865860939025879, ext_time=0.006684303283691406, train_time=0.03258180618286133
[Epoch 3][Step 61], time=0.049424171447753906, ext_time=0.006892681121826172, train_time=0.03306746482849121
[Epoch 3][Step 62], time=0.04906105995178223, ext_time=0.006630659103393555, train_time=0.032965898513793945
    [Step(average) Profiler Level 1 E3 S299]
        L1  sample           0.010380 | send           0.000000
        L1  recv             0.000000 | copy           0.006928 | convert time 0.000000 | train  0.031671
        L1  feature nbytes  715.90 MB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes    0.00 Bytes | remote nbytes 0.00 Bytes
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S299]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.006928
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S299]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.001143 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.000000 | cache combine cache 0.005749 | cache combine remote 0.000000
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S299]
        p50.00_tail_logl2featcopy=0.006965
        p90.00_tail_logl2featcopy=0.007264
        p95.00_tail_logl2featcopy=0.007334
        p99.00_tail_logl2featcopy=0.007545
        p99.90_tail_logl2featcopy=0.009928
[CUDA] cuda: usage: 13.97 GB
Rank=1, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.678224802017212
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 1.0967254638671875e-05, transfer latency: 1.7295036315917969
presamping
presamping takes 6.975293159484863
Rank=3, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.3634145259857178
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 8.106231689453125e-06, transfer latency: 1.3565118312835693
presamping
presamping takes 5.099685907363892
[Epoch 3][Step 63], time=0.04874444007873535, ext_time=0.006886005401611328, train_time=0.032411813735961914
[Epoch 3][Step 64], time=0.04887580871582031, ext_time=0.006755828857421875, train_time=0.03271079063415527
[Epoch 3][Step 65], time=0.04900097846984863, ext_time=0.006900787353515625, train_time=0.03265714645385742
[Epoch 3][Step 66], time=0.0484929084777832, ext_time=0.006611347198486328, train_time=0.03246760368347168
[Epoch 3][Step 67], time=0.049096107482910156, ext_time=0.00701904296875, train_time=0.03261709213256836
[Epoch 3][Step 68], time=0.0492095947265625, ext_time=0.006779909133911133, train_time=0.03307294845581055
[Epoch 3][Step 69], time=0.049321651458740234, ext_time=0.006751298904418945, train_time=0.033202171325683594
[Epoch 3][Step 70], time=0.048362016677856445, ext_time=0.006626605987548828, train_time=0.03230571746826172
[Epoch 3][Step 71], time=0.04870748519897461, ext_time=0.006752490997314453, train_time=0.0324397087097168
[Epoch 3][Step 72], time=0.04892921447753906, ext_time=0.006792306900024414, train_time=0.03268003463745117
[Epoch 3][Step 73], time=0.04868125915527344, ext_time=0.0067403316497802734, train_time=0.03248333930969238
[Epoch 3][Step 74], time=0.049005746841430664, ext_time=0.0067424774169921875, train_time=0.032913923263549805
[Epoch 3], time=3.6751961708068848, loss=4.569808006286621
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  128982 KB |    1646 MB |    1974 GB |    1973 GB |
|       from large pool |  124075 KB |    1641 MB |    1969 GB |    1969 GB |
|       from small pool |    4907 KB |       7 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Active memory         |  128982 KB |    1646 MB |    1974 GB |    1973 GB |
|       from large pool |  124075 KB |    1641 MB |    1969 GB |    1969 GB |
|       from small pool |    4907 KB |       7 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    3488 MB |    3488 MB |    3488 MB |       0 B  |
|       from large pool |    3478 MB |    3478 MB |    3478 MB |       0 B  |
|       from small pool |      10 MB |      10 MB |      10 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |     796 MB |    2684 MB |    1153 GB |    1153 GB |
|       from large pool |     792 MB |    2681 MB |    1148 GB |    1148 GB |
|       from small pool |       3 MB |       4 MB |       5 GB |       5 GB |
|---------------------------------------------------------------------------|
| Allocations           |      66    |      95    |   82238    |   82172    |
|       from large pool |      23    |      43    |   39904    |   39881    |
|       from small pool |      43    |      54    |   42334    |   42291    |
|---------------------------------------------------------------------------|
| Active allocs         |      66    |      95    |   82238    |   82172    |
|       from large pool |      23    |      43    |   39904    |   39881    |
|       from small pool |      43    |      54    |   42334    |   42291    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      24    |      24    |      24    |       0    |
|       from large pool |      19    |      19    |      19    |       0    |
|       from small pool |       5    |       5    |       5    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      44    |   34385    |   34352    |
|       from large pool |      15    |      24    |   22900    |   22885    |
|       from small pool |      18    |      22    |   11485    |   11467    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 15.773062 seconds
[EPOCH_TIME] 3.943265 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 3.677764 seconds
Rank=2, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.3334934711456299
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 9.298324584960938e-06, transfer latency: 1.3444876670837402
presamping
presamping takes 7.130223512649536

