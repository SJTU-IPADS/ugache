succeed=True
[CUDA] cuda: usage: 5.48 GB
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 0},
2 :  {link #0 : 3}, {link #1 : 0}, {link #2 : 1},
3 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2},
0 : local 80, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0},
1 : local 80, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g0 0},
2 : local 80, cpu 0 {link #0 : g3 0}, {link #1 : g0 0}, {link #2 : g1 0},
3 : local 80, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0},
coll_cache:optimal_rep_storage=0.07
coll_cache:optimal_part_storage=0
coll_cache:optimal_cpu_storage=0.93
coll_cache:optimal_local_storage=0.07
coll_cache:optimal_remote_storage=0
coll_cache:optimal_local_rate=0.853111
coll_cache:optimal_remote_rate=0
coll_cache:optimal_cpu_rate=0.146889
z=4378.13
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=4037251072
config:eval_tsp="2023-08-06 19:18:14"
config:num_worker=4
config:num_intra_size=4
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=ogbn-papers100M
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=250
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.07
config:cache_policy=rep
config:omp_thread_num=40
config:unsupervised=False
config:classnum=172
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7fc5c0bcf7c0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=4
Rank=0, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.390242338180542
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 2.5272369384765625e-05, transfer latency: 1.4205467700958252
epoch=4 total_steps=300
presamping
presamping takes 5.80478572845459
start training...
[Epoch 0][Step 0], time=1.181844711303711, ext_time=0.015436410903930664, train_time=1.1546947956085205
[Epoch 0][Step 1], time=0.05455827713012695, ext_time=0.010995149612426758, train_time=0.03347587585449219
[Epoch 0][Step 2], time=0.05340456962585449, ext_time=0.011400699615478516, train_time=0.0326688289642334
[Epoch 0][Step 3], time=0.053713321685791016, ext_time=0.011622190475463867, train_time=0.03261518478393555
[Epoch 0][Step 4], time=0.05316352844238281, ext_time=0.011497974395751953, train_time=0.03234553337097168
[Epoch 0][Step 5], time=0.054491281509399414, ext_time=0.011621713638305664, train_time=0.0333704948425293
[Epoch 0][Step 6], time=0.05380964279174805, ext_time=0.011084794998168945, train_time=0.03344392776489258
[Epoch 0][Step 7], time=0.05311441421508789, ext_time=0.011511087417602539, train_time=0.03218722343444824
[Epoch 0][Step 8], time=0.052823543548583984, ext_time=0.011443853378295898, train_time=0.03204917907714844
[Epoch 0][Step 9], time=0.0535123348236084, ext_time=0.01149439811706543, train_time=0.03267359733581543
[Epoch 0][Step 10], time=0.05380058288574219, ext_time=0.011176586151123047, train_time=0.033299922943115234
[Epoch 0][Step 11], time=0.05321931838989258, ext_time=0.011291027069091797, train_time=0.032625436782836914
[Epoch 0][Step 12], time=0.053391456604003906, ext_time=0.011166572570800781, train_time=0.0329132080078125
[Epoch 0][Step 13], time=0.05306386947631836, ext_time=0.011371850967407227, train_time=0.032268524169921875
[Epoch 0][Step 14], time=0.054464101791381836, ext_time=0.011312246322631836, train_time=0.03377580642700195
[Epoch 0][Step 15], time=0.05332827568054199, ext_time=0.011038780212402344, train_time=0.03287911415100098
[Epoch 0][Step 16], time=0.05289268493652344, ext_time=0.010857582092285156, train_time=0.032675981521606445
[Epoch 0][Step 17], time=0.05375051498413086, ext_time=0.011101484298706055, train_time=0.033406734466552734
[Epoch 0][Step 18], time=0.05362200736999512, ext_time=0.011410713195800781, train_time=0.032828330993652344
[Epoch 0][Step 19], time=0.05377840995788574, ext_time=0.01125478744506836, train_time=0.033153533935546875
[Epoch 0][Step 20], time=0.053783416748046875, ext_time=0.011107206344604492, train_time=0.03326988220214844
[Epoch 0][Step 21], time=0.05363202095031738, ext_time=0.01140451431274414, train_time=0.032800912857055664
[Epoch 0][Step 22], time=0.05306744575500488, ext_time=0.011378765106201172, train_time=0.03226470947265625
[Epoch 0][Step 23], time=0.0529475212097168, ext_time=0.011205196380615234, train_time=0.032335519790649414
[Epoch 0][Step 24], time=0.05348658561706543, ext_time=0.011527061462402344, train_time=0.032623291015625
[Epoch 0][Step 25], time=0.05328202247619629, ext_time=0.011417627334594727, train_time=0.03246927261352539
[Epoch 0][Step 26], time=0.05376410484313965, ext_time=0.011200666427612305, train_time=0.033204078674316406
[Epoch 0][Step 27], time=0.05350494384765625, ext_time=0.011141300201416016, train_time=0.03300666809082031
[Epoch 0][Step 28], time=0.053939104080200195, ext_time=0.011455059051513672, train_time=0.03304886817932129
[Epoch 0][Step 29], time=0.05352187156677246, ext_time=0.011565446853637695, train_time=0.0326235294342041
[Epoch 0][Step 30], time=0.056191444396972656, ext_time=0.011742591857910156, train_time=0.03506660461425781
[Epoch 0][Step 31], time=0.05410909652709961, ext_time=0.010802268981933594, train_time=0.03397417068481445
[Epoch 0][Step 32], time=0.053635358810424805, ext_time=0.010906696319580078, train_time=0.03316473960876465
[Epoch 0][Step 33], time=0.0534207820892334, ext_time=0.01134347915649414, train_time=0.03272819519042969
[Epoch 0][Step 34], time=0.053261518478393555, ext_time=0.011487007141113281, train_time=0.03231978416442871
[Epoch 0][Step 35], time=0.05341506004333496, ext_time=0.011540651321411133, train_time=0.032509565353393555
[Epoch 0][Step 36], time=0.053853511810302734, ext_time=0.011716842651367188, train_time=0.032672882080078125
[Epoch 0][Step 37], time=0.05442953109741211, ext_time=0.011427164077758789, train_time=0.033707380294799805
[Epoch 0][Step 38], time=0.0540158748626709, ext_time=0.011502504348754883, train_time=0.03291678428649902
[Epoch 0][Step 39], time=0.05340123176574707, ext_time=0.011616945266723633, train_time=0.03240776062011719
[Epoch 0][Step 40], time=0.05442023277282715, ext_time=0.011638879776000977, train_time=0.03310728073120117
[Epoch 0][Step 41], time=0.05332589149475098, ext_time=0.011129140853881836, train_time=0.032848358154296875
[Epoch 0][Step 42], time=0.054369449615478516, ext_time=0.011488914489746094, train_time=0.033574819564819336
[Epoch 0][Step 43], time=0.05405449867248535, ext_time=0.011598587036132812, train_time=0.033013105392456055
[Epoch 0][Step 44], time=0.0533750057220459, ext_time=0.011217355728149414, train_time=0.0327908992767334
[Epoch 0][Step 45], time=0.05338764190673828, ext_time=0.011066198348999023, train_time=0.03303384780883789
[Epoch 0][Step 46], time=0.05332231521606445, ext_time=0.01172780990600586, train_time=0.03204989433288574
[Epoch 0][Step 47], time=0.05336952209472656, ext_time=0.011328458786010742, train_time=0.03258061408996582
[Epoch 0][Step 48], time=0.05352306365966797, ext_time=0.011415481567382812, train_time=0.032773494720458984
[Epoch 0][Step 49], time=0.053383588790893555, ext_time=0.011667013168334961, train_time=0.03219914436340332
[Epoch 0][Step 50], time=0.053574323654174805, ext_time=0.011229515075683594, train_time=0.03299403190612793
[Epoch 0][Step 51], time=0.0538325309753418, ext_time=0.011264324188232422, train_time=0.03332042694091797
[Epoch 0][Step 52], time=0.053619384765625, ext_time=0.011301040649414062, train_time=0.03289604187011719
[Epoch 0][Step 53], time=0.053504228591918945, ext_time=0.01150822639465332, train_time=0.03255510330200195
[Epoch 0][Step 54], time=0.05400967597961426, ext_time=0.011263608932495117, train_time=0.03333163261413574
[Epoch 0][Step 55], time=0.05373692512512207, ext_time=0.0119476318359375, train_time=0.031412363052368164
[Epoch 0][Step 56], time=0.05329442024230957, ext_time=0.011525630950927734, train_time=0.03242683410644531
[Epoch 0][Step 57], time=0.05656099319458008, ext_time=0.011042594909667969, train_time=0.03623676300048828
[Epoch 0][Step 58], time=0.05367898941040039, ext_time=0.011220216751098633, train_time=0.033119916915893555
[Epoch 0][Step 59], time=0.05287671089172363, ext_time=0.011276960372924805, train_time=0.03221321105957031
[Epoch 0][Step 60], time=0.05333280563354492, ext_time=0.011131048202514648, train_time=0.03287911415100098
[Epoch 0][Step 61], time=0.05411267280578613, ext_time=0.011595964431762695, train_time=0.03304600715637207
[Epoch 0][Step 62], time=0.05394721031188965, ext_time=0.011218547821044922, train_time=0.033239126205444336
[Epoch 0][Step 63], time=0.05340147018432617, ext_time=0.011227846145629883, train_time=0.032880544662475586
[Epoch 0][Step 64], time=0.05385899543762207, ext_time=0.010883569717407227, train_time=0.033730506896972656
[Epoch 0][Step 65], time=0.053400278091430664, ext_time=0.011064291000366211, train_time=0.033153533935546875
[Epoch 0][Step 66], time=0.053503990173339844, ext_time=0.011384010314941406, train_time=0.03276538848876953
[Epoch 0][Step 67], time=0.05369138717651367, ext_time=0.011403322219848633, train_time=0.03290104866027832
[Epoch 0][Step 68], time=0.05346369743347168, ext_time=0.011299371719360352, train_time=0.0329132080078125
[Epoch 0][Step 69], time=0.056665658950805664, ext_time=0.011272907257080078, train_time=0.03610944747924805
[Epoch 0][Step 70], time=0.05389404296875, ext_time=0.011332035064697266, train_time=0.03306722640991211
[Epoch 0][Step 71], time=0.05427885055541992, ext_time=0.011373758316040039, train_time=0.03348684310913086
[Epoch 0][Step 72], time=0.053143978118896484, ext_time=0.011238336563110352, train_time=0.03245258331298828
[Epoch 0][Step 73], time=0.05365109443664551, ext_time=0.01131129264831543, train_time=0.0329434871673584
[Epoch 0][Step 74], time=0.05383181571960449, ext_time=0.011088848114013672, train_time=0.033393144607543945
[Epoch 0], time=5.16360068321228, loss=4.942997932434082
[Epoch 1][Step 0], time=0.055924415588378906, ext_time=0.012137889862060547, train_time=0.033950090408325195
[Epoch 1][Step 1], time=0.05378413200378418, ext_time=0.010991811752319336, train_time=0.03342413902282715
[Epoch 1][Step 2], time=0.05373382568359375, ext_time=0.01141357421875, train_time=0.03298163414001465
[Epoch 1][Step 3], time=0.05372810363769531, ext_time=0.011609077453613281, train_time=0.03263115882873535
[Epoch 1][Step 4], time=0.053949832916259766, ext_time=0.011464118957519531, train_time=0.03316330909729004
[Epoch 1][Step 5], time=0.05397486686706543, ext_time=0.011663198471069336, train_time=0.03236055374145508
[Epoch 1][Step 6], time=0.05388331413269043, ext_time=0.011157035827636719, train_time=0.03345823287963867
[Epoch 1][Step 7], time=0.053109169006347656, ext_time=0.011467933654785156, train_time=0.032241106033325195
[Epoch 1][Step 8], time=0.0531916618347168, ext_time=0.011436223983764648, train_time=0.032424211502075195
[Epoch 1][Step 9], time=0.05356264114379883, ext_time=0.011465311050415039, train_time=0.032712459564208984
[Epoch 1][Step 10], time=0.05368208885192871, ext_time=0.011139392852783203, train_time=0.03320813179016113
[Epoch 1][Step 11], time=0.053324222564697266, ext_time=0.011281967163085938, train_time=0.03276658058166504
[Epoch 1][Step 12], time=0.05361175537109375, ext_time=0.011192798614501953, train_time=0.033075809478759766
[Epoch 1][Step 13], time=0.05316877365112305, ext_time=0.011368036270141602, train_time=0.03236818313598633
[Epoch 1][Step 14], time=0.054312705993652344, ext_time=0.011286020278930664, train_time=0.033687591552734375
[Epoch 1][Step 15], time=0.053296566009521484, ext_time=0.011054277420043945, train_time=0.03286123275756836
[Epoch 1][Step 16], time=0.05295729637145996, ext_time=0.010840415954589844, train_time=0.03274083137512207
[Epoch 1][Step 17], time=0.053575992584228516, ext_time=0.01118612289428711, train_time=0.03308820724487305
[Epoch 1][Step 18], time=0.05371832847595215, ext_time=0.011413097381591797, train_time=0.03294944763183594
[Epoch 1][Step 19], time=0.05394315719604492, ext_time=0.011238813400268555, train_time=0.03334379196166992
[Epoch 1][Step 20], time=0.05390310287475586, ext_time=0.011112213134765625, train_time=0.03340578079223633
[Epoch 1][Step 21], time=0.053717851638793945, ext_time=0.011441469192504883, train_time=0.03263258934020996
[Epoch 1][Step 22], time=0.053086280822753906, ext_time=0.011432647705078125, train_time=0.03229856491088867
[Epoch 1][Step 23], time=0.0531306266784668, ext_time=0.011199951171875, train_time=0.03255605697631836
[Epoch 1][Step 24], time=0.05372905731201172, ext_time=0.01142430305480957, train_time=0.03297591209411621
[Epoch 1][Step 25], time=0.05357027053833008, ext_time=0.011401653289794922, train_time=0.03282046318054199
[Epoch 1][Step 26], time=0.05405855178833008, ext_time=0.011216163635253906, train_time=0.03348112106323242
[Epoch 1][Step 27], time=0.0540308952331543, ext_time=0.011147737503051758, train_time=0.03357434272766113
[Epoch 1][Step 28], time=0.0540924072265625, ext_time=0.011427640914916992, train_time=0.03322291374206543
[Epoch 1][Step 29], time=0.05372476577758789, ext_time=0.011586189270019531, train_time=0.03282022476196289
[Epoch 1][Step 30], time=0.0538945198059082, ext_time=0.011716127395629883, train_time=0.0327610969543457
[Epoch 1][Step 31], time=0.05398106575012207, ext_time=0.010853290557861328, train_time=0.03377866744995117
[Epoch 1][Step 32], time=0.05320096015930176, ext_time=0.010872364044189453, train_time=0.03293800354003906
[Epoch 1][Step 33], time=0.053205251693725586, ext_time=0.011398553848266602, train_time=0.032465457916259766
[Epoch 1][Step 34], time=0.052788496017456055, ext_time=0.011374473571777344, train_time=0.03201580047607422
[Epoch 1][Step 35], time=0.053327083587646484, ext_time=0.011559247970581055, train_time=0.03241705894470215
[Epoch 1][Step 36], time=0.05343985557556152, ext_time=0.011642932891845703, train_time=0.03231024742126465
[Epoch 1][Step 37], time=0.05404305458068848, ext_time=0.011341094970703125, train_time=0.03339409828186035
[Epoch 1][Step 38], time=0.05407071113586426, ext_time=0.011673212051391602, train_time=0.032814979553222656
[Epoch 1][Step 39], time=0.053244590759277344, ext_time=0.0116119384765625, train_time=0.0322108268737793
[Epoch 1][Step 40], time=0.05432891845703125, ext_time=0.011601924896240234, train_time=0.033236026763916016
[Epoch 1][Step 41], time=0.05332231521606445, ext_time=0.011097431182861328, train_time=0.03291153907775879
[Epoch 1][Step 42], time=0.054326534271240234, ext_time=0.011466741561889648, train_time=0.033547401428222656
[Epoch 1][Step 43], time=0.05378222465515137, ext_time=0.011603355407714844, train_time=0.032801151275634766
[Epoch 1][Step 44], time=0.053349971771240234, ext_time=0.011221647262573242, train_time=0.03278541564941406
[Epoch 1][Step 45], time=0.05365395545959473, ext_time=0.011115789413452148, train_time=0.033203125
[Epoch 1][Step 46], time=0.05323314666748047, ext_time=0.011685371398925781, train_time=0.03209686279296875
[Epoch 1][Step 47], time=0.053316593170166016, ext_time=0.011314868927001953, train_time=0.032532453536987305
[Epoch 1][Step 48], time=0.05368852615356445, ext_time=0.011423349380493164, train_time=0.03295087814331055
[Epoch 1][Step 49], time=0.05321669578552246, ext_time=0.01164388656616211, train_time=0.03222942352294922
[Epoch 1][Step 50], time=0.0535275936126709, ext_time=0.011258840560913086, train_time=0.03292036056518555
[Epoch 1][Step 51], time=0.053985595703125, ext_time=0.011262178421020508, train_time=0.03345966339111328
[Epoch 1][Step 52], time=0.053914546966552734, ext_time=0.011330604553222656, train_time=0.033174753189086914
[Epoch 1][Step 53], time=0.0534822940826416, ext_time=0.011425495147705078, train_time=0.03254342079162598
[Epoch 1][Step 54], time=0.05372428894042969, ext_time=0.011354923248291016, train_time=0.03288626670837402
[Epoch 1][Step 55], time=0.05342912673950195, ext_time=0.01184535026550293, train_time=0.032052040100097656
[Epoch 1][Step 56], time=0.05296945571899414, ext_time=0.011518001556396484, train_time=0.032090187072753906
[Epoch 1][Step 57], time=0.05394887924194336, ext_time=0.011075258255004883, train_time=0.03356671333312988
[Epoch 1][Step 58], time=0.0539708137512207, ext_time=0.011324167251586914, train_time=0.03329300880432129
[Epoch 1][Step 59], time=0.0532076358795166, ext_time=0.011337518692016602, train_time=0.032440900802612305
[Epoch 1][Step 60], time=0.05371212959289551, ext_time=0.011174201965332031, train_time=0.0332188606262207
[Epoch 1][Step 61], time=0.05429816246032715, ext_time=0.01162409782409668, train_time=0.033209800720214844
[Epoch 1][Step 62], time=0.05378007888793945, ext_time=0.011217594146728516, train_time=0.03314995765686035
[Epoch 1][Step 63], time=0.053272247314453125, ext_time=0.011205434799194336, train_time=0.03269028663635254
[Epoch 1][Step 64], time=0.05416059494018555, ext_time=0.010929584503173828, train_time=0.03396773338317871
[Epoch 1][Step 65], time=0.05365157127380371, ext_time=0.011054754257202148, train_time=0.033360958099365234
[Epoch 1][Step 66], time=0.056250572204589844, ext_time=0.011394023895263672, train_time=0.0354924201965332
[Epoch 1][Step 67], time=0.05439162254333496, ext_time=0.011415958404541016, train_time=0.03358268737792969
[Epoch 1][Step 68], time=0.053833961486816406, ext_time=0.011277437210083008, train_time=0.033164024353027344
[Epoch 1][Step 69], time=0.053646087646484375, ext_time=0.011258602142333984, train_time=0.033075809478759766
[Epoch 1][Step 70], time=0.05430412292480469, ext_time=0.011305093765258789, train_time=0.0336148738861084
[Epoch 1][Step 71], time=0.05452275276184082, ext_time=0.011386632919311523, train_time=0.03371906280517578
[Epoch 1][Step 72], time=0.05360269546508789, ext_time=0.011171102523803711, train_time=0.03295445442199707
[Epoch 1][Step 73], time=0.05397677421569824, ext_time=0.011356592178344727, train_time=0.03328061103820801
[Epoch 1][Step 74], time=0.05467963218688965, ext_time=0.01111602783203125, train_time=0.034200191497802734
[Epoch 1], time=4.036884546279907, loss=4.780628681182861
[Epoch 2][Step 0], time=0.055901288986206055, ext_time=0.012180328369140625, train_time=0.03391695022583008
[Epoch 2][Step 1], time=0.0540621280670166, ext_time=0.010993480682373047, train_time=0.033721923828125
[Epoch 2][Step 2], time=0.054708242416381836, ext_time=0.011430025100708008, train_time=0.033953189849853516
[Epoch 2][Step 3], time=0.05352282524108887, ext_time=0.011678218841552734, train_time=0.03186821937561035
[Epoch 2][Step 4], time=0.05384421348571777, ext_time=0.011461734771728516, train_time=0.03299689292907715
[Epoch 2][Step 5], time=0.05485653877258301, ext_time=0.011620521545410156, train_time=0.03369617462158203
[Epoch 2][Step 6], time=0.05431413650512695, ext_time=0.011130094528198242, train_time=0.03388047218322754
[Epoch 2][Step 7], time=0.05362844467163086, ext_time=0.011479616165161133, train_time=0.03270459175109863
[Epoch 2][Step 8], time=0.05313730239868164, ext_time=0.011416196823120117, train_time=0.03237748146057129
[Epoch 2][Step 9], time=0.05406498908996582, ext_time=0.011452674865722656, train_time=0.03246045112609863
[Epoch 2][Step 10], time=0.054355621337890625, ext_time=0.011183500289916992, train_time=0.033798933029174805
[Epoch 2][Step 11], time=0.05396413803100586, ext_time=0.011296749114990234, train_time=0.03331303596496582
[Epoch 2][Step 12], time=0.054155588150024414, ext_time=0.011233329772949219, train_time=0.03359246253967285
[Epoch 2][Step 13], time=0.05376935005187988, ext_time=0.011322736740112305, train_time=0.03305792808532715
[Epoch 2][Step 14], time=0.05495572090148926, ext_time=0.011294126510620117, train_time=0.0342559814453125
[Epoch 2][Step 15], time=0.053968191146850586, ext_time=0.011095285415649414, train_time=0.03339242935180664
[Epoch 2][Step 16], time=0.05350685119628906, ext_time=0.010880231857299805, train_time=0.03323626518249512
[Epoch 2][Step 17], time=0.05416727066040039, ext_time=0.011137247085571289, train_time=0.03375053405761719
[Epoch 2][Step 18], time=0.05412459373474121, ext_time=0.011303424835205078, train_time=0.03347635269165039
[Epoch 2][Step 19], time=0.05431985855102539, ext_time=0.011144876480102539, train_time=0.033831119537353516
[Epoch 2][Step 20], time=0.054198503494262695, ext_time=0.011149883270263672, train_time=0.03363537788391113
[Epoch 2][Step 21], time=0.054114580154418945, ext_time=0.011482477188110352, train_time=0.033087730407714844
[Epoch 2][Step 22], time=0.05364990234375, ext_time=0.011394500732421875, train_time=0.03288102149963379
[Epoch 2][Step 23], time=0.05361223220825195, ext_time=0.011174440383911133, train_time=0.033043622970581055
[Epoch 2][Step 24], time=0.05409717559814453, ext_time=0.011407852172851562, train_time=0.03337240219116211
[Epoch 2][Step 25], time=0.05400800704956055, ext_time=0.011391401290893555, train_time=0.03330254554748535
[Epoch 2][Step 26], time=0.05433487892150879, ext_time=0.01119375228881836, train_time=0.03373599052429199
[Epoch 2][Step 27], time=0.054296016693115234, ext_time=0.01123499870300293, train_time=0.03373551368713379
[Epoch 2][Step 28], time=0.054581642150878906, ext_time=0.011453390121459961, train_time=0.03365898132324219
[Epoch 2][Step 29], time=0.05409359931945801, ext_time=0.011530876159667969, train_time=0.03320813179016113
[Epoch 2][Step 30], time=0.054204702377319336, ext_time=0.011700630187988281, train_time=0.033116817474365234
[Epoch 2][Step 31], time=0.05424761772155762, ext_time=0.010762691497802734, train_time=0.03420066833496094
[Epoch 2][Step 32], time=0.05395698547363281, ext_time=0.01087188720703125, train_time=0.03368639945983887
[Epoch 2][Step 33], time=0.05379295349121094, ext_time=0.011297225952148438, train_time=0.03311276435852051
[Epoch 2][Step 34], time=0.053568363189697266, ext_time=0.011335611343383789, train_time=0.03276419639587402
[Epoch 2][Step 35], time=0.05407977104187012, ext_time=0.011461496353149414, train_time=0.033228158950805664
[Epoch 2][Step 36], time=0.05382847785949707, ext_time=0.011623620986938477, train_time=0.032763004302978516
[Epoch 2][Step 37], time=0.05462479591369629, ext_time=0.011327028274536133, train_time=0.03400111198425293
[Epoch 2][Step 38], time=0.054234981536865234, ext_time=0.011512041091918945, train_time=0.03312993049621582
[Epoch 2][Step 39], time=0.053745269775390625, ext_time=0.011600255966186523, train_time=0.03276848793029785
[Epoch 2][Step 40], time=0.054619789123535156, ext_time=0.01164388656616211, train_time=0.03349447250366211
[Epoch 2][Step 41], time=0.05388355255126953, ext_time=0.011107444763183594, train_time=0.03345322608947754
[Epoch 2][Step 42], time=0.05466866493225098, ext_time=0.011501550674438477, train_time=0.033823251724243164
[Epoch 2][Step 43], time=0.05406594276428223, ext_time=0.011590719223022461, train_time=0.033049821853637695
[Epoch 2][Step 44], time=0.0537724494934082, ext_time=0.011215686798095703, train_time=0.033203840255737305
[Epoch 2][Step 45], time=0.05402731895446777, ext_time=0.011297464370727539, train_time=0.0334172248840332
[Epoch 2][Step 46], time=0.05344700813293457, ext_time=0.011749744415283203, train_time=0.03219246864318848
[Epoch 2][Step 47], time=0.053763389587402344, ext_time=0.011333703994750977, train_time=0.033010244369506836
[Epoch 2][Step 48], time=0.05411505699157715, ext_time=0.011467218399047852, train_time=0.03329801559448242
[Epoch 2][Step 49], time=0.05366683006286621, ext_time=0.011623620986938477, train_time=0.032648563385009766
[Epoch 2][Step 50], time=0.053980350494384766, ext_time=0.011202573776245117, train_time=0.03343462944030762
[Epoch 2][Step 51], time=0.054366350173950195, ext_time=0.011278629302978516, train_time=0.033811330795288086
[Epoch 2][Step 52], time=0.05423593521118164, ext_time=0.011306047439575195, train_time=0.03349423408508301
[Epoch 2][Step 53], time=0.05400705337524414, ext_time=0.011501789093017578, train_time=0.03300166130065918
[Epoch 2][Step 54], time=0.05421280860900879, ext_time=0.011290788650512695, train_time=0.033498525619506836
[Epoch 2][Step 55], time=0.0539393424987793, ext_time=0.01186060905456543, train_time=0.032502174377441406
[Epoch 2][Step 56], time=0.05345916748046875, ext_time=0.011495828628540039, train_time=0.03256988525390625
[Epoch 2][Step 57], time=0.0543515682220459, ext_time=0.011068105697631836, train_time=0.034012556076049805
[Epoch 2][Step 58], time=0.0544893741607666, ext_time=0.011256217956542969, train_time=0.03388237953186035
[Epoch 2][Step 59], time=0.05357837677001953, ext_time=0.011279821395874023, train_time=0.03289198875427246
[Epoch 2][Step 60], time=0.05381369590759277, ext_time=0.011208295822143555, train_time=0.03323507308959961
[Epoch 2][Step 61], time=0.05448722839355469, ext_time=0.011542081832885742, train_time=0.03347945213317871
[Epoch 2][Step 62], time=0.05416703224182129, ext_time=0.01125025749206543, train_time=0.03349661827087402
[Epoch 2][Step 63], time=0.05449366569519043, ext_time=0.011216402053833008, train_time=0.03399801254272461
[Epoch 2][Step 64], time=0.05441927909851074, ext_time=0.010930299758911133, train_time=0.03421592712402344
[Epoch 2][Step 65], time=0.05419421195983887, ext_time=0.011027097702026367, train_time=0.033928871154785156
[Epoch 2][Step 66], time=0.05416154861450195, ext_time=0.011390447616577148, train_time=0.03348827362060547
[Epoch 2][Step 67], time=0.0542140007019043, ext_time=0.011445283889770508, train_time=0.03327775001525879
[Epoch 2][Step 68], time=0.054038047790527344, ext_time=0.011240720748901367, train_time=0.033477067947387695
[Epoch 2][Step 69], time=0.05382394790649414, ext_time=0.011297225952148438, train_time=0.03321123123168945
[Epoch 2][Step 70], time=0.05423736572265625, ext_time=0.011240959167480469, train_time=0.03360748291015625
[Epoch 2][Step 71], time=0.05463576316833496, ext_time=0.011327505111694336, train_time=0.0338892936706543
[Epoch 2][Step 72], time=0.05338692665100098, ext_time=0.011171579360961914, train_time=0.0327448844909668
[Epoch 2][Step 73], time=0.05413627624511719, ext_time=0.011301755905151367, train_time=0.03353548049926758
[Epoch 2][Step 74], time=0.05426979064941406, ext_time=0.011147260665893555, train_time=0.03375983238220215
[Epoch 2], time=4.063561677932739, loss=4.655261039733887
[Epoch 3][Step 0], time=0.05585741996765137, ext_time=0.012175559997558594, train_time=0.03383278846740723
[Epoch 3][Step 1], time=0.05398702621459961, ext_time=0.011017560958862305, train_time=0.033606529235839844
[Epoch 3][Step 2], time=0.05388832092285156, ext_time=0.011466264724731445, train_time=0.0330967903137207
[Epoch 3][Step 3], time=0.053995609283447266, ext_time=0.011656999588012695, train_time=0.03283858299255371
[Epoch 3][Step 4], time=0.054291486740112305, ext_time=0.011404275894165039, train_time=0.033567190170288086
[Epoch 3][Step 5], time=0.053948402404785156, ext_time=0.011621236801147461, train_time=0.03281283378601074
[Epoch 3][Step 6], time=0.053896188735961914, ext_time=0.011142730712890625, train_time=0.03346705436706543
[Epoch 3][Step 7], time=0.053002357482910156, ext_time=0.011511087417602539, train_time=0.032088279724121094
[Epoch 3][Step 8], time=0.052732229232788086, ext_time=0.011366128921508789, train_time=0.03200674057006836
[Epoch 3][Step 9], time=0.05362701416015625, ext_time=0.011451482772827148, train_time=0.032849788665771484
[Epoch 3][Step 10], time=0.05383038520812988, ext_time=0.011178016662597656, train_time=0.033330440521240234
[Epoch 3][Step 11], time=0.05343437194824219, ext_time=0.01125478744506836, train_time=0.03288698196411133
[Epoch 3][Step 12], time=0.053678274154663086, ext_time=0.01108098030090332, train_time=0.03330540657043457
[Epoch 3][Step 13], time=0.053525686264038086, ext_time=0.011371135711669922, train_time=0.032784461975097656
[Epoch 3][Step 14], time=0.054173946380615234, ext_time=0.01130223274230957, train_time=0.03342127799987793
[Epoch 3][Step 15], time=0.05320477485656738, ext_time=0.011043310165405273, train_time=0.03279876708984375
[Epoch 3][Step 16], time=0.05290102958679199, ext_time=0.010911703109741211, train_time=0.032616615295410156
[Epoch 3][Step 17], time=0.05369067192077637, ext_time=0.011132240295410156, train_time=0.0333247184753418
[Epoch 3][Step 18], time=0.053679466247558594, ext_time=0.011463165283203125, train_time=0.0328669548034668
[Epoch 3][Step 19], time=0.05379056930541992, ext_time=0.011197328567504883, train_time=0.03323531150817871
[Epoch 3][Step 20], time=0.05376172065734863, ext_time=0.01111149787902832, train_time=0.03325796127319336
[Epoch 3][Step 21], time=0.053765296936035156, ext_time=0.011435270309448242, train_time=0.031384944915771484
[Epoch 3][Step 22], time=0.05311870574951172, ext_time=0.011375188827514648, train_time=0.03243064880371094
[Epoch 3][Step 23], time=0.053148746490478516, ext_time=0.011193990707397461, train_time=0.032561302185058594
[Epoch 3][Step 24], time=0.05376148223876953, ext_time=0.011450529098510742, train_time=0.033014535903930664
[Epoch 3][Step 25], time=0.05349993705749512, ext_time=0.011373281478881836, train_time=0.03277015686035156
[Epoch 3][Step 26], time=0.05392813682556152, ext_time=0.011249542236328125, train_time=0.03332352638244629
[Epoch 3][Step 27], time=0.053716421127319336, ext_time=0.011190652847290039, train_time=0.03322958946228027
[Epoch 3][Step 28], time=0.05428314208984375, ext_time=0.011475086212158203, train_time=0.03331613540649414
[Epoch 3][Step 29], time=0.05387544631958008, ext_time=0.011522054672241211, train_time=0.03304147720336914
[Epoch 3][Step 30], time=0.05317378044128418, ext_time=0.011785745620727539, train_time=0.031783342361450195
[Epoch 3][Step 31], time=0.053789377212524414, ext_time=0.010826587677001953, train_time=0.0336298942565918
[Epoch 3][Step 32], time=0.053479909896850586, ext_time=0.010930299758911133, train_time=0.03315854072570801
[Epoch 3][Step 33], time=0.05301856994628906, ext_time=0.011302709579467773, train_time=0.03236269950866699
[Epoch 3][Step 34], time=0.05307435989379883, ext_time=0.011312007904052734, train_time=0.032379150390625
[Epoch 3][Step 35], time=0.05351543426513672, ext_time=0.011498212814331055, train_time=0.03266000747680664
[Epoch 3][Step 36], time=0.05337953567504883, ext_time=0.011646270751953125, train_time=0.032303810119628906
[Epoch 3][Step 37], time=0.053870201110839844, ext_time=0.011326313018798828, train_time=0.03325510025024414
[Epoch 3][Step 38], time=0.05364871025085449, ext_time=0.011536121368408203, train_time=0.03253364562988281
[Epoch 3][Step 39], time=0.053102731704711914, ext_time=0.011615276336669922, train_time=0.032097578048706055
[Epoch 3][Step 40], time=0.05434918403625488, ext_time=0.011567354202270508, train_time=0.03331708908081055
[Epoch 3][Step 41], time=0.05338335037231445, ext_time=0.011109352111816406, train_time=0.032990217208862305
[Epoch 3][Step 42], time=0.05449676513671875, ext_time=0.011496305465698242, train_time=0.03367018699645996
[Epoch 3][Step 43], time=0.05369305610656738, ext_time=0.011542081832885742, train_time=0.032723426818847656
[Epoch 3][Step 44], time=0.0533289909362793, ext_time=0.011234760284423828, train_time=0.03271150588989258
[Epoch 3][Step 45], time=0.05356264114379883, ext_time=0.011017322540283203, train_time=0.03322196006774902
[Epoch 3][Step 46], time=0.053116798400878906, ext_time=0.011705160140991211, train_time=0.032027244567871094
[Epoch 3][Step 47], time=0.05339479446411133, ext_time=0.011358261108398438, train_time=0.03257107734680176
[Epoch 3][Step 48], time=0.05376291275024414, ext_time=0.011399030685424805, train_time=0.03302168846130371
[Epoch 3][Step 49], time=0.0531919002532959, ext_time=0.011614561080932617, train_time=0.03218674659729004
[Epoch 3][Step 50], time=0.05351090431213379, ext_time=0.011213302612304688, train_time=0.03294873237609863
[Epoch 3][Step 51], time=0.05386829376220703, ext_time=0.011312723159790039, train_time=0.03329777717590332
[Epoch 3][Step 52], time=0.05370020866394043, ext_time=0.011292695999145508, train_time=0.03301119804382324
[Epoch 3][Step 53], time=0.05402350425720215, ext_time=0.011470556259155273, train_time=0.03311657905578613
[Epoch 3][Step 54], time=0.053850412368774414, ext_time=0.01133871078491211, train_time=0.03308749198913574
[Epoch 3][Step 55], time=0.053522586822509766, ext_time=0.011820793151855469, train_time=0.03219175338745117
[Epoch 3][Step 56], time=0.05322003364562988, ext_time=0.011573076248168945, train_time=0.03227806091308594
[Epoch 3][Step 57], time=0.054131507873535156, ext_time=0.01105642318725586, train_time=0.033774614334106445
[Epoch 3][Step 58], time=0.05404210090637207, ext_time=0.011268377304077148, train_time=0.03343057632446289
[Epoch 3][Step 59], time=0.0530545711517334, ext_time=0.01128244400024414, train_time=0.032363176345825195
[Epoch 3][Step 60], time=0.0534970760345459, ext_time=0.01116800308227539, train_time=0.03301525115966797
[Epoch 3][Step 61], time=0.05373883247375488, ext_time=0.01155853271484375, train_time=0.03270888328552246
[Epoch 3][Step 62], time=0.05333447456359863, ext_time=0.011233091354370117, train_time=0.03271603584289551
[Epoch 3][Step 63], time=0.053101539611816406, ext_time=0.011237621307373047, train_time=0.032578468322753906
    [Step(average) Profiler Level 1 E3 S299]
        L1  sample           0.010321 | send           0.000000
        L1  recv             0.000000 | copy           0.011869 | convert time 0.000000 | train  0.031651
        L1  feature nbytes  715.94 MB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes    0.00 Bytes | remote nbytes 0.00 Bytes
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S299]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.011869
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S299]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.001026 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.000000 | cache combine cache 0.010809 | cache combine remote 0.000000
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S299]
        p50.00_tail_logl2featcopy=0.012124
        p90.00_tail_logl2featcopy=0.012956
        p95.00_tail_logl2featcopy=0.013146
        p99.00_tail_logl2featcopy=0.013359
        p99.90_tail_logl2featcopy=0.016817
[CUDA] cuda: usage: 12.64 GB
Rank=3, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.336942195892334
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 8.58306884765625e-06, transfer latency: 1.3601996898651123
presamping
presamping takes 6.282694101333618
Rank=1, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.358489751815796
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 1.1920928955078125e-05, transfer latency: 1.380648136138916
presamping
presamping takes 6.5874505043029785
[Epoch 3][Step 64], time=0.0539858341217041, ext_time=0.010926246643066406, train_time=0.03379058837890625
[Epoch 3][Step 65], time=0.053605079650878906, ext_time=0.011009931564331055, train_time=0.033292531967163086
[Epoch 3][Step 66], time=0.05367898941040039, ext_time=0.011315584182739258, train_time=0.03303360939025879
[Epoch 3][Step 67], time=0.053765296936035156, ext_time=0.011383295059204102, train_time=0.033026695251464844
[Epoch 3][Step 68], time=0.05348682403564453, ext_time=0.011244773864746094, train_time=0.03294563293457031
[Epoch 3][Step 69], time=0.05315542221069336, ext_time=0.011300086975097656, train_time=0.03252983093261719
[Epoch 3][Step 70], time=0.05375266075134277, ext_time=0.01128697395324707, train_time=0.03299450874328613
[Epoch 3][Step 71], time=0.05405902862548828, ext_time=0.011337995529174805, train_time=0.03330516815185547
[Epoch 3][Step 72], time=0.05282092094421387, ext_time=0.01114511489868164, train_time=0.032217979431152344
[Epoch 3][Step 73], time=0.053839683532714844, ext_time=0.011326074600219727, train_time=0.03321552276611328
[Epoch 3][Step 74], time=0.053610801696777344, ext_time=0.011048316955566406, train_time=0.03321123123168945
[Epoch 3], time=4.028484106063843, loss=4.559545040130615
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  128780 KB |    1683 MB |    1973 GB |    1973 GB |
|       from large pool |  123860 KB |    1679 MB |    1969 GB |    1969 GB |
|       from small pool |    4920 KB |       7 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Active memory         |  128780 KB |    1683 MB |    1973 GB |    1973 GB |
|       from large pool |  123860 KB |    1679 MB |    1969 GB |    1969 GB |
|       from small pool |    4920 KB |       7 MB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2494 MB |    2494 MB |    2494 MB |       0 B  |
|       from large pool |    2484 MB |    2484 MB |    2484 MB |       0 B  |
|       from small pool |      10 MB |      10 MB |      10 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  538868 KB |    1496 MB |    1048 GB |    1047 GB |
|       from large pool |  535596 KB |    1493 MB |    1043 GB |    1042 GB |
|       from small pool |    3272 KB |       4 MB |       5 GB |       5 GB |
|---------------------------------------------------------------------------|
| Allocations           |      66    |      95    |   82236    |   82170    |
|       from large pool |      23    |      43    |   39902    |   39879    |
|       from small pool |      43    |      54    |   42334    |   42291    |
|---------------------------------------------------------------------------|
| Active allocs         |      66    |      95    |   82236    |   82170    |
|       from large pool |      23    |      43    |   39902    |   39879    |
|       from small pool |      43    |      54    |   42334    |   42291    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      22    |      22    |      22    |       0    |
|       from large pool |      17    |      17    |      17    |       0    |
|       from small pool |       5    |       5    |       5    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      33    |      45    |   34423    |   34390    |
|       from large pool |      17    |      26    |   22310    |   22293    |
|       from small pool |      16    |      25    |   12113    |   12097    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 17.293331 seconds
[EPOCH_TIME] 4.323333 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 4.046144 seconds
Rank=2, Graph loaded.
!!!!Train_dataloader(with 75 items) enumerate latency: 1.336836814880371
torch.Size([4000]) torch.Size([4000, 1])
torch.Size([4200]) torch.Size([4200, 1])
!!!!Train_data_list(with 75 items) enumerate latency: 7.3909759521484375e-06, transfer latency: 1.3547046184539795
presamping
presamping takes 5.393365144729614

