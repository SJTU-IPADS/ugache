succeed=True
[CUDA] cuda: usage: 5.32 GB
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID
Set parameter TimeLimit to value 200
Set parameter MIPGap to value 0.05
Set parameter ScaleFlag to value 1
Set parameter DegenMoves to value 2
Set parameter Cuts to value 2
Set parameter LogFile to value "cppsolver.log"
Set parameter Threads to value 48
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)
Thread count: 48 physical cores, 96 logical processors, using up to 48 threads
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Optimize a model with 101440 rows, 15753 columns and 255032 nonzeros
Model fingerprint: 0xf8436725
Variable types: 9 continuous, 15744 integer (15744 binary)
Coefficient statistics:
  Matrix range     [8e-09, 1e+05]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 1e+05]
Warning: Model contains large matrix coefficient range
         Consider reformulating model or setting NumericFocus parameter
         to avoid numerical issues.
Found heuristic solution: objective 2.000000e+09
Presolve removed 86030 rows and 7 columns
Presolve time: 0.24s
Presolved: 15410 rows, 15746 columns, 74995 nonzeros
Found heuristic solution: objective 659836.08408
Variable types: 1 continuous, 15745 integer (15744 binary)

Root relaxation: objective 1.694358e+04, 21718 iterations, 3.25 seconds (3.69 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0 16943.5842    0 6673 659836.084 16943.5842  97.4%     -    3s
H    0     0                    284652.52748 16943.5842  94.0%     -    3s
H    0     0                    281483.46492 16943.5842  94.0%     -    3s
H    0     0                    48914.753888 16943.5842  65.4%     -    3s
H    0     0                    42130.439139 16943.5842  59.8%     -    7s
H    0     0                    38606.966905 16943.5842  56.1%     -    7s
H    0     0                    38287.814920 17427.0513  54.5%     -    9s
H    0     0                    35717.651773 17427.0513  51.2%     -    9s
H    0     0                    35397.809133 17427.0513  50.8%     -    9s
     0     0 17452.3564    0 4643 35397.8091 17452.3564  50.7%     -   10s
H    0     0                    32507.418007 17452.3564  46.3%     -   11s
H    0     0                    28252.373190 17512.3741  38.0%     -   12s
H    0     0                    27306.482923 17512.3741  35.9%     -   12s
     0     0 17512.3741    0 4828 27306.4829 17512.3741  35.9%     -   12s
H    0     0                    27125.889854 17665.4753  34.9%     -   17s
H    0     0                    26897.348945 17665.4753  34.3%     -   17s
H    0     0                    25917.405162 17665.4753  31.8%     -   17s
H    0     0                    25629.795972 17665.4753  31.1%     -   17s
H    0     0                    25401.036968 17665.4753  30.5%     -   17s
H    0     0                    25310.755889 17665.4753  30.2%     -   17s
H    0     0                    25220.336735 17665.4753  30.0%     -   17s
H    0     0                    24857.431840 17665.4753  28.9%     -   17s
     0     0 17665.4753    0 4547 24857.4318 17665.4753  28.9%     -   17s
H    0     0                    22281.043570 17693.5054  20.6%     -   22s
H    0     0                    21945.541595 17693.5054  19.4%     -   22s
H    0     0                    21866.110817 17693.5054  19.1%     -   22s
H    0     0                    21729.365631 17693.5054  18.6%     -   22s
     0     0 17697.9814    0 4973 21729.3656 17697.9814  18.6%     -   22s
H    0     0                    21422.453076 17697.9814  17.4%     -   24s
     0     0 17698.5144    0 5030 21422.4531 17698.5144  17.4%     -   24s
     0     0 17699.5869    0 5090 21422.4531 17699.5869  17.4%     -   26s
H    0     0                    21335.350279 17700.8179  17.0%     -   28s
     0     0 17702.0342    0 5061 21335.3503 17702.0342  17.0%     -   28s
     0     0 17702.0342    0 4908 21335.3503 17702.0342  17.0%     -   29s
     0     0 17702.0342    0 4889 21335.3503 17702.0342  17.0%     -   29s
     0     0 17702.0342    0 4906 21335.3503 17702.0342  17.0%     -   29s
     0     0 17702.0342    0 4927 21335.3503 17702.0342  17.0%     -   29s
H    0     0                    19281.625375 17914.9630  7.09%     -   42s
H    0     0                    19279.502009 17914.9630  7.08%     -   42s
H    0     0                    19210.491707 17914.9630  6.74%     -   42s
H    0     0                    19148.410467 17914.9630  6.44%     -   42s
     0     0 17914.9630    0 1418 19148.4105 17914.9630  6.44%     -   42s
     0     0 17922.8324    0 1196 19148.4105 17922.8324  6.40%     -   43s
     0     0 17922.8324    0 1190 19148.4105 17922.8324  6.40%     -   43s
     0     0 18002.4490    0  758 19148.4105 18002.4490  5.98%     -   46s
H    0     0                    18611.375476 18002.4490  3.27%     -   52s

Cutting planes:
  Gomory: 86
  Lift-and-project: 145
  Flow cover: 1
  Zero half: 3820

Explored 1 nodes (87059 simplex iterations) in 53.00 seconds (46.58 work units)
Thread count was 48 (of 96 available processors)

Solution count 10: 18611.4 19148.4 19210.5 ... 21945.5

Optimal solution found (tolerance 5.00e-02)
Best objective 1.861137547580e+04, best bound 1.800244900120e+04, gap 3.2718%
coll_cache:optimal_local_rate=0.630941,0.636308,0.651269,0.613583,0.659875,0.638625,0.633576,0.618519,
coll_cache:optimal_remote_rate=0.363476,0.358109,0.343147,0.380833,0.334542,0.355792,0.36084,0.375897,
coll_cache:optimal_cpu_rate=0.00558334,0.00558334,0.00558334,0.00558334,0.00558334,0.00558334,0.00558334,0.00558334,
z=18611.4
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
test_result:init:feat_nbytes=67182966784
test_result:init:cache_nbytes=16862923776
    [Step(average) Profiler Level 1 E3 S999]
        L1  sample           0.008252 | send           0.000000
        L1  recv             0.000000 | copy           0.016590 | convert time 0.000000 | train  0.018287
        L1  feature nbytes    2.21 GB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes      18.13 MB | remote nbytes  810.39 MB
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S999]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.016590
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S999]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.001946 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.004238 | cache combine cache 0.003322 | cache combine remote 0.011261
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S999]
        p50.00_tail_logl2featcopy=0.016782
        p90.00_tail_logl2featcopy=0.017110
        p95.00_tail_logl2featcopy=0.017151
        p99.00_tail_logl2featcopy=0.017486
        p99.90_tail_logl2featcopy=0.040365
[CUDA] cuda: usage: 31.42 GB
Rank=5, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.007563, per step: 0.000061
presamping
presamping takes 19.600682497024536
creating_intra_node_communicator root=4, local_size=4, world_size=8
Rank=4, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.007406, per step: 0.000059
presamping
presamping takes 19.06167721748352
Rank=2, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.009015, per step: 0.000072
presamping
presamping takes 19.184480905532837
Rank=6, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.006737, per step: 0.000054
presamping
presamping takes 18.97177267074585
Rank=3, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.012255, per step: 0.000098
presamping
presamping takes 19.74281907081604
config:eval_tsp="2023-04-14 07:23:53"
config:num_worker=8
config:num_intra_size=4
config:root_dir=/nvme/songxiaoniu/graph-learning/wholegraph
config:graph_name=com-friendster
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=10,25
config:hiddensize=256
config:num_layer=2
config:model=sage
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.25
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=48
config:unsupervised=True
config:classnum=100
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7f8390ab73d0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=8
Rank=0, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005733, per step: 0.000046
epoch=4 total_steps=500
presamping
presamping takes 19.048559427261353
start training...
[Epoch 0], time=7.077716827392578, loss=0.6931472420692444
[Epoch 1], time=5.396440505981445, loss=0.6931472420692444
[Epoch 2], time=5.406419038772583, loss=0.6931472420692444
[Epoch 3], time=5.396245002746582, loss=0.6931472420692444
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  150745 KB |    1428 MB |    1551 GB |    1551 GB |
|       from large pool |  140345 KB |    1418 MB |    1537 GB |    1537 GB |
|       from small pool |   10400 KB |      14 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Active memory         |  150745 KB |    1428 MB |    1551 GB |    1551 GB |
|       from large pool |  140345 KB |    1418 MB |    1537 GB |    1537 GB |
|       from small pool |   10400 KB |      14 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    4960 MB |    4960 MB |    4960 MB |       0 B  |
|       from large pool |    4940 MB |    4940 MB |    4940 MB |       0 B  |
|       from small pool |      20 MB |      20 MB |      20 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  144167 KB |    1220 MB |    1309 GB |    1309 GB |
|       from large pool |  136135 KB |    1213 MB |    1295 GB |    1295 GB |
|       from small pool |    8032 KB |       9 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| Allocations           |      58    |      74    |  102449    |  102391    |
|       from large pool |      12    |      22    |   32281    |   32269    |
|       from small pool |      46    |      57    |   70168    |   70122    |
|---------------------------------------------------------------------------|
| Active allocs         |      58    |      74    |  102449    |  102391    |
|       from large pool |      12    |      22    |   32281    |   32269    |
|       from small pool |      46    |      57    |   70168    |   70122    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      29    |      29    |      29    |       0    |
|       from large pool |      19    |      19    |      19    |       0    |
|       from small pool |      10    |      10    |      10    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      32    |      43    |   36634    |   36602    |
|       from large pool |      10    |      19    |   18167    |   18157    |
|       from small pool |      22    |      29    |   18467    |   18445    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 23.278478 seconds
[EPOCH_TIME] 5.819620 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 5.401569 seconds
Rank=7, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005762, per step: 0.000046
presamping
presamping takes 17.977015256881714
Rank=1, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.012191, per step: 0.000098
presamping
presamping takes 19.571000814437866

