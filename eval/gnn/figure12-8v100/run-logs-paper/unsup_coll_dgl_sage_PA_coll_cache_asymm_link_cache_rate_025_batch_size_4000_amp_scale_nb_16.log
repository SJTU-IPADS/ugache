succeed=True
[CUDA] cuda: usage: 5.39 GB
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
0 : local 72, cpu 8 {link #0 : g3 24}, {link #1 : g6 24}, {link #2 : g1 12}, {link #3 : g2 12},
1 : local 72, cpu 8 {link #0 : g7 24}, {link #1 : g2 24}, {link #2 : g3 12}, {link #3 : g0 12},
2 : local 72, cpu 8 {link #0 : g1 24}, {link #1 : g3 24}, {link #2 : g0 12}, {link #3 : g4 12},
3 : local 72, cpu 8 {link #0 : g2 24}, {link #1 : g0 24}, {link #2 : g5 12}, {link #3 : g1 12},
4 : local 72, cpu 8 {link #0 : g5 24}, {link #1 : g7 24}, {link #2 : g2 12}, {link #3 : g6 12},
5 : local 72, cpu 8 {link #0 : g6 24}, {link #1 : g4 24}, {link #2 : g7 12}, {link #3 : g3 12},
6 : local 72, cpu 8 {link #0 : g0 24}, {link #1 : g5 24}, {link #2 : g4 12}, {link #3 : g7 12},
7 : local 72, cpu 8 {link #0 : g4 24}, {link #1 : g1 24}, {link #2 : g6 12}, {link #3 : g5 12},
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID
Set parameter TimeLimit to value 200
Set parameter MIPGap to value 0.05
Set parameter ScaleFlag to value 1
Set parameter DegenMoves to value 2
Set parameter Cuts to value 2
Set parameter LogFile to value "cppsolver.log"
Set parameter Threads to value 48
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Gurobi Optimizer version 9.5.2 build v9.5.2rc0 (linux64)
Thread count: 48 physical cores, 96 logical processors, using up to 48 threads
Academic license - for non-commercial use only - registered to xiaoniu.sxn@sjtu.edu.cn
Optimize a model with 131008 rows, 20345 columns and 329400 nonzeros
Model fingerprint: 0x1cad020a
Variable types: 9 continuous, 20336 integer (20336 binary)
Coefficient statistics:
  Matrix range     [5e-09, 5e+04]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 7e+04]
Warning: Model contains large matrix coefficient range
         Consider reformulating model or setting NumericFocus parameter
         to avoid numerical issues.
Found heuristic solution: objective 362493.86690
Presolve removed 111118 rows and 7 columns
Presolve time: 0.39s
Presolved: 19890 rows, 20338 columns, 96467 nonzeros
Variable types: 1 continuous, 20337 integer (20336 binary)

Root relaxation: objective 1.457721e+04, 25974 iterations, 3.36 seconds (3.79 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0 14577.2062    0 9561 362493.867 14577.2062  96.0%     -    4s
H    0     0                    296232.28175 14577.2062  95.1%     -    4s
H    0     0                    296180.03301 14577.2062  95.1%     -    4s
H    0     0                    153834.23927 14577.2062  90.5%     -    6s
     0     0 15296.0221    0 8228 153834.239 15296.0221  90.1%     -   10s
H    0     0                    153640.68235 15296.0221  90.0%     -   11s
H    0     0                    95382.724830 15296.0221  84.0%     -   11s
     0     0 15612.8371    0 7672 95382.7248 15612.8371  83.6%     -   13s
     0     0 15890.2057    0 7691 95382.7248 15890.2057  83.3%     -   16s
H    0     0                    91975.089218 16121.4471  82.5%     -   20s
H    0     0                    86327.463107 16121.4471  81.3%     -   20s
     0     0 16121.4471    0 7607 86327.4631 16121.4471  81.3%     -   20s
     0     0 16209.1205    0 7868 86327.4631 16209.1205  81.2%     -   24s
     0     0 16228.3599    0 7868 86327.4631 16228.3599  81.2%     -   26s
     0     0 16229.7085    0 7992 86327.4631 16229.7085  81.2%     -   27s
     0     0 16230.4852    0 8021 86327.4631 16230.4852  81.2%     -   27s
     0     0 16230.5869    0 8015 86327.4631 16230.5869  81.2%     -   28s
H    0     0                    77918.428740 16967.6568  78.2%     -   43s
H    0     0                    63167.621810 16967.6568  73.1%     -   43s
H    0     0                    61862.019079 16967.6568  72.6%     -   43s
H    0     0                    61555.506738 16967.6568  72.4%     -   43s
H    0     0                    55540.736686 16967.6568  69.5%     -   43s
H    0     0                    54843.395399 16967.6568  69.1%     -   43s
H    0     0                    49058.450743 16967.6568  65.4%     -   43s
H    0     0                    48751.246659 16967.6568  65.2%     -   43s
     0     0 16967.6568    0 4224 48751.2467 16967.6568  65.2%     -   43s
H    0     0                    42710.203893 17386.8924  59.3%     -   48s
H    0     0                    38998.056841 17386.8924  55.4%     -   48s
H    0     0                    38507.854956 17386.8924  54.8%     -   48s
H    0     0                    31940.158209 17386.8924  45.6%     -   48s
H    0     0                    31328.197791 17386.8924  44.5%     -   48s
     0     0 17386.8924    0 3047 31328.1978 17386.8924  44.5%     -   48s
H    0     0                    28278.210341 17442.8417  38.3%     -   50s
     0     0 17442.8417    0 2854 28278.2103 17442.8417  38.3%     -   51s
H    0     0                    26784.851854 17449.0463  34.9%     -   54s
     0     0 17449.0463    0 2713 26784.8519 17449.0463  34.9%     -   54s
     0     0 17449.3742    0 2712 26784.8519 17449.3742  34.9%     -   55s
H    0     0                    23932.282687 17449.3742  27.1%     -   58s
     0     0 17874.5660    0 1350 23932.2827 17874.5660  25.3%     -   61s
H    0     0                    21015.443855 17874.5660  14.9%     -   62s
     0     0 17936.1724    0 1130 21015.4439 17936.1724  14.7%     -   63s
     0     0 17939.0844    0 1116 21015.4439 17939.0844  14.6%     -   63s
     0     0 17941.5154    0 1094 21015.4439 17941.5154  14.6%     -   64s
     0     0 17987.4338    0  590 21015.4439 17987.4338  14.4%     -   70s
     0     0 17989.0179    0  567 21015.4439 17989.0179  14.4%     -   71s
     0     0 17993.5830    0  346 21015.4439 17993.5830  14.4%     -   75s
H    0     0                    18261.013468 17993.5830  1.46%     -   90s

Cutting planes:
  Gomory: 78
  Lift-and-project: 217
  Cover: 1
  MIR: 6
  StrongCG: 1
  Flow cover: 7
  Zero half: 4869

Explored 1 nodes (122054 simplex iterations) in 90.38 seconds (97.03 work units)
Thread count was 48 (of 96 available processors)

Solution count 10: 18261 21015.4 23932.3 ... 42710.2

Optimal solution found (tolerance 5.00e-02)
Best objective 1.826101346776e+04, best bound 1.799358299740e+04, gap 1.4645%
coll_cache:optimal_local_rate=0.274861,0.224582,0.279942,0.235586,0.272957,0.242521,0.22927,0.252201,
coll_cache:optimal_remote_rate=0.701116,0.751395,0.696035,0.740392,0.703021,0.733457,0.746708,0.723777,
coll_cache:optimal_cpu_rate=0.0240225,0.0240225,0.0240225,0.0240225,0.0240225,0.0240225,0.0240225,0.0240225,
z=18261
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
test_result:init:feat_nbytes=56862697472
test_result:init:cache_nbytes=14272536576
    [Step(average) Profiler Level 1 E3 S999]
        L1  sample           0.007761 | send           0.000000
        L1  recv             0.000000 | copy           0.011559 | convert time 0.000000 | train  0.012929
        L1  feature nbytes    1.07 GB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes      30.14 MB | remote nbytes  788.80 MB
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S999]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.011559
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S999]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.001959 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.008805 | cache combine cache 0.000880 | cache combine remote 0.008600
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S999]
        p50.00_tail_logl2featcopy=0.011578
        p90.00_tail_logl2featcopy=0.011976
        p95.00_tail_logl2featcopy=0.012082
        p99.00_tail_logl2featcopy=0.012816
        p99.90_tail_logl2featcopy=0.024853
[CUDA] cuda: usage: 21.07 GB
Rank=3, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.012151, per step: 0.000097
presamping
presamping takes 22.94064426422119
creating_intra_node_communicator root=4, local_size=4, world_size=8
Rank=4, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.008726, per step: 0.000070
presamping
presamping takes 23.710598707199097
Rank=6, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.006034, per step: 0.000048
presamping
presamping takes 22.576032400131226
Rank=2, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.006941, per step: 0.000056
presamping
presamping takes 21.959089040756226
Rank=5, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.006716, per step: 0.000054
presamping
presamping takes 22.665933847427368
config:eval_tsp="2023-04-14 21:25:53"
config:num_worker=8
config:num_intra_size=4
config:root_dir=/nvme/songxiaoniu/graph-learning/wholegraph
config:graph_name=ogbn-papers100M
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=10,25
config:hiddensize=256
config:num_layer=2
config:model=sage
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=True
config:cache_percentage=0.25
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=48
config:unsupervised=True
config:classnum=172
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7fb3127ca400>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=4, world_size=8
Rank=0, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005676, per step: 0.000045
epoch=4 total_steps=500
presamping
presamping takes 22.297407388687134
start training...
[Epoch 0], time=5.6448211669921875, loss=0.6931472420692444
[Epoch 1], time=4.059648513793945, loss=0.6931472420692444
[Epoch 2], time=4.032498598098755, loss=0.6931472420692444
[Epoch 3], time=4.027072906494141, loss=0.6931472420692444
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  141720 KB |     784 MB |    1207 GB |    1207 GB |
|       from large pool |  131599 KB |     774 MB |    1194 GB |    1194 GB |
|       from small pool |   10121 KB |      14 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Active memory         |  141720 KB |     784 MB |    1207 GB |    1207 GB |
|       from large pool |  131599 KB |     774 MB |    1194 GB |    1194 GB |
|       from small pool |   10121 KB |      14 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    2430 MB |    2430 MB |    2430 MB |       0 B  |
|       from large pool |    2410 MB |    2410 MB |    2410 MB |       0 B  |
|       from small pool |      20 MB |      20 MB |      20 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |   54887 KB |     880 MB |    1373 GB |    1373 GB |
|       from large pool |   46577 KB |     873 MB |    1359 GB |    1359 GB |
|       from small pool |    8310 KB |      11 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Allocations           |      58    |      74    |  102450    |  102392    |
|       from large pool |      11    |      21    |   32278    |   32267    |
|       from small pool |      47    |      58    |   70172    |   70125    |
|---------------------------------------------------------------------------|
| Active allocs         |      58    |      74    |  102450    |  102392    |
|       from large pool |      11    |      21    |   32278    |   32267    |
|       from small pool |      47    |      58    |   70172    |   70125    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      26    |      26    |      26    |       0    |
|       from large pool |      16    |      16    |      16    |       0    |
|       from small pool |      10    |      10    |      10    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      30    |      43    |   35146    |   35116    |
|       from large pool |       7    |      17    |   17200    |   17193    |
|       from small pool |      23    |      33    |   17946    |   17923    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 17.765454 seconds
[EPOCH_TIME] 4.441364 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 4.030003 seconds
Rank=7, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005554, per step: 0.000044
presamping
presamping takes 21.375511646270752
Rank=1, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.012098, per step: 0.000097
presamping
presamping takes 23.592397928237915

