succeed=True
worker 0 running with pid=63729
config:eval_tsp="2023-08-06 07:05:16"
config:num_worker=8
config:num_intra_size=8
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=com-friendster
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=False
config:use_collcache=False
config:cache_percentage=0.25
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=56
config:unsupervised=True
config:classnum=100
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7fb692c149a0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=8, world_size=8
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=0, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005315, per step: 0.000043
epoch=4 total_steps=500
start training...
[Epoch 0][Step 0], time=1.6106460094451904, ext_time=0.053253889083862305, train_time=1.5228731632232666
[Epoch 0][Step 1], time=0.15570569038391113, ext_time=0.043218135833740234, train_time=0.08327221870422363
[Epoch 0][Step 2], time=0.12836718559265137, ext_time=0.022734880447387695, train_time=0.08345341682434082
[Epoch 0][Step 3], time=0.15557479858398438, ext_time=0.02131199836730957, train_time=0.11212992668151855
[Epoch 0][Step 4], time=0.11968040466308594, ext_time=0.02272343635559082, train_time=0.07484579086303711
[Epoch 0][Step 5], time=0.12807631492614746, ext_time=0.02330613136291504, train_time=0.08262777328491211
[Epoch 0][Step 6], time=0.13578462600708008, ext_time=0.02292490005493164, train_time=0.0907282829284668
[Epoch 0][Step 7], time=0.11999726295471191, ext_time=0.022814035415649414, train_time=0.07516908645629883
[Epoch 0][Step 8], time=0.11226773262023926, ext_time=0.029720544815063477, train_time=0.06034445762634277
[Epoch 0][Step 9], time=0.1197655200958252, ext_time=0.022841691970825195, train_time=0.07484912872314453
[Epoch 0][Step 10], time=0.7698814868927002, ext_time=0.022761106491088867, train_time=0.7249557971954346
[Epoch 0][Step 11], time=0.11347842216491699, ext_time=0.022778987884521484, train_time=0.06859540939331055
[Epoch 0][Step 12], time=0.12757015228271484, ext_time=0.023260116577148438, train_time=0.08213639259338379
[Epoch 0][Step 13], time=0.1133127212524414, ext_time=0.022830724716186523, train_time=0.06835532188415527
[Epoch 0][Step 14], time=0.11351251602172852, ext_time=0.022762060165405273, train_time=0.06864690780639648
[Epoch 0][Step 15], time=0.11297774314880371, ext_time=0.022693872451782227, train_time=0.06816744804382324
[Epoch 0][Step 16], time=0.11340022087097168, ext_time=0.02289438247680664, train_time=0.06845521926879883
[Epoch 0][Step 17], time=0.12693166732788086, ext_time=0.022824525833129883, train_time=0.08199143409729004
[Epoch 0][Step 18], time=0.3867983818054199, ext_time=0.02281785011291504, train_time=0.3417704105377197
[Epoch 0][Step 19], time=0.11401104927062988, ext_time=0.0227968692779541, train_time=0.0690617561340332
[Epoch 0][Step 20], time=0.11368417739868164, ext_time=0.022696256637573242, train_time=0.06885695457458496
[Epoch 0][Step 21], time=0.45034170150756836, ext_time=0.02290058135986328, train_time=0.40529394149780273
[Epoch 0][Step 22], time=0.11362457275390625, ext_time=0.02289581298828125, train_time=0.06852841377258301
[Epoch 0][Step 23], time=0.11352419853210449, ext_time=0.022841215133666992, train_time=0.06855463981628418
[Epoch 0][Step 24], time=0.11313724517822266, ext_time=0.02269768714904785, train_time=0.06823301315307617
[Epoch 0][Step 25], time=0.11254239082336426, ext_time=0.029570341110229492, train_time=0.06061959266662598
[Epoch 0][Step 26], time=0.11342215538024902, ext_time=0.022790908813476562, train_time=0.06850957870483398
[Epoch 0][Step 27], time=0.13190031051635742, ext_time=0.023429393768310547, train_time=0.08636832237243652
[Epoch 0][Step 28], time=0.11852097511291504, ext_time=0.022885799407958984, train_time=0.06853985786437988
[Epoch 0][Step 29], time=0.11265897750854492, ext_time=0.022966623306274414, train_time=0.06762909889221191
[Epoch 0][Step 30], time=0.11279058456420898, ext_time=0.0230104923248291, train_time=0.06760954856872559
[Epoch 0][Step 31], time=0.11264657974243164, ext_time=0.02288365364074707, train_time=0.06754350662231445
[Epoch 0][Step 32], time=0.1128549575805664, ext_time=0.02288079261779785, train_time=0.06777834892272949
[Epoch 0][Step 33], time=0.11290597915649414, ext_time=0.02265024185180664, train_time=0.06814432144165039
[Epoch 0][Step 34], time=0.11350345611572266, ext_time=0.022809982299804688, train_time=0.06858944892883301
[Epoch 0][Step 35], time=0.11319136619567871, ext_time=0.022876977920532227, train_time=0.0681915283203125
[Epoch 0][Step 36], time=0.11333584785461426, ext_time=0.022843599319458008, train_time=0.06839346885681152
[Epoch 0][Step 37], time=0.11349225044250488, ext_time=0.02276134490966797, train_time=0.06863737106323242
[Epoch 0][Step 38], time=0.11280488967895508, ext_time=0.02299046516418457, train_time=0.06764650344848633
[Epoch 0][Step 39], time=0.1133878231048584, ext_time=0.02284383773803711, train_time=0.06855034828186035
[Epoch 0][Step 40], time=0.11313962936401367, ext_time=0.02264118194580078, train_time=0.06808137893676758
[Epoch 0][Step 41], time=0.11306643486022949, ext_time=0.02280116081237793, train_time=0.0681602954864502
[Epoch 0][Step 42], time=0.1137247085571289, ext_time=0.02287435531616211, train_time=0.06868791580200195
[Epoch 0][Step 43], time=1.189687967300415, ext_time=0.022814512252807617, train_time=1.144709587097168
[Epoch 0][Step 44], time=0.11305356025695801, ext_time=0.022885799407958984, train_time=0.0678548812866211
[Epoch 0][Step 45], time=0.11333274841308594, ext_time=0.022705793380737305, train_time=0.0685276985168457
[Epoch 0][Step 46], time=0.11330556869506836, ext_time=0.02281475067138672, train_time=0.06835246086120605
[Epoch 0][Step 47], time=0.11319947242736816, ext_time=0.022750377655029297, train_time=0.06842422485351562
[Epoch 0][Step 48], time=0.427232027053833, ext_time=0.023071765899658203, train_time=0.38210487365722656
[Epoch 0][Step 49], time=0.11365628242492676, ext_time=0.023009538650512695, train_time=0.06851387023925781
[Epoch 0][Step 50], time=0.11417603492736816, ext_time=0.022953033447265625, train_time=0.06908822059631348
[Epoch 0][Step 51], time=0.12232232093811035, ext_time=0.022885799407958984, train_time=0.07727885246276855
[Epoch 0][Step 52], time=0.1142122745513916, ext_time=0.022916316986083984, train_time=0.06905937194824219
[Epoch 0][Step 53], time=0.11400938034057617, ext_time=0.022827863693237305, train_time=0.06899285316467285
[Epoch 0][Step 54], time=0.11310648918151855, ext_time=0.022762537002563477, train_time=0.06819748878479004
[Epoch 0][Step 55], time=0.11326265335083008, ext_time=0.02281332015991211, train_time=0.0682990550994873
[Epoch 0][Step 56], time=0.12496542930603027, ext_time=0.022728443145751953, train_time=0.0802149772644043
[Epoch 0][Step 57], time=0.11337876319885254, ext_time=0.022931337356567383, train_time=0.06825876235961914
[Epoch 0][Step 58], time=0.11330151557922363, ext_time=0.022706031799316406, train_time=0.0685269832611084
[Epoch 0][Step 59], time=0.11288833618164062, ext_time=0.022946596145629883, train_time=0.06785464286804199
[Epoch 0][Step 60], time=0.1252613067626953, ext_time=0.022921323776245117, train_time=0.08025741577148438
[Epoch 0][Step 61], time=0.11344170570373535, ext_time=0.022774696350097656, train_time=0.06847143173217773
[Epoch 0][Step 62], time=0.11248207092285156, ext_time=0.02280116081237793, train_time=0.06755208969116211
[Epoch 0][Step 63], time=0.11312317848205566, ext_time=0.02291703224182129, train_time=0.06803083419799805
[Epoch 0][Step 64], time=0.1124579906463623, ext_time=0.022823333740234375, train_time=0.06750774383544922
[Epoch 0][Step 65], time=0.11594343185424805, ext_time=0.02287578582763672, train_time=0.07097005844116211
[Epoch 0][Step 66], time=0.1132657527923584, ext_time=0.022954225540161133, train_time=0.06826138496398926
[Epoch 0][Step 67], time=0.11397647857666016, ext_time=0.02291083335876465, train_time=0.0689535140991211
[Epoch 0][Step 68], time=0.11338281631469727, ext_time=0.022791624069213867, train_time=0.06847572326660156
[Epoch 0][Step 69], time=0.11294031143188477, ext_time=0.023162841796875, train_time=0.06764769554138184
[Epoch 0][Step 70], time=0.13457965850830078, ext_time=0.022760391235351562, train_time=0.06853318214416504
[Epoch 0][Step 71], time=0.11326932907104492, ext_time=0.022780179977416992, train_time=0.06830668449401855
[Epoch 0][Step 72], time=0.11308884620666504, ext_time=0.02280449867248535, train_time=0.0681447982788086
[Epoch 0][Step 73], time=0.11392402648925781, ext_time=0.022929668426513672, train_time=0.06882357597351074
[Epoch 0][Step 74], time=0.11379861831665039, ext_time=0.02284693717956543, train_time=0.0688478946685791
[Epoch 0][Step 75], time=0.11290907859802246, ext_time=0.022728443145751953, train_time=0.0680396556854248
[Epoch 0][Step 76], time=0.1129152774810791, ext_time=0.022831201553344727, train_time=0.06797409057617188
[Epoch 0][Step 77], time=0.11354994773864746, ext_time=0.022918701171875, train_time=0.06842327117919922
[Epoch 0][Step 78], time=0.11353826522827148, ext_time=0.02302861213684082, train_time=0.06841778755187988
[Epoch 0][Step 79], time=0.11328864097595215, ext_time=0.022902250289916992, train_time=0.068328857421875
[Epoch 0][Step 80], time=0.11370515823364258, ext_time=0.022889375686645508, train_time=0.06871271133422852
[Epoch 0][Step 81], time=0.1134192943572998, ext_time=0.022863149642944336, train_time=0.06852936744689941
[Epoch 0][Step 82], time=0.11330246925354004, ext_time=0.022737741470336914, train_time=0.06845688819885254
[Epoch 0][Step 83], time=0.11318159103393555, ext_time=0.022893667221069336, train_time=0.06815123558044434
[Epoch 0][Step 84], time=0.11341071128845215, ext_time=0.022960424423217773, train_time=0.0683290958404541
[Epoch 0][Step 85], time=0.11387228965759277, ext_time=0.02287769317626953, train_time=0.06892061233520508
[Epoch 0][Step 86], time=0.11284112930297852, ext_time=0.022829771041870117, train_time=0.06789493560791016
[Epoch 0][Step 87], time=0.11346435546875, ext_time=0.022948026657104492, train_time=0.06834578514099121
[Epoch 0][Step 88], time=0.11355304718017578, ext_time=0.02289891242980957, train_time=0.06842613220214844
[Epoch 0][Step 89], time=0.12246322631835938, ext_time=0.022768735885620117, train_time=0.0776207447052002
[Epoch 0][Step 90], time=0.12064862251281738, ext_time=0.022888660430908203, train_time=0.07561135292053223
[Epoch 0][Step 91], time=0.11297917366027832, ext_time=0.02296280860900879, train_time=0.06787276268005371
[Epoch 0][Step 92], time=0.1133570671081543, ext_time=0.02287149429321289, train_time=0.06842708587646484
[Epoch 0][Step 93], time=0.11275482177734375, ext_time=0.022775888442993164, train_time=0.06790304183959961
[Epoch 0][Step 94], time=0.1126563549041748, ext_time=0.02278757095336914, train_time=0.06781554222106934
[Epoch 0][Step 95], time=0.11284399032592773, ext_time=0.022924184799194336, train_time=0.06776762008666992
[Epoch 0][Step 96], time=0.11281681060791016, ext_time=0.022789478302001953, train_time=0.06784486770629883
[Epoch 0][Step 97], time=0.11310029029846191, ext_time=0.02301764488220215, train_time=0.0679476261138916
[Epoch 0][Step 98], time=0.11374640464782715, ext_time=0.02298879623413086, train_time=0.06763339042663574
[Epoch 0][Step 99], time=0.11322259902954102, ext_time=0.02284717559814453, train_time=0.06833124160766602
[Epoch 0][Step 100], time=0.11313319206237793, ext_time=0.022889375686645508, train_time=0.06810498237609863
[Epoch 0][Step 101], time=0.11337637901306152, ext_time=0.022803068161010742, train_time=0.06843733787536621
[Epoch 0][Step 102], time=0.11376237869262695, ext_time=0.022907495498657227, train_time=0.06870841979980469
[Epoch 0][Step 103], time=0.1128840446472168, ext_time=0.022828340530395508, train_time=0.06791353225708008
[Epoch 0][Step 104], time=0.11321234703063965, ext_time=0.023032426834106445, train_time=0.06805753707885742
[Epoch 0][Step 105], time=0.11271429061889648, ext_time=0.02292919158935547, train_time=0.0675802230834961
[Epoch 0][Step 106], time=1.1148302555084229, ext_time=0.023351669311523438, train_time=1.0692923069000244
[Epoch 0][Step 107], time=0.1148681640625, ext_time=0.02279949188232422, train_time=0.06988239288330078
[Epoch 0][Step 108], time=0.1152029037475586, ext_time=0.02283310890197754, train_time=0.07018232345581055
[Epoch 0][Step 109], time=0.11475801467895508, ext_time=0.023122549057006836, train_time=0.0695791244506836
[Epoch 0][Step 110], time=0.11490368843078613, ext_time=0.022808074951171875, train_time=0.06999707221984863
[Epoch 0][Step 111], time=0.11344099044799805, ext_time=0.022907018661499023, train_time=0.06832718849182129
[Epoch 0][Step 112], time=0.1276562213897705, ext_time=0.02276611328125, train_time=0.06878352165222168
[Epoch 0][Step 113], time=0.1179969310760498, ext_time=0.022730588912963867, train_time=0.0731203556060791
[Epoch 0][Step 114], time=0.11320853233337402, ext_time=0.022742748260498047, train_time=0.06826519966125488
[Epoch 0][Step 115], time=0.11310672760009766, ext_time=0.022860288619995117, train_time=0.06815576553344727
[Epoch 0][Step 116], time=0.11325287818908691, ext_time=0.02271413803100586, train_time=0.06836366653442383
[Epoch 0][Step 117], time=0.11325907707214355, ext_time=0.022748231887817383, train_time=0.06840252876281738
[Epoch 0][Step 118], time=0.11276602745056152, ext_time=0.02287578582763672, train_time=0.06783056259155273
[Epoch 0][Step 119], time=0.11315655708312988, ext_time=0.022770166397094727, train_time=0.06830120086669922
[Epoch 0][Step 120], time=0.11385536193847656, ext_time=0.02270221710205078, train_time=0.06909489631652832
[Epoch 0][Step 121], time=0.11316871643066406, ext_time=0.022965669631958008, train_time=0.0679621696472168
[Epoch 0][Step 122], time=0.11312341690063477, ext_time=0.022754669189453125, train_time=0.06823277473449707
[Epoch 0][Step 123], time=0.11335515975952148, ext_time=0.022761106491088867, train_time=0.06844115257263184
[Epoch 0][Step 124], time=0.11329936981201172, ext_time=0.0229189395904541, train_time=0.06820511817932129
[Epoch 0], time=19.628751754760742, loss=0.6931472420692444
[Epoch 1][Step 0], time=0.11326289176940918, ext_time=0.022889137268066406, train_time=0.06818103790283203
[Epoch 1][Step 1], time=0.11355376243591309, ext_time=0.022966384887695312, train_time=0.06814241409301758
[Epoch 1][Step 2], time=0.1135416030883789, ext_time=0.022819995880126953, train_time=0.0686030387878418
[Epoch 1][Step 3], time=0.11359262466430664, ext_time=0.02271294593811035, train_time=0.06877470016479492
[Epoch 1][Step 4], time=0.11304163932800293, ext_time=0.022948741912841797, train_time=0.06798863410949707
[Epoch 1][Step 5], time=0.11321210861206055, ext_time=0.022794485092163086, train_time=0.06831717491149902
[Epoch 1][Step 6], time=0.11328244209289551, ext_time=0.02278423309326172, train_time=0.06839132308959961
[Epoch 1][Step 7], time=0.11316442489624023, ext_time=0.0228579044342041, train_time=0.06817340850830078
[Epoch 1][Step 8], time=0.11801767349243164, ext_time=0.022855281829833984, train_time=0.07291603088378906
[Epoch 1][Step 9], time=0.11317801475524902, ext_time=0.022873640060424805, train_time=0.06828665733337402
[Epoch 1][Step 10], time=0.11303138732910156, ext_time=0.022868633270263672, train_time=0.06798076629638672
[Epoch 1][Step 11], time=0.11368680000305176, ext_time=0.022846698760986328, train_time=0.06874847412109375
[Epoch 1][Step 12], time=0.11352396011352539, ext_time=0.022855758666992188, train_time=0.06846737861633301
[Epoch 1][Step 13], time=0.11308526992797852, ext_time=0.022929906845092773, train_time=0.06814122200012207
[Epoch 1][Step 14], time=0.11320018768310547, ext_time=0.0228271484375, train_time=0.06825733184814453
[Epoch 1][Step 15], time=0.11316442489624023, ext_time=0.022741317749023438, train_time=0.06833505630493164
[Epoch 1][Step 16], time=0.11367917060852051, ext_time=0.0230100154876709, train_time=0.06853890419006348
[Epoch 1][Step 17], time=0.11244487762451172, ext_time=0.022915124893188477, train_time=0.0673215389251709
[Epoch 1][Step 18], time=0.11292171478271484, ext_time=0.022977352142333984, train_time=0.06784892082214355
[Epoch 1][Step 19], time=0.11327719688415527, ext_time=0.0228574275970459, train_time=0.06820416450500488
[Epoch 1][Step 20], time=0.11315727233886719, ext_time=0.02263617515563965, train_time=0.068389892578125
[Epoch 1][Step 21], time=0.11319923400878906, ext_time=0.022912025451660156, train_time=0.06804609298706055
[Epoch 1][Step 22], time=0.11344671249389648, ext_time=0.022855520248413086, train_time=0.06848478317260742
[Epoch 1][Step 23], time=0.11314702033996582, ext_time=0.02284979820251465, train_time=0.06817984580993652
[Epoch 1][Step 24], time=0.11308431625366211, ext_time=0.022893667221069336, train_time=0.06798338890075684
[Epoch 1][Step 25], time=0.11297941207885742, ext_time=0.022944927215576172, train_time=0.0678565502166748
[Epoch 1][Step 26], time=0.11303186416625977, ext_time=0.022851228713989258, train_time=0.06807088851928711
[Epoch 1][Step 27], time=0.11295127868652344, ext_time=0.022829771041870117, train_time=0.06803226470947266
[Epoch 1][Step 28], time=0.11300849914550781, ext_time=0.022925138473510742, train_time=0.06804537773132324
[Epoch 1][Step 29], time=0.11732196807861328, ext_time=0.022920608520507812, train_time=0.06792092323303223
[Epoch 1][Step 30], time=0.11267352104187012, ext_time=0.022931575775146484, train_time=0.06751823425292969
[Epoch 1][Step 31], time=0.11357283592224121, ext_time=0.02278590202331543, train_time=0.06862592697143555
[Epoch 1][Step 32], time=0.11279988288879395, ext_time=0.02303934097290039, train_time=0.06771564483642578
[Epoch 1][Step 33], time=0.11335539817810059, ext_time=0.02277970314025879, train_time=0.06851387023925781
[Epoch 1][Step 34], time=0.11374998092651367, ext_time=0.022873401641845703, train_time=0.06878995895385742
[Epoch 1][Step 35], time=0.1132512092590332, ext_time=0.02279376983642578, train_time=0.0683438777923584
[Epoch 1][Step 36], time=0.11290884017944336, ext_time=0.022832870483398438, train_time=0.06798458099365234
[Epoch 1][Step 37], time=0.11309671401977539, ext_time=0.02277350425720215, train_time=0.0681619644165039
[Epoch 1][Step 38], time=0.11342692375183105, ext_time=0.02295517921447754, train_time=0.06823444366455078
[Epoch 1][Step 39], time=0.11345314979553223, ext_time=0.02306962013244629, train_time=0.06833982467651367
[Epoch 1][Step 40], time=0.11314058303833008, ext_time=0.022640705108642578, train_time=0.06832766532897949
[Epoch 1][Step 41], time=0.11278939247131348, ext_time=0.022789478302001953, train_time=0.06786298751831055
[Epoch 1][Step 42], time=0.11347627639770508, ext_time=0.022916078567504883, train_time=0.06833934783935547
[Epoch 1][Step 43], time=0.11333060264587402, ext_time=0.022863388061523438, train_time=0.06841182708740234
[Epoch 1][Step 44], time=0.1129293441772461, ext_time=0.022989749908447266, train_time=0.0678248405456543
[Epoch 1][Step 45], time=0.11404538154602051, ext_time=0.022757768630981445, train_time=0.06919741630554199
[Epoch 1][Step 46], time=0.1138005256652832, ext_time=0.023004531860351562, train_time=0.06854414939880371
[Epoch 1][Step 47], time=0.11338496208190918, ext_time=0.022876739501953125, train_time=0.06836843490600586
[Epoch 1][Step 48], time=0.11333012580871582, ext_time=0.02273845672607422, train_time=0.06852269172668457
[Epoch 1][Step 49], time=0.11324477195739746, ext_time=0.02280712127685547, train_time=0.06830906867980957
[Epoch 1][Step 50], time=0.11357426643371582, ext_time=0.02288508415222168, train_time=0.06854534149169922
[Epoch 1][Step 51], time=0.11306333541870117, ext_time=0.022902250289916992, train_time=0.06797075271606445
[Epoch 1][Step 52], time=0.11359143257141113, ext_time=0.022655963897705078, train_time=0.06881189346313477
[Epoch 1][Step 53], time=0.11234211921691895, ext_time=0.022867918014526367, train_time=0.0674126148223877
[Epoch 1][Step 54], time=0.11304879188537598, ext_time=0.022855281829833984, train_time=0.068023681640625
[Epoch 1][Step 55], time=0.11380362510681152, ext_time=0.022779226303100586, train_time=0.06906843185424805
[Epoch 1][Step 56], time=0.11293554306030273, ext_time=0.02286553382873535, train_time=0.06809449195861816
[Epoch 1][Step 57], time=0.1132652759552002, ext_time=0.022870302200317383, train_time=0.06829714775085449
[Epoch 1][Step 58], time=0.11838340759277344, ext_time=0.02280592918395996, train_time=0.07351541519165039
[Epoch 1][Step 59], time=0.1130824089050293, ext_time=0.022759437561035156, train_time=0.06817269325256348
[Epoch 1][Step 60], time=0.1132044792175293, ext_time=0.022871017456054688, train_time=0.06816339492797852
[Epoch 1][Step 61], time=0.11365580558776855, ext_time=0.022669076919555664, train_time=0.06887245178222656
[Epoch 1][Step 62], time=0.11327457427978516, ext_time=0.023017406463623047, train_time=0.0680851936340332
[Epoch 1][Step 63], time=0.11341714859008789, ext_time=0.02289557456970215, train_time=0.06838345527648926
[Epoch 1][Step 64], time=0.11251449584960938, ext_time=0.022825002670288086, train_time=0.06765484809875488
[Epoch 1][Step 65], time=0.11317682266235352, ext_time=0.02272319793701172, train_time=0.06827998161315918
[Epoch 1][Step 66], time=0.1130375862121582, ext_time=0.02283763885498047, train_time=0.06798744201660156
[Epoch 1][Step 67], time=0.11337637901306152, ext_time=0.022782087326049805, train_time=0.06844711303710938
[Epoch 1][Step 68], time=0.1135711669921875, ext_time=0.022719144821166992, train_time=0.06875061988830566
[Epoch 1][Step 69], time=0.11311745643615723, ext_time=0.023037195205688477, train_time=0.06795454025268555
[Epoch 1][Step 70], time=0.11365389823913574, ext_time=0.022745609283447266, train_time=0.06879281997680664
[Epoch 1][Step 71], time=0.11759376525878906, ext_time=0.022883892059326172, train_time=0.06818032264709473
[Epoch 1][Step 72], time=0.1130068302154541, ext_time=0.022874116897583008, train_time=0.06795072555541992
[Epoch 1][Step 73], time=0.11388778686523438, ext_time=0.022883892059326172, train_time=0.06885552406311035
[Epoch 1][Step 74], time=0.11359620094299316, ext_time=0.02276301383972168, train_time=0.06868577003479004
[Epoch 1][Step 75], time=0.11263418197631836, ext_time=0.022658586502075195, train_time=0.06782984733581543
[Epoch 1][Step 76], time=0.1134958267211914, ext_time=0.02288341522216797, train_time=0.06842851638793945
[Epoch 1][Step 77], time=0.11366963386535645, ext_time=0.022924184799194336, train_time=0.06857419013977051
[Epoch 1][Step 78], time=0.11323952674865723, ext_time=0.023056507110595703, train_time=0.06808757781982422
[Epoch 1][Step 79], time=0.11373519897460938, ext_time=0.022699356079101562, train_time=0.06895041465759277
[Epoch 1][Step 80], time=0.11349773406982422, ext_time=0.02281045913696289, train_time=0.06859803199768066
[Epoch 1][Step 81], time=0.11331605911254883, ext_time=0.02291274070739746, train_time=0.06820917129516602
[Epoch 1][Step 82], time=0.11331057548522949, ext_time=0.022787809371948242, train_time=0.06851434707641602
[Epoch 1][Step 83], time=0.11347126960754395, ext_time=0.022948503494262695, train_time=0.06841182708740234
[Epoch 1][Step 84], time=0.11323094367980957, ext_time=0.022719860076904297, train_time=0.06841039657592773
[Epoch 1][Step 85], time=0.11726069450378418, ext_time=0.022985458374023438, train_time=0.07219791412353516
[Epoch 1][Step 86], time=0.11332392692565918, ext_time=0.02286982536315918, train_time=0.06832385063171387
[Epoch 1][Step 87], time=0.1125040054321289, ext_time=0.022990703582763672, train_time=0.06743407249450684
[Epoch 1][Step 88], time=0.11303472518920898, ext_time=0.022893428802490234, train_time=0.06804800033569336
[Epoch 1][Step 89], time=0.11327672004699707, ext_time=0.022818326950073242, train_time=0.06837892532348633
[Epoch 1][Step 90], time=0.12366843223571777, ext_time=0.02283453941345215, train_time=0.07875609397888184
[Epoch 1][Step 91], time=0.11295366287231445, ext_time=0.022970199584960938, train_time=0.0678706169128418
[Epoch 1][Step 92], time=0.11418652534484863, ext_time=0.02281498908996582, train_time=0.06929445266723633
[Epoch 1][Step 93], time=0.11367535591125488, ext_time=0.02279376983642578, train_time=0.06879973411560059
[Epoch 1][Step 94], time=0.11327528953552246, ext_time=0.022819995880126953, train_time=0.06831598281860352
[Epoch 1][Step 95], time=0.11318826675415039, ext_time=0.022907733917236328, train_time=0.06815171241760254
[Epoch 1][Step 96], time=0.11273074150085449, ext_time=0.022656679153442383, train_time=0.06792259216308594
[Epoch 1][Step 97], time=0.11348724365234375, ext_time=0.02300858497619629, train_time=0.06828856468200684
[Epoch 1][Step 98], time=0.11296248435974121, ext_time=0.022841691970825195, train_time=0.06800389289855957
[Epoch 1][Step 99], time=0.11349844932556152, ext_time=0.02294325828552246, train_time=0.0684208869934082
[Epoch 1][Step 100], time=0.11334037780761719, ext_time=0.022873878479003906, train_time=0.06831693649291992
[Epoch 1][Step 101], time=0.11318397521972656, ext_time=0.02300286293029785, train_time=0.06813526153564453
[Epoch 1][Step 102], time=0.11346316337585449, ext_time=0.022937536239624023, train_time=0.06834626197814941
[Epoch 1][Step 103], time=0.11349201202392578, ext_time=0.02287888526916504, train_time=0.06849503517150879
[Epoch 1][Step 104], time=0.11293911933898926, ext_time=0.022861719131469727, train_time=0.06802582740783691
[Epoch 1][Step 105], time=0.11331820487976074, ext_time=0.022876262664794922, train_time=0.06827425956726074
[Epoch 1][Step 106], time=0.11313891410827637, ext_time=0.022846698760986328, train_time=0.06810164451599121
[Epoch 1][Step 107], time=0.11338043212890625, ext_time=0.02270960807800293, train_time=0.06861305236816406
[Epoch 1][Step 108], time=0.11342668533325195, ext_time=0.02289414405822754, train_time=0.06848692893981934
[Epoch 1][Step 109], time=0.11376333236694336, ext_time=0.023001432418823242, train_time=0.06857705116271973
[Epoch 1][Step 110], time=0.11344146728515625, ext_time=0.02281022071838379, train_time=0.06850409507751465
[Epoch 1][Step 111], time=0.11358022689819336, ext_time=0.022875547409057617, train_time=0.06861042976379395
[Epoch 1][Step 112], time=0.11336326599121094, ext_time=0.022800207138061523, train_time=0.06840348243713379
[Epoch 1][Step 113], time=0.1176445484161377, ext_time=0.02288508415222168, train_time=0.06788802146911621
[Epoch 1][Step 114], time=0.11320376396179199, ext_time=0.02268505096435547, train_time=0.0683748722076416
[Epoch 1][Step 115], time=0.1135404109954834, ext_time=0.02294135093688965, train_time=0.0684664249420166
[Epoch 1][Step 116], time=0.38802552223205566, ext_time=0.022670984268188477, train_time=0.34327030181884766
[Epoch 1][Step 117], time=0.11275601387023926, ext_time=0.022881031036376953, train_time=0.06776213645935059
[Epoch 1][Step 118], time=0.5611717700958252, ext_time=0.029837846755981445, train_time=0.5091221332550049
[Epoch 1][Step 119], time=0.11361455917358398, ext_time=0.022897958755493164, train_time=0.06812667846679688
[Epoch 1][Step 120], time=0.1139063835144043, ext_time=0.022925138473510742, train_time=0.06759786605834961
[Epoch 1][Step 121], time=0.1133275032043457, ext_time=0.02300429344177246, train_time=0.06763076782226562
[Epoch 1][Step 122], time=0.11275744438171387, ext_time=0.02279186248779297, train_time=0.06724929809570312
[Epoch 1][Step 123], time=0.11294031143188477, ext_time=0.022925615310668945, train_time=0.06656193733215332
[Epoch 1][Step 124], time=0.11316251754760742, ext_time=0.022933006286621094, train_time=0.0677042007446289
[Epoch 1], time=14.925010681152344, loss=0.6931472420692444
[Epoch 2][Step 0], time=0.11306118965148926, ext_time=0.022816181182861328, train_time=0.06683707237243652
[Epoch 2][Step 1], time=0.11339330673217773, ext_time=0.022886037826538086, train_time=0.06800413131713867
[Epoch 2][Step 2], time=0.11349749565124512, ext_time=0.022879362106323242, train_time=0.06804347038269043
[Epoch 2][Step 3], time=0.1305842399597168, ext_time=0.022658348083496094, train_time=0.08582544326782227
[Epoch 2][Step 4], time=0.11359214782714844, ext_time=0.022812604904174805, train_time=0.06856942176818848
[Epoch 2][Step 5], time=0.11339974403381348, ext_time=0.02277064323425293, train_time=0.0685572624206543
[Epoch 2][Step 6], time=0.11330890655517578, ext_time=0.022905826568603516, train_time=0.0683135986328125
[Epoch 2][Step 7], time=0.11649847030639648, ext_time=0.022796630859375, train_time=0.07150959968566895
[Epoch 2][Step 8], time=0.11303257942199707, ext_time=0.022867918014526367, train_time=0.0680849552154541
[Epoch 2][Step 9], time=0.11274099349975586, ext_time=0.02277398109436035, train_time=0.06790518760681152
[Epoch 2][Step 10], time=0.11346173286437988, ext_time=0.022862672805786133, train_time=0.06850266456604004
[Epoch 2][Step 11], time=0.1158912181854248, ext_time=0.022772789001464844, train_time=0.07097935676574707
[Epoch 2][Step 12], time=0.12461662292480469, ext_time=0.022819042205810547, train_time=0.07962274551391602
[Epoch 2][Step 13], time=0.11314678192138672, ext_time=0.02288055419921875, train_time=0.06824350357055664
[Epoch 2][Step 14], time=0.11360025405883789, ext_time=0.022878408432006836, train_time=0.06863260269165039
[Epoch 2][Step 15], time=0.11311006546020508, ext_time=0.02285313606262207, train_time=0.06814360618591309
[Epoch 2][Step 16], time=0.11357712745666504, ext_time=0.023005247116088867, train_time=0.06847620010375977
[Epoch 2][Step 17], time=0.11305642127990723, ext_time=0.0228574275970459, train_time=0.06813716888427734
[Epoch 2][Step 18], time=0.1131279468536377, ext_time=0.022850990295410156, train_time=0.06814193725585938
[Epoch 2][Step 19], time=0.11339950561523438, ext_time=0.022860050201416016, train_time=0.06839513778686523
[Epoch 2][Step 20], time=0.11294674873352051, ext_time=0.022533178329467773, train_time=0.06831574440002441
[Epoch 2][Step 21], time=0.11329102516174316, ext_time=0.023029565811157227, train_time=0.06806182861328125
[Epoch 2][Step 22], time=0.11363506317138672, ext_time=0.02274608612060547, train_time=0.06876850128173828
[Epoch 2][Step 23], time=0.11323833465576172, ext_time=0.02283620834350586, train_time=0.06829309463500977
[Epoch 2][Step 24], time=0.11278843879699707, ext_time=0.022786617279052734, train_time=0.06780719757080078
[Epoch 2][Step 25], time=0.11339640617370605, ext_time=0.02285623550415039, train_time=0.06835675239562988
[Epoch 2][Step 26], time=0.1139061450958252, ext_time=0.022765398025512695, train_time=0.0689997673034668
[Epoch 2][Step 27], time=0.11308550834655762, ext_time=0.02276325225830078, train_time=0.06825923919677734
[Epoch 2][Step 28], time=0.11331796646118164, ext_time=0.022776365280151367, train_time=0.06854820251464844
[Epoch 2][Step 29], time=0.11324167251586914, ext_time=0.022901296615600586, train_time=0.06817269325256348
[Epoch 2][Step 30], time=0.11713290214538574, ext_time=0.022908449172973633, train_time=0.06806635856628418
[Epoch 2][Step 31], time=0.11301064491271973, ext_time=0.022775650024414062, train_time=0.06800198554992676
[Epoch 2][Step 32], time=0.11314082145690918, ext_time=0.023010730743408203, train_time=0.06807208061218262
[Epoch 2][Step 33], time=0.11409330368041992, ext_time=0.022783756256103516, train_time=0.06919002532958984
[Epoch 2][Step 34], time=0.11337924003601074, ext_time=0.022991180419921875, train_time=0.06836700439453125
[Epoch 2][Step 35], time=0.1134488582611084, ext_time=0.02282118797302246, train_time=0.06850552558898926
[Epoch 2][Step 36], time=0.11285519599914551, ext_time=0.022859573364257812, train_time=0.06789493560791016
[Epoch 2][Step 37], time=0.11270689964294434, ext_time=0.022760629653930664, train_time=0.06787991523742676
[Epoch 2][Step 38], time=0.11330819129943848, ext_time=0.02289128303527832, train_time=0.06833267211914062
[Epoch 2][Step 39], time=0.11345028877258301, ext_time=0.022893667221069336, train_time=0.06841325759887695
[Epoch 2][Step 40], time=0.11328458786010742, ext_time=0.022765398025512695, train_time=0.06844305992126465
[Epoch 2][Step 41], time=0.11308550834655762, ext_time=0.02282547950744629, train_time=0.06815648078918457
[Epoch 2][Step 42], time=0.11379623413085938, ext_time=0.02292013168334961, train_time=0.06875443458557129
[Epoch 2][Step 43], time=0.11320376396179199, ext_time=0.022917985916137695, train_time=0.06806468963623047
[Epoch 2][Step 44], time=0.11264157295227051, ext_time=0.022940635681152344, train_time=0.0675668716430664
[Epoch 2][Step 45], time=0.11370730400085449, ext_time=0.02277064323425293, train_time=0.06876063346862793
[Epoch 2][Step 46], time=0.11311483383178711, ext_time=0.022999286651611328, train_time=0.06800341606140137
[Epoch 2][Step 47], time=0.11359000205993652, ext_time=0.022833585739135742, train_time=0.06856560707092285
[Epoch 2][Step 48], time=0.11302709579467773, ext_time=0.022789716720581055, train_time=0.06813859939575195
[Epoch 2][Step 49], time=0.11259150505065918, ext_time=0.022957801818847656, train_time=0.0675203800201416
[Epoch 2][Step 50], time=0.1133735179901123, ext_time=0.02286243438720703, train_time=0.06835532188415527
[Epoch 2][Step 51], time=0.11318612098693848, ext_time=0.022904157638549805, train_time=0.06808853149414062
[Epoch 2][Step 52], time=0.11345195770263672, ext_time=0.022667646408081055, train_time=0.06866788864135742
[Epoch 2][Step 53], time=0.11272406578063965, ext_time=0.022729158401489258, train_time=0.0680398941040039
[Epoch 2][Step 54], time=0.11319732666015625, ext_time=0.022792577743530273, train_time=0.06817317008972168
[Epoch 2][Step 55], time=0.11339664459228516, ext_time=0.02272176742553711, train_time=0.06857061386108398
[Epoch 2][Step 56], time=0.11305952072143555, ext_time=0.022882938385009766, train_time=0.0681924819946289
[Epoch 2][Step 57], time=0.11298131942749023, ext_time=0.022833824157714844, train_time=0.06797003746032715
[Epoch 2][Step 58], time=0.11316394805908203, ext_time=0.022770166397094727, train_time=0.06830239295959473
[Epoch 2][Step 59], time=0.11329817771911621, ext_time=0.022893428802490234, train_time=0.06824398040771484
[Epoch 2][Step 60], time=0.11337780952453613, ext_time=0.022804975509643555, train_time=0.06847071647644043
[Epoch 2][Step 61], time=0.11329793930053711, ext_time=0.02273249626159668, train_time=0.06848835945129395
[Epoch 2][Step 62], time=0.11333346366882324, ext_time=0.022880077362060547, train_time=0.0683436393737793
[Epoch 2][Step 63], time=0.43187808990478516, ext_time=0.022773027420043945, train_time=0.3869931697845459
[Epoch 2][Step 64], time=0.11313581466674805, ext_time=0.022915363311767578, train_time=0.06814217567443848
[Epoch 2][Step 65], time=0.11315131187438965, ext_time=0.022797346115112305, train_time=0.06825757026672363
[Epoch 2][Step 66], time=0.11255908012390137, ext_time=0.022870540618896484, train_time=0.06755757331848145
[Epoch 2][Step 67], time=0.11336731910705566, ext_time=0.022754192352294922, train_time=0.06856274604797363
[Epoch 2][Step 68], time=0.11365342140197754, ext_time=0.022919178009033203, train_time=0.06856060028076172
[Epoch 2][Step 69], time=0.11255383491516113, ext_time=0.02305436134338379, train_time=0.06742358207702637
[Epoch 2][Step 70], time=0.13133525848388672, ext_time=0.022678136825561523, train_time=0.08648157119750977
[Epoch 2][Step 71], time=0.11283469200134277, ext_time=0.022823572158813477, train_time=0.06795763969421387
[Epoch 2][Step 72], time=0.12718796730041504, ext_time=0.02284717559814453, train_time=0.07564139366149902
[Epoch 2][Step 73], time=0.11361289024353027, ext_time=0.022997617721557617, train_time=0.06843996047973633
[Epoch 2][Step 74], time=0.1132516860961914, ext_time=0.022833824157714844, train_time=0.06828141212463379
[Epoch 2][Step 75], time=0.11295080184936523, ext_time=0.02284097671508789, train_time=0.06788396835327148
[Epoch 2][Step 76], time=1.3082811832427979, ext_time=0.022795915603637695, train_time=1.26340651512146
[Epoch 2][Step 77], time=0.11369681358337402, ext_time=0.02286052703857422, train_time=0.06866216659545898
[Epoch 2][Step 78], time=0.11301636695861816, ext_time=0.022873878479003906, train_time=0.06799554824829102
[Epoch 2][Step 79], time=0.11316132545471191, ext_time=0.02289581298828125, train_time=0.06818342208862305
[Epoch 2][Step 80], time=0.11364483833312988, ext_time=0.022897720336914062, train_time=0.06862330436706543
[Epoch 2][Step 81], time=0.11366891860961914, ext_time=0.022922992706298828, train_time=0.06855940818786621
[Epoch 2][Step 82], time=0.11321496963500977, ext_time=0.022869110107421875, train_time=0.06819605827331543
[Epoch 2][Step 83], time=0.1183321475982666, ext_time=0.02292776107788086, train_time=0.0733633041381836
[Epoch 2][Step 84], time=0.11352729797363281, ext_time=0.0229952335357666, train_time=0.0684518814086914
[Epoch 2][Step 85], time=0.11303901672363281, ext_time=0.02277064323425293, train_time=0.0682375431060791
[Epoch 2][Step 86], time=0.11316919326782227, ext_time=0.0227968692779541, train_time=0.06827235221862793
[Epoch 2][Step 87], time=0.11318349838256836, ext_time=0.022797584533691406, train_time=0.06826448440551758
[Epoch 2][Step 88], time=0.11317682266235352, ext_time=0.0228273868560791, train_time=0.06819701194763184
[Epoch 2][Step 89], time=0.11567807197570801, ext_time=0.022908687591552734, train_time=0.07068729400634766
[Epoch 2][Step 90], time=0.11335134506225586, ext_time=0.02292656898498535, train_time=0.06825947761535645
[Epoch 2][Step 91], time=0.11325788497924805, ext_time=0.022905588150024414, train_time=0.06827783584594727
[Epoch 2][Step 92], time=0.1133272647857666, ext_time=0.022725343704223633, train_time=0.06845211982727051
[Epoch 2][Step 93], time=0.11300373077392578, ext_time=0.022699832916259766, train_time=0.06818532943725586
[Epoch 2][Step 94], time=0.1131749153137207, ext_time=0.022893428802490234, train_time=0.06812763214111328
[Epoch 2][Step 95], time=0.11339735984802246, ext_time=0.022803544998168945, train_time=0.06847500801086426
[Epoch 2][Step 96], time=0.11309027671813965, ext_time=0.022798538208007812, train_time=0.06805133819580078
[Epoch 2][Step 97], time=0.11341309547424316, ext_time=0.022895097732543945, train_time=0.0683743953704834
[Epoch 2][Step 98], time=0.11367344856262207, ext_time=0.022906064987182617, train_time=0.06861543655395508
[Epoch 2][Step 99], time=0.11324477195739746, ext_time=0.022809743881225586, train_time=0.06838059425354004
[Epoch 2][Step 100], time=0.820805549621582, ext_time=0.022943735122680664, train_time=0.7756788730621338
[Epoch 2][Step 101], time=0.11390352249145508, ext_time=0.022955894470214844, train_time=0.06846928596496582
[Epoch 2][Step 102], time=0.11382198333740234, ext_time=0.022892236709594727, train_time=0.06870007514953613
[Epoch 2][Step 103], time=0.11305975914001465, ext_time=0.022965669631958008, train_time=0.06793761253356934
[Epoch 2][Step 104], time=0.11269807815551758, ext_time=0.02280592918395996, train_time=0.06785178184509277
[Epoch 2][Step 105], time=0.11254143714904785, ext_time=0.022713422775268555, train_time=0.06780481338500977
[Epoch 2][Step 106], time=0.11365079879760742, ext_time=0.022753238677978516, train_time=0.06877970695495605
[Epoch 2][Step 107], time=0.11316227912902832, ext_time=0.02290201187133789, train_time=0.06823134422302246
[Epoch 2][Step 108], time=0.11320614814758301, ext_time=0.022998332977294922, train_time=0.06802892684936523
[Epoch 2][Step 109], time=0.11339783668518066, ext_time=0.023039817810058594, train_time=0.06817364692687988
[Epoch 2][Step 110], time=0.1298975944519043, ext_time=0.022870779037475586, train_time=0.08489632606506348
[Epoch 2][Step 111], time=0.1134347915649414, ext_time=0.022667407989501953, train_time=0.06863164901733398
[Epoch 2][Step 112], time=0.11338996887207031, ext_time=0.022730588912963867, train_time=0.06859254837036133
[Epoch 2][Step 113], time=0.11321544647216797, ext_time=0.022893428802490234, train_time=0.06828045845031738
[Epoch 2][Step 114], time=0.11495208740234375, ext_time=0.022882461547851562, train_time=0.06765151023864746
[Epoch 2][Step 115], time=0.11317014694213867, ext_time=0.02294468879699707, train_time=0.06813287734985352
[Epoch 2][Step 116], time=0.11312222480773926, ext_time=0.022616863250732422, train_time=0.06835603713989258
[Epoch 2][Step 117], time=0.11328291893005371, ext_time=0.02280402183532715, train_time=0.06839942932128906
[Epoch 2][Step 118], time=0.11294078826904297, ext_time=0.022918701171875, train_time=0.06799149513244629
[Epoch 2][Step 119], time=0.11305522918701172, ext_time=0.022919654846191406, train_time=0.0677485466003418
[Epoch 2][Step 120], time=0.11352157592773438, ext_time=0.02284097671508789, train_time=0.06845974922180176
[Epoch 2][Step 121], time=0.11366462707519531, ext_time=0.0231935977935791, train_time=0.06840109825134277
[Epoch 2][Step 122], time=0.11255717277526855, ext_time=0.022762060165405273, train_time=0.06760764122009277
[Epoch 2][Step 123], time=0.1133575439453125, ext_time=0.022771596908569336, train_time=0.06840062141418457
[Epoch 2][Step 124], time=0.1129906177520752, ext_time=0.022856712341308594, train_time=0.06801509857177734
[Epoch 2], time=16.479947566986084, loss=0.6931472420692444
[Epoch 3][Step 0], time=0.11359906196594238, ext_time=0.022785425186157227, train_time=0.06873250007629395
[Epoch 3][Step 1], time=0.1134786605834961, ext_time=0.022871971130371094, train_time=0.06860828399658203
[Epoch 3][Step 2], time=0.11341118812561035, ext_time=0.02276468276977539, train_time=0.06849980354309082
[Epoch 3][Step 3], time=0.11364030838012695, ext_time=0.022828102111816406, train_time=0.06873202323913574
[Epoch 3][Step 4], time=0.11293363571166992, ext_time=0.022947072982788086, train_time=0.06795144081115723
[Epoch 3][Step 5], time=0.11334514617919922, ext_time=0.02283167839050293, train_time=0.06841397285461426
[Epoch 3][Step 6], time=0.11342239379882812, ext_time=0.022771835327148438, train_time=0.0685875415802002
[Epoch 3][Step 7], time=0.11307430267333984, ext_time=0.022868871688842773, train_time=0.06811857223510742
[Epoch 3][Step 8], time=0.11234617233276367, ext_time=0.02279210090637207, train_time=0.0674288272857666
[Epoch 3][Step 9], time=0.11313366889953613, ext_time=0.022873401641845703, train_time=0.06815409660339355
[Epoch 3][Step 10], time=0.11609196662902832, ext_time=0.022973299026489258, train_time=0.07096219062805176
[Epoch 3][Step 11], time=0.11304283142089844, ext_time=0.022891759872436523, train_time=0.06807136535644531
[Epoch 3][Step 12], time=0.11324095726013184, ext_time=0.022768020629882812, train_time=0.06829667091369629
[Epoch 3][Step 13], time=0.11296963691711426, ext_time=0.022986650466918945, train_time=0.06787776947021484
[Epoch 3][Step 14], time=0.11340594291687012, ext_time=0.022806644439697266, train_time=0.06850266456604004
[Epoch 3][Step 15], time=0.1130976676940918, ext_time=0.022751808166503906, train_time=0.06821346282958984
[Epoch 3][Step 16], time=0.11338448524475098, ext_time=0.02302241325378418, train_time=0.06813549995422363
[Epoch 3][Step 17], time=0.11316609382629395, ext_time=0.022866249084472656, train_time=0.06818652153015137
[Epoch 3][Step 18], time=0.11311030387878418, ext_time=0.022883892059326172, train_time=0.0680837631225586
[Epoch 3][Step 19], time=0.113555908203125, ext_time=0.022763729095458984, train_time=0.06876373291015625
[Epoch 3][Step 20], time=0.1131136417388916, ext_time=0.022633075714111328, train_time=0.0683436393737793
[Epoch 3][Step 21], time=0.11312246322631836, ext_time=0.022842884063720703, train_time=0.06805634498596191
[Epoch 3][Step 22], time=0.11329007148742676, ext_time=0.022862911224365234, train_time=0.06837606430053711
[Epoch 3][Step 23], time=0.11361145973205566, ext_time=0.02297210693359375, train_time=0.06855654716491699
[Epoch 3][Step 24], time=0.1126246452331543, ext_time=0.022848129272460938, train_time=0.06765365600585938
[Epoch 3][Step 25], time=0.11325216293334961, ext_time=0.0228574275970459, train_time=0.06828117370605469
[Epoch 3][Step 26], time=0.11326098442077637, ext_time=0.02278876304626465, train_time=0.06831479072570801
[Epoch 3][Step 27], time=0.1132512092590332, ext_time=0.02289581298828125, train_time=0.06808733940124512
[Epoch 3][Step 28], time=0.11353778839111328, ext_time=0.022738218307495117, train_time=0.06873106956481934
[Epoch 3][Step 29], time=0.11300134658813477, ext_time=0.022799968719482422, train_time=0.06813311576843262
[Epoch 3][Step 30], time=0.1130828857421875, ext_time=0.022936344146728516, train_time=0.06794142723083496
[Epoch 3][Step 31], time=0.11507344245910645, ext_time=0.022895336151123047, train_time=0.0679771900177002
[Epoch 3][Step 32], time=0.1127934455871582, ext_time=0.022899150848388672, train_time=0.0677635669708252
[Epoch 3][Step 33], time=0.11306953430175781, ext_time=0.02280902862548828, train_time=0.06817984580993652
[Epoch 3][Step 34], time=0.11351943016052246, ext_time=0.023032665252685547, train_time=0.0683901309967041
[Epoch 3][Step 35], time=0.11357259750366211, ext_time=0.02294445037841797, train_time=0.06849145889282227
[Epoch 3][Step 36], time=0.11356425285339355, ext_time=0.022890567779541016, train_time=0.06859707832336426
[Epoch 3][Step 37], time=0.11315250396728516, ext_time=0.0228574275970459, train_time=0.0682215690612793
[Epoch 3][Step 38], time=0.11362195014953613, ext_time=0.02293539047241211, train_time=0.06859278678894043
[Epoch 3][Step 39], time=0.11300945281982422, ext_time=0.02285313606262207, train_time=0.06803631782531738
[Epoch 3][Step 40], time=0.11339950561523438, ext_time=0.022826433181762695, train_time=0.0684659481048584
[Epoch 3][Step 41], time=0.11314272880554199, ext_time=0.022785425186157227, train_time=0.06823134422302246
[Epoch 3][Step 42], time=0.11381387710571289, ext_time=0.02281665802001953, train_time=0.0688478946685791
[Epoch 3][Step 43], time=0.11299943923950195, ext_time=0.022878170013427734, train_time=0.06803441047668457
[Epoch 3][Step 44], time=0.11311984062194824, ext_time=0.022969484329223633, train_time=0.06808996200561523
[Epoch 3][Step 45], time=0.1135718822479248, ext_time=0.022678375244140625, train_time=0.06883525848388672
[Epoch 3][Step 46], time=0.11368036270141602, ext_time=0.022844314575195312, train_time=0.06869721412658691
[Epoch 3][Step 47], time=0.11343765258789062, ext_time=0.022947072982788086, train_time=0.06836748123168945
[Epoch 3][Step 48], time=0.11366629600524902, ext_time=0.022934436798095703, train_time=0.06862306594848633
[Epoch 3][Step 49], time=0.11301684379577637, ext_time=0.022917747497558594, train_time=0.0679175853729248
[Epoch 3][Step 50], time=0.11325716972351074, ext_time=0.022922277450561523, train_time=0.06817054748535156
[Epoch 3][Step 51], time=0.11307930946350098, ext_time=0.022801876068115234, train_time=0.06818675994873047
[Epoch 3][Step 52], time=0.1137537956237793, ext_time=0.02275538444519043, train_time=0.0688011646270752
[Epoch 3][Step 53], time=0.1129148006439209, ext_time=0.022742271423339844, train_time=0.06813168525695801
[Epoch 3][Step 54], time=0.11311197280883789, ext_time=0.022779464721679688, train_time=0.06821203231811523
[Epoch 3][Step 55], time=0.11387324333190918, ext_time=0.02296137809753418, train_time=0.06873035430908203
[Epoch 3][Step 56], time=0.11295700073242188, ext_time=0.022867679595947266, train_time=0.06812691688537598
[Epoch 3][Step 57], time=0.11314535140991211, ext_time=0.022893905639648438, train_time=0.06818413734436035
[Epoch 3][Step 58], time=0.11345767974853516, ext_time=0.029526948928833008, train_time=0.06169486045837402
[Epoch 3][Step 59], time=0.1131744384765625, ext_time=0.02277660369873047, train_time=0.06839513778686523
[Epoch 3][Step 60], time=0.11329221725463867, ext_time=0.022927045822143555, train_time=0.06826591491699219
[Epoch 3][Step 61], time=0.1130971908569336, ext_time=0.02269148826599121, train_time=0.06835579872131348
[Epoch 3][Step 62], time=0.1129159927368164, ext_time=0.022837400436401367, train_time=0.06799101829528809
[Epoch 3][Step 63], time=0.11339616775512695, ext_time=0.022939443588256836, train_time=0.06832456588745117
[Epoch 3][Step 64], time=0.11294078826904297, ext_time=0.02293229103088379, train_time=0.06792998313903809
[Epoch 3][Step 65], time=0.11347365379333496, ext_time=0.022771120071411133, train_time=0.06855416297912598
[Epoch 3][Step 66], time=0.11288332939147949, ext_time=0.022879362106323242, train_time=0.06787443161010742
[Epoch 3][Step 67], time=0.1136319637298584, ext_time=0.022862911224365234, train_time=0.06867313385009766
[Epoch 3][Step 68], time=0.1133737564086914, ext_time=0.022918224334716797, train_time=0.0678863525390625
[Epoch 3][Step 69], time=0.11248421669006348, ext_time=0.023077011108398438, train_time=0.06732296943664551
[Epoch 3][Step 70], time=0.11348152160644531, ext_time=0.02266860008239746, train_time=0.06873297691345215
[Epoch 3][Step 71], time=0.11339950561523438, ext_time=0.02270674705505371, train_time=0.06855010986328125
[Epoch 3][Step 72], time=0.11301350593566895, ext_time=0.022933006286621094, train_time=0.06800365447998047
[Epoch 3][Step 73], time=0.1158285140991211, ext_time=0.02295684814453125, train_time=0.06853795051574707
[Epoch 3][Step 74], time=0.11356258392333984, ext_time=0.022723674774169922, train_time=0.06870436668395996
[Epoch 3][Step 75], time=0.11303234100341797, ext_time=0.022753477096557617, train_time=0.0680842399597168
[Epoch 3][Step 76], time=0.1134483814239502, ext_time=0.02278614044189453, train_time=0.06852555274963379
[Epoch 3][Step 77], time=1.423717975616455, ext_time=0.022797584533691406, train_time=1.3788104057312012
[Epoch 3][Step 78], time=0.11341214179992676, ext_time=0.022968530654907227, train_time=0.06819725036621094
[Epoch 3][Step 79], time=0.11326837539672852, ext_time=0.022841691970825195, train_time=0.06821727752685547
[Epoch 3][Step 80], time=0.11358380317687988, ext_time=0.02288675308227539, train_time=0.06856727600097656
[Epoch 3][Step 81], time=0.11339759826660156, ext_time=0.022920608520507812, train_time=0.06836533546447754
[Epoch 3][Step 82], time=0.11304306983947754, ext_time=0.022815704345703125, train_time=0.06812071800231934
[Epoch 3][Step 83], time=0.11353302001953125, ext_time=0.02287125587463379, train_time=0.06856751441955566
[Epoch 3][Step 84], time=0.11360907554626465, ext_time=0.02284693717956543, train_time=0.0686643123626709
[Epoch 3][Step 85], time=0.11323380470275879, ext_time=0.02292323112487793, train_time=0.06815528869628906
[Epoch 3][Step 86], time=0.11251616477966309, ext_time=0.022733211517333984, train_time=0.0677027702331543
[Epoch 3][Step 87], time=0.11324334144592285, ext_time=0.022771835327148438, train_time=0.06831049919128418
[Epoch 3][Step 88], time=0.11315083503723145, ext_time=0.022800922393798828, train_time=0.06818866729736328
[Epoch 3][Step 89], time=0.1125338077545166, ext_time=0.022895097732543945, train_time=0.06746578216552734
[Epoch 3][Step 90], time=0.11640238761901855, ext_time=0.022851228713989258, train_time=0.07149410247802734
[Epoch 3][Step 91], time=0.11311793327331543, ext_time=0.022838592529296875, train_time=0.06815195083618164
[Epoch 3][Step 92], time=0.11367964744567871, ext_time=0.022970914840698242, train_time=0.0684971809387207
[Epoch 3][Step 93], time=0.11357617378234863, ext_time=0.02282857894897461, train_time=0.06850457191467285
[Epoch 3][Step 94], time=0.11317825317382812, ext_time=0.02286052703857422, train_time=0.06816530227661133
[Epoch 3][Step 95], time=0.11280202865600586, ext_time=0.02276611328125, train_time=0.0679173469543457
[Epoch 3][Step 96], time=0.11259841918945312, ext_time=0.02270817756652832, train_time=0.06783294677734375
[Epoch 3][Step 97], time=0.11280345916748047, ext_time=0.022838354110717773, train_time=0.06792092323303223
[Epoch 3][Step 98], time=0.11338281631469727, ext_time=0.022860288619995117, train_time=0.06848788261413574
[Epoch 3][Step 99], time=0.1129615306854248, ext_time=0.02290177345275879, train_time=0.0679788589477539
[Epoch 3][Step 100], time=0.11262726783752441, ext_time=0.02288818359375, train_time=0.06757974624633789
[Epoch 3][Step 101], time=0.11334037780761719, ext_time=0.022902488708496094, train_time=0.06819343566894531
[Epoch 3][Step 102], time=0.1136770248413086, ext_time=0.022778749465942383, train_time=0.06873464584350586
[Epoch 3][Step 103], time=0.11326169967651367, ext_time=0.022892475128173828, train_time=0.06815004348754883
[Epoch 3][Step 104], time=0.11322212219238281, ext_time=0.022962570190429688, train_time=0.06809759140014648
[Epoch 3][Step 105], time=0.11284351348876953, ext_time=0.0229189395904541, train_time=0.06781411170959473
[Epoch 3][Step 106], time=0.11353707313537598, ext_time=0.023000001907348633, train_time=0.06832146644592285
[Epoch 3][Step 107], time=0.11350274085998535, ext_time=0.022966861724853516, train_time=0.06845641136169434
[Epoch 3][Step 108], time=0.11341428756713867, ext_time=0.022901535034179688, train_time=0.06834793090820312
[Epoch 3][Step 109], time=0.11284756660461426, ext_time=0.023046016693115234, train_time=0.06768679618835449
[Epoch 3][Step 110], time=0.11273980140686035, ext_time=0.022754430770874023, train_time=0.06782984733581543
[Epoch 3][Step 111], time=0.11363673210144043, ext_time=0.022907733917236328, train_time=0.06860589981079102
[Epoch 3][Step 112], time=0.11365652084350586, ext_time=0.022854328155517578, train_time=0.06882333755493164
[Epoch 3][Step 113], time=0.11357378959655762, ext_time=0.022821664810180664, train_time=0.06865191459655762
[Epoch 3][Step 114], time=0.1130516529083252, ext_time=0.022948026657104492, train_time=0.06799507141113281
[Epoch 3][Step 115], time=0.11523604393005371, ext_time=0.0228884220123291, train_time=0.06867051124572754
[Epoch 3][Step 116], time=0.11345148086547852, ext_time=0.022696256637573242, train_time=0.06860232353210449
[Epoch 3][Step 117], time=0.11320185661315918, ext_time=0.022929668426513672, train_time=0.06814098358154297
[Epoch 3][Step 118], time=0.11299300193786621, ext_time=0.02290177345275879, train_time=0.06800484657287598
[Epoch 3][Step 119], time=0.1129915714263916, ext_time=0.022958040237426758, train_time=0.06783509254455566
[Epoch 3][Step 120], time=0.1129770278930664, ext_time=0.022898435592651367, train_time=0.06790518760681152
[Epoch 3][Step 121], time=0.11310267448425293, ext_time=0.02288985252380371, train_time=0.0679922103881836
[Epoch 3][Step 122], time=0.11320781707763672, ext_time=0.022877216339111328, train_time=0.06825709342956543
[Epoch 3][Step 123], time=0.11320137977600098, ext_time=0.022838115692138672, train_time=0.06821322441101074
[Epoch 3][Step 124], time=0.11295580863952637, ext_time=0.02289867401123047, train_time=0.06800603866577148
[Epoch 3], time=15.483162641525269, loss=0.6931472420692444
    [Step(average) Profiler Level 1 E3 S999]
        L1  sample           0.022256 | send           0.000000
        L1  recv             0.000000 | copy           0.034514 | convert time 0.000000 | train  0.068225
        L1  feature nbytes 0.00 Bytes | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes    0.00 Bytes | remote nbytes 0.00 Bytes
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S999]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.034514
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S999]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000000 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.000000 | cache combine cache 0.000000 | cache combine remote 0.000000
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S999]
        p50.00_tail_logl2featcopy=0.035507
        p90.00_tail_logl2featcopy=0.036849
        p95.00_tail_logl2featcopy=0.037075
        p99.00_tail_logl2featcopy=0.041275
        p99.90_tail_logl2featcopy=0.063230
[CUDA] cuda: usage: 58.77 GB
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    6690 MB |   17643 MB |   17231 GB |   17224 GB |
|       from large pool |    6680 MB |   17633 MB |   17218 GB |   17211 GB |
|       from small pool |      10 MB |      16 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Active memory         |    6690 MB |   17643 MB |   17231 GB |   17224 GB |
|       from large pool |    6680 MB |   17633 MB |   17218 GB |   17211 GB |
|       from small pool |      10 MB |      16 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   44990 MB |   62314 MB |  100280 MB |   55290 MB |
|       from large pool |   44968 MB |   62292 MB |  100254 MB |   55286 MB |
|       from small pool |      22 MB |      22 MB |      26 MB |       4 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   12523 MB |   22549 MB |   13056 GB |   13044 GB |
|       from large pool |   12515 MB |   22543 MB |   13042 GB |   13030 GB |
|       from small pool |       7 MB |      10 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Allocations           |      72    |      99    |  143412    |  143340    |
|       from large pool |      23    |      44    |   67000    |   66977    |
|       from small pool |      49    |      60    |   76412    |   76363    |
|---------------------------------------------------------------------------|
| Active allocs         |      72    |      99    |  143412    |  143340    |
|       from large pool |      23    |      44    |   67000    |   66977    |
|       from small pool |      49    |      60    |   76412    |   76363    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      38    |      40    |      53    |      15    |
|       from large pool |      27    |      29    |      40    |      13    |
|       from small pool |      11    |      11    |      13    |       2    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      48    |      56    |   55207    |   55159    |
|       from large pool |      25    |      35    |   37271    |   37246    |
|       from small pool |      23    |      28    |   17936    |   17913    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 66.517938 seconds
[EPOCH_TIME] 16.629484 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 15.981723 seconds
worker 7 running with pid=63743
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=7, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005672, per step: 0.000045
worker 4 running with pid=63737
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=4, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.006429, per step: 0.000051
worker 2 running with pid=63732
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=2, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005706, per step: 0.000046
worker 6 running with pid=63741
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=6, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005472, per step: 0.000044
worker 5 running with pid=63739
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=5, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005983, per step: 0.000048
worker 3 running with pid=63735
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=3, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.006199, per step: 0.000050
worker 1 running with pid=63730
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  153606500, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536, 3266219078,  317074742, 1665128055,
        3225579545,  427334013,  150148188,  726851972, 1000854521, 1061370191,
         213395108,  526478766,  115662647, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  485049346,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         130713544,  372892975, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  449947793, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  466346177,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,   77640938,
         645516367, 3053389785, 3376243999,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  342702905,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075, 3454564992, 1866677628,
        3557612909, 3295574170, 1840253021,  589845084,   38434987, 3025516425,
         133597469,  597513635, 2348166713, 3535021406])
Rank=1, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005789, per step: 0.000046

