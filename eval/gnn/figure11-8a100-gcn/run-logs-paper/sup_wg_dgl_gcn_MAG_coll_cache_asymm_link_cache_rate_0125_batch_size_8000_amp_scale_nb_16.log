succeed=True
config:eval_tsp="2023-08-06 09:54:28"
config:num_worker=8
config:num_intra_size=8
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=mag240m-homo
config:epochs=4
config:batchsize=8000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=True
config:use_collcache=False
config:cache_percentage=0.25
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=56
config:unsupervised=False
config:classnum=153
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7f75632a1790>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=8, world_size=8
Rank=0, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.20925664901733398
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 3.337860107421875e-06, transfer latency: 0.15512514114379883
epoch=4 total_steps=68
start training...
[Epoch 0][Step 0], time=2.006146192550659, ext_time=0.04605221748352051, train_time=1.9417381286621094
[Epoch 0][Step 1], time=0.11028695106506348, ext_time=0.021845340728759766, train_time=0.07887506484985352
[Epoch 0][Step 2], time=0.10416769981384277, ext_time=0.024471759796142578, train_time=0.07029414176940918
[Epoch 0][Step 3], time=0.0980379581451416, ext_time=0.02360677719116211, train_time=0.0650184154510498
[Epoch 0][Step 4], time=0.11079907417297363, ext_time=0.02324390411376953, train_time=0.07814383506774902
[Epoch 0][Step 5], time=0.09709763526916504, ext_time=0.02312302589416504, train_time=0.06450271606445312
[Epoch 0][Step 6], time=0.10697388648986816, ext_time=0.02261829376220703, train_time=0.0749204158782959
[Epoch 0][Step 7], time=0.09686756134033203, ext_time=0.019314050674438477, train_time=0.06165003776550293
[Epoch 0][Step 8], time=0.09655952453613281, ext_time=0.022274255752563477, train_time=0.06473588943481445
[Epoch 0][Step 9], time=0.09803342819213867, ext_time=0.021871089935302734, train_time=0.0667116641998291
[Epoch 0][Step 10], time=0.09675765037536621, ext_time=0.02053380012512207, train_time=0.06691670417785645
[Epoch 0][Step 11], time=0.0969233512878418, ext_time=0.02222442626953125, train_time=0.06510639190673828
[Epoch 0][Step 12], time=0.09753108024597168, ext_time=0.023390531539916992, train_time=0.06470537185668945
[Epoch 0][Step 13], time=0.09786653518676758, ext_time=0.023609638214111328, train_time=0.06485748291015625
[Epoch 0][Step 14], time=0.09655523300170898, ext_time=0.024532794952392578, train_time=0.06259417533874512
[Epoch 0][Step 15], time=0.09762811660766602, ext_time=0.02298283576965332, train_time=0.06387543678283691
[Epoch 0][Step 16], time=0.09699535369873047, ext_time=0.02291131019592285, train_time=0.06468987464904785
[Epoch 0], time=3.6063790321350098, loss=4.986957550048828
[Epoch 1][Step 0], time=0.6903431415557861, ext_time=0.024625539779663086, train_time=0.6558680534362793
[Epoch 1][Step 1], time=0.09695744514465332, ext_time=0.022503137588500977, train_time=0.06497979164123535
[Epoch 1][Step 2], time=0.0975649356842041, ext_time=0.024664878845214844, train_time=0.06346344947814941
[Epoch 1][Step 3], time=0.0974435806274414, ext_time=0.022721290588378906, train_time=0.06528663635253906
[Epoch 1][Step 4], time=0.0980682373046875, ext_time=0.023147106170654297, train_time=0.06556463241577148
[Epoch 1][Step 5], time=0.09709668159484863, ext_time=0.024133682250976562, train_time=0.06347370147705078
[Epoch 1][Step 6], time=0.09708452224731445, ext_time=0.023099184036254883, train_time=0.06452202796936035
[Epoch 1][Step 7], time=0.09741044044494629, ext_time=0.023797988891601562, train_time=0.06418728828430176
[Epoch 1][Step 8], time=0.09754276275634766, ext_time=0.022533655166625977, train_time=0.0654759407043457
[Epoch 1][Step 9], time=0.09697198867797852, ext_time=0.021854162216186523, train_time=0.06566262245178223
[Epoch 1][Step 10], time=0.09784317016601562, ext_time=0.01991581916809082, train_time=0.06865406036376953
[Epoch 1][Step 11], time=0.09659719467163086, ext_time=0.02207660675048828, train_time=0.06494975090026855
[Epoch 1][Step 12], time=0.0967257022857666, ext_time=0.02297210693359375, train_time=0.06430411338806152
[Epoch 1][Step 13], time=0.09807443618774414, ext_time=0.024328947067260742, train_time=0.06432700157165527
[Epoch 1][Step 14], time=0.09696459770202637, ext_time=0.024514198303222656, train_time=0.0630033016204834
[Epoch 1][Step 15], time=0.0977022647857666, ext_time=0.021440505981445312, train_time=0.06475424766540527
[Epoch 1][Step 16], time=0.09705710411071777, ext_time=0.022916555404663086, train_time=0.06476330757141113
[Epoch 1], time=2.248539447784424, loss=4.943299770355225
[Epoch 2][Step 0], time=0.6449263095855713, ext_time=0.024127960205078125, train_time=0.610938549041748
[Epoch 2][Step 1], time=0.09760260581970215, ext_time=0.023848533630371094, train_time=0.06431770324707031
[Epoch 2][Step 2], time=0.09750771522521973, ext_time=0.02491927146911621, train_time=0.06314301490783691
[Epoch 2][Step 3], time=0.09689545631408691, ext_time=0.022023677825927734, train_time=0.0654451847076416
[Epoch 2][Step 4], time=0.09783363342285156, ext_time=0.023662567138671875, train_time=0.06481337547302246
[Epoch 2][Step 5], time=0.09645652770996094, ext_time=0.02287602424621582, train_time=0.0640866756439209
[Epoch 2][Step 6], time=0.09719204902648926, ext_time=0.023891210556030273, train_time=0.0638895034790039
[Epoch 2][Step 7], time=0.09674406051635742, ext_time=0.023441553115844727, train_time=0.0638282299041748
[Epoch 2][Step 8], time=0.09659957885742188, ext_time=0.022797346115112305, train_time=0.06428790092468262
[Epoch 2][Step 9], time=0.09736251831054688, ext_time=0.0238494873046875, train_time=0.06409502029418945
[Epoch 2][Step 10], time=0.0977177619934082, ext_time=0.02032756805419922, train_time=0.0680854320526123
[Epoch 2][Step 11], time=0.09691929817199707, ext_time=0.02228999137878418, train_time=0.06505084037780762
[Epoch 2][Step 12], time=0.09760761260986328, ext_time=0.02290964126586914, train_time=0.06528234481811523
[Epoch 2][Step 13], time=0.0980372428894043, ext_time=0.02021336555480957, train_time=0.06403040885925293
[Epoch 2][Step 14], time=0.09690618515014648, ext_time=0.024875164031982422, train_time=0.06258392333984375
[Epoch 2][Step 15], time=0.09762358665466309, ext_time=0.02411651611328125, train_time=0.06410384178161621
[Epoch 2][Step 16], time=0.09720396995544434, ext_time=0.0231935977935791, train_time=0.06459522247314453
[Epoch 2], time=2.2022368907928467, loss=4.902449607849121
[Epoch 3][Step 0], time=0.20685815811157227, ext_time=0.02385687828063965, train_time=0.1730809211730957
[Epoch 3][Step 1], time=0.09729719161987305, ext_time=0.021745920181274414, train_time=0.06614470481872559
[Epoch 3][Step 2], time=0.09747099876403809, ext_time=0.024309635162353516, train_time=0.0637202262878418
[Epoch 3][Step 3], time=0.09722542762756348, ext_time=0.02504587173461914, train_time=0.06273460388183594
[Epoch 3][Step 4], time=0.09641361236572266, ext_time=0.02236032485961914, train_time=0.0647122859954834
[Epoch 3][Step 5], time=0.096893310546875, ext_time=0.023575305938720703, train_time=0.06382298469543457
[Epoch 3][Step 6], time=0.09783267974853516, ext_time=0.0213930606842041, train_time=0.06696963310241699
[Epoch 3][Step 7], time=0.09656810760498047, ext_time=0.018855810165405273, train_time=0.06174969673156738
[Epoch 3][Step 8], time=0.09722042083740234, ext_time=0.02164745330810547, train_time=0.06607437133789062
[Epoch 3][Step 9], time=0.09764289855957031, ext_time=0.023655176162719727, train_time=0.06452512741088867
[Epoch 3][Step 10], time=0.09661316871643066, ext_time=0.019436120986938477, train_time=0.06787347793579102
[Epoch 3][Step 11], time=0.0968637466430664, ext_time=0.02233600616455078, train_time=0.06493425369262695
[Epoch 3][Step 12], time=0.09685111045837402, ext_time=0.023392438888549805, train_time=0.06394696235656738
    [Step(average) Profiler Level 1 E3 S135]
        L1  sample           0.009649 | send           0.000000
        L1  recv             0.000000 | copy           0.023750 | convert time 0.000000 | train  0.088436
        L1  feature nbytes 0.00 Bytes | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes    0.00 Bytes | remote nbytes 0.00 Bytes
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S135]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.023750
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S135]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000000 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.000000 | cache combine cache 0.000000 | cache combine remote 0.000000
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S135]
        p50.00_tail_logl2featcopy=0.023754
        p90.00_tail_logl2featcopy=0.025811
        p95.00_tail_logl2featcopy=0.026202
        p99.00_tail_logl2featcopy=0.047964
        p99.90_tail_logl2featcopy=0.049559
[CUDA] cuda: usage: 79.12 GB
[Epoch 3][Step 13], time=0.09754371643066406, ext_time=0.02353835105895996, train_time=0.06461238861083984
[Epoch 3][Step 14], time=0.09708380699157715, ext_time=0.02360677719116211, train_time=0.06402468681335449
[Epoch 3][Step 15], time=0.09756970405578613, ext_time=0.02050471305847168, train_time=0.06398582458496094
[Epoch 3][Step 16], time=0.09754109382629395, ext_time=0.023511648178100586, train_time=0.06461715698242188
[Epoch 3], time=1.7625346183776855, loss=4.864315986633301
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3654 MB |   15939 MB |    1753 GB |    1750 GB |
|       from large pool |    3646 MB |   15931 MB |    1751 GB |    1748 GB |
|       from small pool |       8 MB |      14 MB |       1 GB |       1 GB |
|---------------------------------------------------------------------------|
| Active memory         |    3654 MB |   15939 MB |    1753 GB |    1750 GB |
|       from large pool |    3646 MB |   15931 MB |    1751 GB |    1748 GB |
|       from small pool |       8 MB |      14 MB |       1 GB |       1 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   28986 MB |   28986 MB |   28986 MB |       0 B  |
|       from large pool |   28968 MB |   28968 MB |   28968 MB |       0 B  |
|       from small pool |      18 MB |      18 MB |      18 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4801 MB |   12001 MB |    1091 GB |    1086 GB |
|       from large pool |    4795 MB |   11993 MB |    1089 GB |    1085 GB |
|       from small pool |       5 MB |       8 MB |       1 GB |       1 GB |
|---------------------------------------------------------------------------|
| Allocations           |      65    |     101    |   17174    |   17109    |
|       from large pool |      23    |      50    |    8435    |    8412    |
|       from small pool |      42    |      53    |    8739    |    8697    |
|---------------------------------------------------------------------------|
| Active allocs         |      65    |     101    |   17174    |   17109    |
|       from large pool |      23    |      50    |    8435    |    8412    |
|       from small pool |      42    |      53    |    8739    |    8697    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      28    |      28    |      28    |       0    |
|       from large pool |      19    |      19    |      19    |       0    |
|       from small pool |       9    |       9    |       9    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      40    |      52    |    6637    |    6597    |
|       from large pool |      18    |      29    |    4219    |    4201    |
|       from small pool |      22    |      26    |    2418    |    2396    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 9.820281 seconds
[EPOCH_TIME] 2.455070 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 1.982509 seconds
Rank=7, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.22516822814941406
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 3.337860107421875e-06, transfer latency: 0.16383600234985352
Rank=6, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.21217751502990723
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 3.5762786865234375e-06, transfer latency: 0.15426278114318848
Rank=5, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.21785330772399902
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 3.337860107421875e-06, transfer latency: 0.1573643684387207
Rank=3, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.218064546585083
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 4.291534423828125e-06, transfer latency: 0.15731120109558105
Rank=1, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.21708965301513672
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 4.0531158447265625e-06, transfer latency: 0.15812182426452637
Rank=4, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.2167809009552002
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 3.337860107421875e-06, transfer latency: 0.1572721004486084
Rank=2, Graph loaded.
!!!!Train_dataloader(with 17 items) enumerate latency: 0.22275042533874512
torch.Size([8000]) torch.Size([8000])
torch.Size([8400]) torch.Size([8400])
!!!!Train_data_list(with 17 items) enumerate latency: 4.76837158203125e-06, transfer latency: 0.15909457206726074

