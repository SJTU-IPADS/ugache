succeed=True
[CUDA] cuda: usage: 6.91 GB
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
0 :  {link #0 : 1}, {link #1 : 2}, {link #2 : 3}, {link #3 : 4}, {link #4 : 5}, {link #5 : 6}, {link #6 : 7},
1 :  {link #0 : 2}, {link #1 : 3}, {link #2 : 4}, {link #3 : 5}, {link #4 : 6}, {link #5 : 7}, {link #6 : 0},
2 :  {link #0 : 3}, {link #1 : 4}, {link #2 : 5}, {link #3 : 6}, {link #4 : 7}, {link #5 : 0}, {link #6 : 1},
3 :  {link #0 : 4}, {link #1 : 5}, {link #2 : 6}, {link #3 : 7}, {link #4 : 0}, {link #5 : 1}, {link #6 : 2},
4 :  {link #0 : 5}, {link #1 : 6}, {link #2 : 7}, {link #3 : 0}, {link #4 : 1}, {link #5 : 2}, {link #6 : 3},
5 :  {link #0 : 6}, {link #1 : 7}, {link #2 : 0}, {link #3 : 1}, {link #4 : 2}, {link #5 : 3}, {link #6 : 4},
6 :  {link #0 : 7}, {link #1 : 0}, {link #2 : 1}, {link #3 : 2}, {link #4 : 3}, {link #5 : 4}, {link #6 : 5},
7 :  {link #0 : 0}, {link #1 : 1}, {link #2 : 2}, {link #3 : 3}, {link #4 : 4}, {link #5 : 5}, {link #6 : 6},
0 : local 108, cpu 0 {link #0 : g1 0}, {link #1 : g2 0}, {link #2 : g3 0}, {link #3 : g4 0}, {link #4 : g5 0}, {link #5 : g6 0}, {link #6 : g7 0},
1 : local 108, cpu 0 {link #0 : g2 0}, {link #1 : g3 0}, {link #2 : g4 0}, {link #3 : g5 0}, {link #4 : g6 0}, {link #5 : g7 0}, {link #6 : g0 0},
2 : local 108, cpu 0 {link #0 : g3 0}, {link #1 : g4 0}, {link #2 : g5 0}, {link #3 : g6 0}, {link #4 : g7 0}, {link #5 : g0 0}, {link #6 : g1 0},
3 : local 108, cpu 0 {link #0 : g4 0}, {link #1 : g5 0}, {link #2 : g6 0}, {link #3 : g7 0}, {link #4 : g0 0}, {link #5 : g1 0}, {link #6 : g2 0},
4 : local 108, cpu 0 {link #0 : g5 0}, {link #1 : g6 0}, {link #2 : g7 0}, {link #3 : g0 0}, {link #4 : g1 0}, {link #5 : g2 0}, {link #6 : g3 0},
5 : local 108, cpu 0 {link #0 : g6 0}, {link #1 : g7 0}, {link #2 : g0 0}, {link #3 : g1 0}, {link #4 : g2 0}, {link #5 : g3 0}, {link #6 : g4 0},
6 : local 108, cpu 0 {link #0 : g7 0}, {link #1 : g0 0}, {link #2 : g1 0}, {link #3 : g2 0}, {link #4 : g3 0}, {link #5 : g4 0}, {link #6 : g5 0},
7 : local 108, cpu 0 {link #0 : g0 0}, {link #1 : g1 0}, {link #2 : g2 0}, {link #3 : g3 0}, {link #4 : g4 0}, {link #5 : g5 0}, {link #6 : g6 0},
test_result:init:cache_nbytes=56862697472
test_result:init:cache_nbytes=56862697472
test_result:init:cache_nbytes=56862697472
test_result:init:cache_nbytes=56862697472
test_result:init:cache_nbytes=56862697472
test_result:init:cache_nbytes=56862697472
test_result:init:cache_nbytes=56862697472
test_result:init:cache_nbytes=56862697472
worker 0 running with pid=57403
config:eval_tsp="2023-08-06 18:57:40"
config:num_worker=8
config:num_intra_size=8
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=ogbn-papers100M
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=False
config:use_collcache=True
config:cache_percentage=1.0
config:cache_policy=rep
config:omp_thread_num=56
config:unsupervised=True
config:classnum=172
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7f7c66c479d0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=8, world_size=8
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=0, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005658, per step: 0.000045
epoch=4 total_steps=500
presamping
presamping takes 36.32094717025757
start training...
[Epoch 0][Step 0], time=2.0468344688415527, ext_time=0.027704954147338867, train_time=1.9962544441223145
[Epoch 0][Step 1], time=0.09193730354309082, ext_time=0.004191160202026367, train_time=0.05375337600708008
[Epoch 0][Step 2], time=0.07424402236938477, ext_time=0.004157066345214844, train_time=0.04875683784484863
[Epoch 0][Step 3], time=0.074554443359375, ext_time=0.004096508026123047, train_time=0.04914665222167969
[Epoch 0][Step 4], time=0.3670053482055664, ext_time=0.0041005611419677734, train_time=0.3415713310241699
[Epoch 0][Step 5], time=0.07256388664245605, ext_time=0.004075765609741211, train_time=0.04724836349487305
[Epoch 0][Step 6], time=0.07366418838500977, ext_time=0.004085063934326172, train_time=0.04834127426147461
[Epoch 0][Step 7], time=0.06909990310668945, ext_time=0.0040857791900634766, train_time=0.04374551773071289
[Epoch 0][Step 8], time=0.07288718223571777, ext_time=0.004091501235961914, train_time=0.047516584396362305
[Epoch 0][Step 9], time=0.06927490234375, ext_time=0.004070281982421875, train_time=0.044036865234375
[Epoch 0][Step 10], time=0.06864166259765625, ext_time=0.004086732864379883, train_time=0.0432429313659668
[Epoch 0][Step 11], time=0.0693972110748291, ext_time=0.00409245491027832, train_time=0.04402637481689453
[Epoch 0][Step 12], time=0.07441329956054688, ext_time=0.004112958908081055, train_time=0.048892974853515625
[Epoch 0][Step 13], time=0.0689845085144043, ext_time=0.004109382629394531, train_time=0.04351520538330078
[Epoch 0][Step 14], time=0.07119083404541016, ext_time=0.004088401794433594, train_time=0.045847177505493164
[Epoch 0][Step 15], time=0.06875944137573242, ext_time=0.0040934085845947266, train_time=0.04336190223693848
[Epoch 0][Step 16], time=0.06923794746398926, ext_time=0.004077434539794922, train_time=0.04390668869018555
[Epoch 0][Step 17], time=0.06986618041992188, ext_time=0.004109621047973633, train_time=0.044345855712890625
[Epoch 0][Step 18], time=0.06885075569152832, ext_time=0.004105091094970703, train_time=0.043349504470825195
[Epoch 0][Step 19], time=0.07330894470214844, ext_time=0.0040950775146484375, train_time=0.04790639877319336
[Epoch 0][Step 20], time=0.0695807933807373, ext_time=0.004112958908081055, train_time=0.044123172760009766
[Epoch 0][Step 21], time=0.07223200798034668, ext_time=0.004083395004272461, train_time=0.04692721366882324
[Epoch 0][Step 22], time=0.06921601295471191, ext_time=0.004113912582397461, train_time=0.043796539306640625
[Epoch 0][Step 23], time=0.06912708282470703, ext_time=0.004105806350708008, train_time=0.04362034797668457
[Epoch 0][Step 24], time=0.06984972953796387, ext_time=0.0041141510009765625, train_time=0.04437708854675293
[Epoch 0][Step 25], time=0.32282590866088867, ext_time=0.004118442535400391, train_time=0.29735755920410156
[Epoch 0][Step 26], time=0.06961631774902344, ext_time=0.004097461700439453, train_time=0.044103145599365234
[Epoch 0][Step 27], time=0.06907534599304199, ext_time=0.004097938537597656, train_time=0.04366922378540039
[Epoch 0][Step 28], time=0.06885385513305664, ext_time=0.0041010379791259766, train_time=0.043352603912353516
[Epoch 0][Step 29], time=0.0695037841796875, ext_time=0.004102468490600586, train_time=0.04408526420593262
[Epoch 0][Step 30], time=0.3659353256225586, ext_time=0.004095315933227539, train_time=0.33257579803466797
[Epoch 0][Step 31], time=0.07147359848022461, ext_time=0.00411677360534668, train_time=0.04545092582702637
[Epoch 0][Step 32], time=0.07840538024902344, ext_time=0.004109859466552734, train_time=0.05289030075073242
[Epoch 0][Step 33], time=0.06895613670349121, ext_time=0.0040857791900634766, train_time=0.043586015701293945
[Epoch 0][Step 34], time=0.0693354606628418, ext_time=0.004104137420654297, train_time=0.04369711875915527
[Epoch 0][Step 35], time=0.07022643089294434, ext_time=0.004082918167114258, train_time=0.04483509063720703
[Epoch 0][Step 36], time=0.07042121887207031, ext_time=0.0041239261627197266, train_time=0.04486656188964844
[Epoch 0][Step 37], time=0.069000244140625, ext_time=0.004088878631591797, train_time=0.04365849494934082
[Epoch 0][Step 38], time=0.06884098052978516, ext_time=0.004098415374755859, train_time=0.043351173400878906
[Epoch 0][Step 39], time=0.07062005996704102, ext_time=0.004103183746337891, train_time=0.045111656188964844
[Epoch 0][Step 40], time=0.0691680908203125, ext_time=0.004159212112426758, train_time=0.04358649253845215
[Epoch 0][Step 41], time=0.36400580406188965, ext_time=0.004102230072021484, train_time=0.3385326862335205
[Epoch 0][Step 42], time=0.07186484336853027, ext_time=0.004098415374755859, train_time=0.04648613929748535
[Epoch 0][Step 43], time=0.0689382553100586, ext_time=0.004076242446899414, train_time=0.043648481369018555
[Epoch 0][Step 44], time=0.0698850154876709, ext_time=0.0041027069091796875, train_time=0.04434537887573242
[Epoch 0][Step 45], time=0.35938596725463867, ext_time=0.004117012023925781, train_time=0.33395886421203613
[Epoch 0][Step 46], time=0.07161688804626465, ext_time=0.0041027069091796875, train_time=0.046137332916259766
[Epoch 0][Step 47], time=0.06998920440673828, ext_time=0.004098415374755859, train_time=0.04451298713684082
[Epoch 0][Step 48], time=0.07157206535339355, ext_time=0.004102468490600586, train_time=0.0460662841796875
[Epoch 0][Step 49], time=0.0691213607788086, ext_time=0.004087924957275391, train_time=0.043746232986450195
[Epoch 0][Step 50], time=0.06896138191223145, ext_time=0.004088401794433594, train_time=0.043588876724243164
[Epoch 0][Step 51], time=0.0687410831451416, ext_time=0.004101276397705078, train_time=0.043291330337524414
[Epoch 0][Step 52], time=0.07029008865356445, ext_time=0.004105567932128906, train_time=0.0448155403137207
[Epoch 0][Step 53], time=0.06971454620361328, ext_time=0.0040891170501708984, train_time=0.044332265853881836
[Epoch 0][Step 54], time=0.06855082511901855, ext_time=0.004088163375854492, train_time=0.04328346252441406
[Epoch 0][Step 55], time=0.06901097297668457, ext_time=0.0041046142578125, train_time=0.04351353645324707
[Epoch 0][Step 56], time=0.06899905204772949, ext_time=0.004093170166015625, train_time=0.043572425842285156
[Epoch 0][Step 57], time=0.06914496421813965, ext_time=0.004087686538696289, train_time=0.04374337196350098
[Epoch 0][Step 58], time=0.06957769393920898, ext_time=0.004100322723388672, train_time=0.044109344482421875
[Epoch 0][Step 59], time=0.06866908073425293, ext_time=0.004077911376953125, train_time=0.043296098709106445
[Epoch 0][Step 60], time=0.06898760795593262, ext_time=0.004076957702636719, train_time=0.04363059997558594
[Epoch 0][Step 61], time=0.06885623931884766, ext_time=0.004080533981323242, train_time=0.0435338020324707
[Epoch 0][Step 62], time=0.30191588401794434, ext_time=0.0041065216064453125, train_time=0.27641725540161133
[Epoch 0][Step 63], time=0.06966161727905273, ext_time=0.004097700119018555, train_time=0.044213294982910156
[Epoch 0][Step 64], time=0.06942486763000488, ext_time=0.004097461700439453, train_time=0.04403042793273926
[Epoch 0][Step 65], time=0.06949114799499512, ext_time=0.004086494445800781, train_time=0.044141292572021484
[Epoch 0][Step 66], time=0.0690312385559082, ext_time=0.004096269607543945, train_time=0.04358553886413574
[Epoch 0][Step 67], time=0.06906652450561523, ext_time=0.004098653793334961, train_time=0.04360055923461914
[Epoch 0][Step 68], time=0.0692434310913086, ext_time=0.0041027069091796875, train_time=0.04328656196594238
[Epoch 0][Step 69], time=0.06901741027832031, ext_time=0.004081010818481445, train_time=0.04360628128051758
[Epoch 0][Step 70], time=0.06944012641906738, ext_time=0.0040819644927978516, train_time=0.04410123825073242
[Epoch 0][Step 71], time=0.06899094581604004, ext_time=0.004109382629394531, train_time=0.04347658157348633
[Epoch 0][Step 72], time=0.0758066177368164, ext_time=0.004095554351806641, train_time=0.043572187423706055
[Epoch 0][Step 73], time=0.06962418556213379, ext_time=0.00412297248840332, train_time=0.044057369232177734
[Epoch 0][Step 74], time=0.06891918182373047, ext_time=0.004101276397705078, train_time=0.043454885482788086
[Epoch 0][Step 75], time=0.39960503578186035, ext_time=0.004087924957275391, train_time=0.3741939067840576
[Epoch 0][Step 76], time=0.06911396980285645, ext_time=0.004111766815185547, train_time=0.04362082481384277
[Epoch 0][Step 77], time=0.06896662712097168, ext_time=0.00411224365234375, train_time=0.04342031478881836
[Epoch 0][Step 78], time=0.07071590423583984, ext_time=0.00408935546875, train_time=0.0453336238861084
[Epoch 0][Step 79], time=0.06957364082336426, ext_time=0.004106760025024414, train_time=0.04409646987915039
[Epoch 0][Step 80], time=0.4825761318206787, ext_time=0.004091024398803711, train_time=0.4571833610534668
[Epoch 0][Step 81], time=0.06889843940734863, ext_time=0.004093647003173828, train_time=0.04343461990356445
[Epoch 0][Step 82], time=0.07132506370544434, ext_time=0.004129886627197266, train_time=0.04571127891540527
[Epoch 0][Step 83], time=0.06915426254272461, ext_time=0.0041539669036865234, train_time=0.04360389709472656
[Epoch 0][Step 84], time=0.06973123550415039, ext_time=0.004089832305908203, train_time=0.04431748390197754
[Epoch 0][Step 85], time=0.07015132904052734, ext_time=0.0040819644927978516, train_time=0.044811248779296875
[Epoch 0][Step 86], time=0.06882834434509277, ext_time=0.004091978073120117, train_time=0.043396711349487305
[Epoch 0][Step 87], time=0.07742547988891602, ext_time=0.00411224365234375, train_time=0.05191779136657715
[Epoch 0][Step 88], time=0.06901264190673828, ext_time=0.00407719612121582, train_time=0.04364657402038574
[Epoch 0][Step 89], time=0.06901788711547852, ext_time=0.004122018814086914, train_time=0.043473005294799805
[Epoch 0][Step 90], time=0.06902027130126953, ext_time=0.004087686538696289, train_time=0.04366326332092285
[Epoch 0][Step 91], time=0.06892967224121094, ext_time=0.004113197326660156, train_time=0.04337310791015625
[Epoch 0][Step 92], time=0.06923556327819824, ext_time=0.004100322723388672, train_time=0.04385185241699219
[Epoch 0][Step 93], time=0.06903505325317383, ext_time=0.004111766815185547, train_time=0.04354548454284668
[Epoch 0][Step 94], time=0.07088971138000488, ext_time=0.004095792770385742, train_time=0.045408010482788086
[Epoch 0][Step 95], time=0.06877756118774414, ext_time=0.004086732864379883, train_time=0.04338645935058594
[Epoch 0][Step 96], time=0.33138251304626465, ext_time=0.004118919372558594, train_time=0.30581140518188477
[Epoch 0][Step 97], time=0.06939506530761719, ext_time=0.004096031188964844, train_time=0.044090986251831055
[Epoch 0][Step 98], time=0.06885623931884766, ext_time=0.0040798187255859375, train_time=0.04346036911010742
[Epoch 0][Step 99], time=0.06987738609313965, ext_time=0.004105329513549805, train_time=0.0443882942199707
[Epoch 0][Step 100], time=0.06887435913085938, ext_time=0.004082679748535156, train_time=0.043528079986572266
[Epoch 0][Step 101], time=0.06952238082885742, ext_time=0.004101991653442383, train_time=0.0440526008605957
[Epoch 0][Step 102], time=0.06932640075683594, ext_time=0.00411534309387207, train_time=0.04376697540283203
[Epoch 0][Step 103], time=0.36556267738342285, ext_time=0.004131793975830078, train_time=0.33992791175842285
[Epoch 0][Step 104], time=0.0697934627532959, ext_time=0.0042362213134765625, train_time=0.04334306716918945
[Epoch 0][Step 105], time=0.06905007362365723, ext_time=0.004101276397705078, train_time=0.04354977607727051
[Epoch 0][Step 106], time=0.06909966468811035, ext_time=0.004147529602050781, train_time=0.043593645095825195
[Epoch 0][Step 107], time=0.06939148902893066, ext_time=0.0040836334228515625, train_time=0.04404735565185547
[Epoch 0][Step 108], time=0.06905269622802734, ext_time=0.004087686538696289, train_time=0.043698787689208984
[Epoch 0][Step 109], time=0.0687253475189209, ext_time=0.00410771369934082, train_time=0.04326176643371582
[Epoch 0][Step 110], time=0.06925153732299805, ext_time=0.004070758819580078, train_time=0.0439603328704834
[Epoch 0][Step 111], time=0.0697031021118164, ext_time=0.00407099723815918, train_time=0.04444169998168945
[Epoch 0][Step 112], time=0.07100033760070801, ext_time=0.0041103363037109375, train_time=0.04541420936584473
[Epoch 0][Step 113], time=0.06922054290771484, ext_time=0.004149436950683594, train_time=0.043734073638916016
[Epoch 0][Step 114], time=0.07509636878967285, ext_time=0.004099607467651367, train_time=0.043585777282714844
[Epoch 0][Step 115], time=0.06902480125427246, ext_time=0.0041081905364990234, train_time=0.043523550033569336
[Epoch 0][Step 116], time=0.06902265548706055, ext_time=0.004125356674194336, train_time=0.04342460632324219
[Epoch 0][Step 117], time=0.06972408294677734, ext_time=0.004110574722290039, train_time=0.044260263442993164
[Epoch 0][Step 118], time=0.07010245323181152, ext_time=0.00410151481628418, train_time=0.04469013214111328
[Epoch 0][Step 119], time=0.06911468505859375, ext_time=0.0041196346282958984, train_time=0.043554067611694336
[Epoch 0][Step 120], time=0.06901168823242188, ext_time=0.0040743350982666016, train_time=0.04371237754821777
[Epoch 0][Step 121], time=0.06891536712646484, ext_time=0.004095315933227539, train_time=0.04342532157897949
[Epoch 0][Step 122], time=0.06894135475158691, ext_time=0.00409698486328125, train_time=0.043473243713378906
[Epoch 0][Step 123], time=0.06884431838989258, ext_time=0.004093170166015625, train_time=0.043439388275146484
[Epoch 0][Step 124], time=0.06882905960083008, ext_time=0.004093647003173828, train_time=0.04340553283691406
[Epoch 0], time=13.713508129119873, loss=0.4540254473686218
[Epoch 1][Step 0], time=0.07015061378479004, ext_time=0.004121541976928711, train_time=0.0445098876953125
[Epoch 1][Step 1], time=0.06878376007080078, ext_time=0.004105329513549805, train_time=0.04332256317138672
[Epoch 1][Step 2], time=0.06920242309570312, ext_time=0.004096031188964844, train_time=0.04373812675476074
[Epoch 1][Step 3], time=0.06894183158874512, ext_time=0.004108428955078125, train_time=0.043436527252197266
[Epoch 1][Step 4], time=0.06902360916137695, ext_time=0.004110097885131836, train_time=0.04354047775268555
[Epoch 1][Step 5], time=0.06873774528503418, ext_time=0.004083395004272461, train_time=0.04331660270690918
[Epoch 1][Step 6], time=0.06909728050231934, ext_time=0.004099130630493164, train_time=0.043607234954833984
[Epoch 1][Step 7], time=0.06919431686401367, ext_time=0.004106044769287109, train_time=0.04372048377990723
[Epoch 1][Step 8], time=0.06931710243225098, ext_time=0.004090785980224609, train_time=0.0439000129699707
[Epoch 1][Step 9], time=0.06873822212219238, ext_time=0.004105567932128906, train_time=0.04326009750366211
[Epoch 1][Step 10], time=0.06899595260620117, ext_time=0.004101276397705078, train_time=0.043496131896972656
[Epoch 1][Step 11], time=0.06963944435119629, ext_time=0.0041081905364990234, train_time=0.04416394233703613
[Epoch 1][Step 12], time=0.07001996040344238, ext_time=0.004097938537597656, train_time=0.04454493522644043
[Epoch 1][Step 13], time=0.06903934478759766, ext_time=0.004096269607543945, train_time=0.043604135513305664
[Epoch 1][Step 14], time=0.06911134719848633, ext_time=0.0040798187255859375, train_time=0.04377174377441406
[Epoch 1][Step 15], time=0.06891226768493652, ext_time=0.0040874481201171875, train_time=0.04346632957458496
[Epoch 1][Step 16], time=0.06879138946533203, ext_time=0.004106998443603516, train_time=0.0432896614074707
[Epoch 1][Step 17], time=0.07002377510070801, ext_time=0.004097461700439453, train_time=0.044629573822021484
[Epoch 1][Step 18], time=0.06851506233215332, ext_time=0.0040895938873291016, train_time=0.04322314262390137
[Epoch 1][Step 19], time=0.06864809989929199, ext_time=0.0040740966796875, train_time=0.043294429779052734
[Epoch 1][Step 20], time=0.06869339942932129, ext_time=0.004103183746337891, train_time=0.043228864669799805
[Epoch 1][Step 21], time=0.07076311111450195, ext_time=0.004099369049072266, train_time=0.04541015625
[Epoch 1][Step 22], time=0.06906986236572266, ext_time=0.004082441329956055, train_time=0.04374957084655762
[Epoch 1][Step 23], time=0.06911277770996094, ext_time=0.0041046142578125, train_time=0.04365801811218262
[Epoch 1][Step 24], time=0.0689077377319336, ext_time=0.004104137420654297, train_time=0.043396711349487305
[Epoch 1][Step 25], time=0.06912851333618164, ext_time=0.0041239261627197266, train_time=0.04353809356689453
[Epoch 1][Step 26], time=0.06980085372924805, ext_time=0.004095315933227539, train_time=0.044345855712890625
[Epoch 1][Step 27], time=0.06987476348876953, ext_time=0.0041065216064453125, train_time=0.043418169021606445
[Epoch 1][Step 28], time=0.06909656524658203, ext_time=0.004114627838134766, train_time=0.043509483337402344
[Epoch 1][Step 29], time=0.06952691078186035, ext_time=0.004103660583496094, train_time=0.044059038162231445
[Epoch 1][Step 30], time=0.06894493103027344, ext_time=0.0041043758392333984, train_time=0.04345703125
[Epoch 1][Step 31], time=0.07589960098266602, ext_time=0.004122257232666016, train_time=0.04354739189147949
[Epoch 1][Step 32], time=0.06998372077941895, ext_time=0.0041120052337646484, train_time=0.044480323791503906
[Epoch 1][Step 33], time=0.06904077529907227, ext_time=0.004090070724487305, train_time=0.0436251163482666
[Epoch 1][Step 34], time=0.06908369064331055, ext_time=0.004093647003173828, train_time=0.0436253547668457
[Epoch 1][Step 35], time=0.0691688060760498, ext_time=0.004092216491699219, train_time=0.043786048889160156
[Epoch 1][Step 36], time=0.06895613670349121, ext_time=0.004100799560546875, train_time=0.04349565505981445
[Epoch 1][Step 37], time=0.31656861305236816, ext_time=0.004114866256713867, train_time=0.2910304069519043
[Epoch 1][Step 38], time=0.07468962669372559, ext_time=0.004126548767089844, train_time=0.049039363861083984
[Epoch 1][Step 39], time=0.06902003288269043, ext_time=0.004105329513549805, train_time=0.04354500770568848
[Epoch 1][Step 40], time=0.06912922859191895, ext_time=0.00412440299987793, train_time=0.04351615905761719
[Epoch 1][Step 41], time=0.06881546974182129, ext_time=0.004101991653442383, train_time=0.04330611228942871
[Epoch 1][Step 42], time=0.07097005844116211, ext_time=0.004098176956176758, train_time=0.043329715728759766
[Epoch 1][Step 43], time=0.0688941478729248, ext_time=0.004067659378051758, train_time=0.043639183044433594
[Epoch 1][Step 44], time=0.06949520111083984, ext_time=0.004086971282958984, train_time=0.04414629936218262
[Epoch 1][Step 45], time=0.0690157413482666, ext_time=0.004091978073120117, train_time=0.043605804443359375
[Epoch 1][Step 46], time=0.06873941421508789, ext_time=0.004103422164916992, train_time=0.04329562187194824
[Epoch 1][Step 47], time=0.06888723373413086, ext_time=0.004103660583496094, train_time=0.04340791702270508
[Epoch 1][Step 48], time=0.06891655921936035, ext_time=0.00411224365234375, train_time=0.043361663818359375
[Epoch 1][Step 49], time=0.06894087791442871, ext_time=0.004099607467651367, train_time=0.04354357719421387
[Epoch 1][Step 50], time=0.0689549446105957, ext_time=0.004086017608642578, train_time=0.04356837272644043
[Epoch 1][Step 51], time=0.06874322891235352, ext_time=0.00407862663269043, train_time=0.04341626167297363
[Epoch 1][Step 52], time=0.07079172134399414, ext_time=0.004109859466552734, train_time=0.043218135833740234
[Epoch 1][Step 53], time=0.06914615631103516, ext_time=0.0041046142578125, train_time=0.043692588806152344
[Epoch 1][Step 54], time=0.07129931449890137, ext_time=0.0040819644927978516, train_time=0.04587149620056152
[Epoch 1][Step 55], time=0.06909561157226562, ext_time=0.004127979278564453, train_time=0.04351043701171875
[Epoch 1][Step 56], time=0.06900787353515625, ext_time=0.004096508026123047, train_time=0.043619632720947266
[Epoch 1][Step 57], time=0.0691368579864502, ext_time=0.004077911376953125, train_time=0.04373908042907715
[Epoch 1][Step 58], time=0.06849503517150879, ext_time=0.00408172607421875, train_time=0.0432436466217041
[Epoch 1][Step 59], time=0.06885790824890137, ext_time=0.004086971282958984, train_time=0.043511390686035156
[Epoch 1][Step 60], time=0.06874299049377441, ext_time=0.004094839096069336, train_time=0.04336142539978027
[Epoch 1][Step 61], time=0.06899213790893555, ext_time=0.00410008430480957, train_time=0.043523550033569336
[Epoch 1][Step 62], time=0.06867837905883789, ext_time=0.0040760040283203125, train_time=0.04327106475830078
[Epoch 1][Step 63], time=0.0691213607788086, ext_time=0.004091739654541016, train_time=0.043703556060791016
[Epoch 1][Step 64], time=0.06896162033081055, ext_time=0.004094362258911133, train_time=0.043531179428100586
[Epoch 1][Step 65], time=0.23915767669677734, ext_time=0.004097700119018555, train_time=0.21372151374816895
[Epoch 1][Step 66], time=0.23550128936767578, ext_time=0.004089832305908203, train_time=0.21008563041687012
[Epoch 1][Step 67], time=0.06921029090881348, ext_time=0.00409698486328125, train_time=0.04379773139953613
[Epoch 1][Step 68], time=0.06881213188171387, ext_time=0.004095315933227539, train_time=0.04343056678771973
[Epoch 1][Step 69], time=0.06882834434509277, ext_time=0.004103422164916992, train_time=0.043356895446777344
[Epoch 1][Step 70], time=0.06929159164428711, ext_time=0.004083871841430664, train_time=0.043967485427856445
[Epoch 1][Step 71], time=0.06907439231872559, ext_time=0.004098176956176758, train_time=0.04366350173950195
[Epoch 1][Step 72], time=0.06897139549255371, ext_time=0.004098415374755859, train_time=0.043482065200805664
[Epoch 1][Step 73], time=0.07556366920471191, ext_time=0.004099607467651367, train_time=0.04335284233093262
[Epoch 1][Step 74], time=0.06894207000732422, ext_time=0.00410914421081543, train_time=0.043415069580078125
[Epoch 1][Step 75], time=0.06881046295166016, ext_time=0.004085540771484375, train_time=0.0433962345123291
[Epoch 1][Step 76], time=0.06900238990783691, ext_time=0.0040857791900634766, train_time=0.04358172416687012
[Epoch 1][Step 77], time=0.068939208984375, ext_time=0.004086017608642578, train_time=0.04356646537780762
[Epoch 1][Step 78], time=0.06892538070678711, ext_time=0.004092693328857422, train_time=0.04345226287841797
[Epoch 1][Step 79], time=0.06893324851989746, ext_time=0.004103422164916992, train_time=0.04348158836364746
[Epoch 1][Step 80], time=0.06906414031982422, ext_time=0.004110574722290039, train_time=0.0435185432434082
[Epoch 1][Step 81], time=0.06980133056640625, ext_time=0.004099130630493164, train_time=0.044338226318359375
[Epoch 1][Step 82], time=0.0691218376159668, ext_time=0.004118680953979492, train_time=0.043520450592041016
[Epoch 1][Step 83], time=0.0689857006072998, ext_time=0.004096508026123047, train_time=0.04357147216796875
[Epoch 1][Step 84], time=0.07004213333129883, ext_time=0.004077434539794922, train_time=0.04474759101867676
[Epoch 1][Step 85], time=0.06915450096130371, ext_time=0.004099607467651367, train_time=0.043738365173339844
[Epoch 1][Step 86], time=0.0688931941986084, ext_time=0.004105806350708008, train_time=0.04337787628173828
[Epoch 1][Step 87], time=0.19413447380065918, ext_time=0.004110574722290039, train_time=0.16861891746520996
[Epoch 1][Step 88], time=0.06889510154724121, ext_time=0.00408625602722168, train_time=0.04352855682373047
[Epoch 1][Step 89], time=0.06881833076477051, ext_time=0.004093647003173828, train_time=0.04338192939758301
[Epoch 1][Step 90], time=0.07009434700012207, ext_time=0.004078388214111328, train_time=0.04476332664489746
[Epoch 1][Step 91], time=0.06895661354064941, ext_time=0.004102468490600586, train_time=0.04348039627075195
[Epoch 1][Step 92], time=0.06891202926635742, ext_time=0.0040934085845947266, train_time=0.04345512390136719
[Epoch 1][Step 93], time=0.06914043426513672, ext_time=0.004119873046875, train_time=0.043631553649902344
[Epoch 1][Step 94], time=0.06886696815490723, ext_time=0.004095792770385742, train_time=0.04339432716369629
[Epoch 1][Step 95], time=0.06879878044128418, ext_time=0.004093170166015625, train_time=0.04339933395385742
[Epoch 1][Step 96], time=0.06914281845092773, ext_time=0.0040891170501708984, train_time=0.04374217987060547
[Epoch 1][Step 97], time=0.06906366348266602, ext_time=0.004080057144165039, train_time=0.043701171875
[Epoch 1][Step 98], time=0.06906366348266602, ext_time=0.004101276397705078, train_time=0.043602943420410156
[Epoch 1][Step 99], time=0.06886029243469238, ext_time=0.0040853023529052734, train_time=0.04345273971557617
[Epoch 1][Step 100], time=0.07003426551818848, ext_time=0.004115104675292969, train_time=0.044501304626464844
[Epoch 1][Step 101], time=0.06905221939086914, ext_time=0.00407719612121582, train_time=0.0437169075012207
[Epoch 1][Step 102], time=0.06898951530456543, ext_time=0.0041046142578125, train_time=0.043483734130859375
[Epoch 1][Step 103], time=0.06879281997680664, ext_time=0.00410914421081543, train_time=0.0432734489440918
[Epoch 1][Step 104], time=0.0688176155090332, ext_time=0.004068613052368164, train_time=0.043515920639038086
[Epoch 1][Step 105], time=0.06960582733154297, ext_time=0.004111289978027344, train_time=0.044100284576416016
[Epoch 1][Step 106], time=0.07024574279785156, ext_time=0.004097700119018555, train_time=0.04483318328857422
[Epoch 1][Step 107], time=0.06917548179626465, ext_time=0.004090309143066406, train_time=0.04378986358642578
[Epoch 1][Step 108], time=0.06909537315368652, ext_time=0.004092693328857422, train_time=0.043689727783203125
[Epoch 1][Step 109], time=0.06882047653198242, ext_time=0.004098415374755859, train_time=0.043349504470825195
[Epoch 1][Step 110], time=0.32761502265930176, ext_time=0.004099369049072266, train_time=0.30219602584838867
[Epoch 1][Step 111], time=0.06999373435974121, ext_time=0.0040972232818603516, train_time=0.04461240768432617
[Epoch 1][Step 112], time=0.07387089729309082, ext_time=0.00410914421081543, train_time=0.048316240310668945
[Epoch 1][Step 113], time=0.06981420516967773, ext_time=0.004094123840332031, train_time=0.04439496994018555
[Epoch 1][Step 114], time=0.06905460357666016, ext_time=0.004098415374755859, train_time=0.04361295700073242
[Epoch 1][Step 115], time=0.07681536674499512, ext_time=0.004111528396606445, train_time=0.04337143898010254
[Epoch 1][Step 116], time=0.06958746910095215, ext_time=0.004117488861083984, train_time=0.044031620025634766
[Epoch 1][Step 117], time=0.06891870498657227, ext_time=0.004102468490600586, train_time=0.043444156646728516
[Epoch 1][Step 118], time=0.07051348686218262, ext_time=0.004106044769287109, train_time=0.04497504234313965
[Epoch 1][Step 119], time=0.06909632682800293, ext_time=0.004129171371459961, train_time=0.04347109794616699
[Epoch 1][Step 120], time=0.06902480125427246, ext_time=0.004075050354003906, train_time=0.04371285438537598
[Epoch 1][Step 121], time=0.06890034675598145, ext_time=0.0041027069091796875, train_time=0.04342961311340332
[Epoch 1][Step 122], time=0.06878352165222168, ext_time=0.004096508026123047, train_time=0.04332470893859863
[Epoch 1][Step 123], time=0.06879711151123047, ext_time=0.004103899002075195, train_time=0.04330563545227051
[Epoch 1][Step 124], time=0.06885957717895508, ext_time=0.004080533981323242, train_time=0.043486595153808594
[Epoch 1], time=9.652840375900269, loss=0.43397077918052673
[Epoch 2][Step 0], time=0.06931424140930176, ext_time=0.004124879837036133, train_time=0.04361081123352051
[Epoch 2][Step 1], time=0.06907534599304199, ext_time=0.004119396209716797, train_time=0.04349970817565918
[Epoch 2][Step 2], time=0.0689094066619873, ext_time=0.004099130630493164, train_time=0.04338479042053223
[Epoch 2][Step 3], time=0.06900310516357422, ext_time=0.00412440299987793, train_time=0.04339742660522461
[Epoch 2][Step 4], time=0.0688624382019043, ext_time=0.004080533981323242, train_time=0.04349207878112793
[Epoch 2][Step 5], time=0.06870341300964355, ext_time=0.004080772399902344, train_time=0.043334245681762695
[Epoch 2][Step 6], time=0.07016181945800781, ext_time=0.004101276397705078, train_time=0.04471015930175781
[Epoch 2][Step 7], time=0.06893205642700195, ext_time=0.004085540771484375, train_time=0.04364800453186035
[Epoch 2][Step 8], time=0.07045507431030273, ext_time=0.004095315933227539, train_time=0.04503774642944336
[Epoch 2][Step 9], time=0.06872797012329102, ext_time=0.004080533981323242, train_time=0.043364763259887695
[Epoch 2][Step 10], time=0.06868553161621094, ext_time=0.004084348678588867, train_time=0.04328417778015137
[Epoch 2][Step 11], time=0.06962704658508301, ext_time=0.004090547561645508, train_time=0.044219017028808594
[Epoch 2][Step 12], time=0.06896710395812988, ext_time=0.004107236862182617, train_time=0.04344367980957031
[Epoch 2][Step 13], time=0.06923174858093262, ext_time=0.00410771369934082, train_time=0.043715715408325195
[Epoch 2][Step 14], time=0.06905007362365723, ext_time=0.004102468490600586, train_time=0.04358983039855957
[Epoch 2][Step 15], time=0.0691838264465332, ext_time=0.004098415374755859, train_time=0.04371833801269531
[Epoch 2][Step 16], time=0.07004809379577637, ext_time=0.004086971282958984, train_time=0.044647932052612305
[Epoch 2][Step 17], time=0.0691077709197998, ext_time=0.004091739654541016, train_time=0.043703556060791016
[Epoch 2][Step 18], time=0.06890368461608887, ext_time=0.0040950775146484375, train_time=0.043495893478393555
[Epoch 2][Step 19], time=0.06923270225524902, ext_time=0.004119157791137695, train_time=0.043633460998535156
[Epoch 2][Step 20], time=0.06889963150024414, ext_time=0.004113435745239258, train_time=0.04336667060852051
[Epoch 2][Step 21], time=0.06921195983886719, ext_time=0.0040705204010009766, train_time=0.04388284683227539
[Epoch 2][Step 22], time=0.06943655014038086, ext_time=0.004094839096069336, train_time=0.04403495788574219
[Epoch 2][Step 23], time=0.06913208961486816, ext_time=0.004111766815185547, train_time=0.04366183280944824
[Epoch 2][Step 24], time=0.06889677047729492, ext_time=0.004105567932128906, train_time=0.04336833953857422
[Epoch 2][Step 25], time=0.06911945343017578, ext_time=0.004113197326660156, train_time=0.04356122016906738
[Epoch 2][Step 26], time=0.06886982917785645, ext_time=0.004084110260009766, train_time=0.04351353645324707
[Epoch 2][Step 27], time=0.07051825523376465, ext_time=0.004106283187866211, train_time=0.04338693618774414
[Epoch 2][Step 28], time=0.06986355781555176, ext_time=0.00409388542175293, train_time=0.044496774673461914
[Epoch 2][Step 29], time=0.06864404678344727, ext_time=0.004099845886230469, train_time=0.043300628662109375
[Epoch 2][Step 30], time=0.06913638114929199, ext_time=0.004122018814086914, train_time=0.04357504844665527
[Epoch 2][Step 31], time=0.06902122497558594, ext_time=0.004106283187866211, train_time=0.04349160194396973
[Epoch 2][Step 32], time=0.341876745223999, ext_time=0.0040934085845947266, train_time=0.310793399810791
[Epoch 2][Step 33], time=0.06936073303222656, ext_time=0.004085540771484375, train_time=0.04406237602233887
[Epoch 2][Step 34], time=0.06888484954833984, ext_time=0.0041043758392333984, train_time=0.04337620735168457
[Epoch 2][Step 35], time=0.06889986991882324, ext_time=0.004071950912475586, train_time=0.04372000694274902
[Epoch 2][Step 36], time=0.06902766227722168, ext_time=0.0041141510009765625, train_time=0.04345273971557617
[Epoch 2][Step 37], time=0.0689096450805664, ext_time=0.004112720489501953, train_time=0.04341268539428711
[Epoch 2][Step 38], time=0.0689077377319336, ext_time=0.0040988922119140625, train_time=0.04345703125
[Epoch 2][Step 39], time=0.06917381286621094, ext_time=0.004106283187866211, train_time=0.043685197830200195
[Epoch 2][Step 40], time=0.07335734367370605, ext_time=0.004122257232666016, train_time=0.04770016670227051
[Epoch 2][Step 41], time=0.06884026527404785, ext_time=0.004155635833740234, train_time=0.04334878921508789
[Epoch 2][Step 42], time=0.0689396858215332, ext_time=0.0040912628173828125, train_time=0.04350161552429199
[Epoch 2][Step 43], time=0.0692286491394043, ext_time=0.004064321517944336, train_time=0.04395604133605957
[Epoch 2][Step 44], time=0.0690617561340332, ext_time=0.004111528396606445, train_time=0.043547868728637695
[Epoch 2][Step 45], time=0.06894063949584961, ext_time=0.00410771369934082, train_time=0.043401241302490234
[Epoch 2][Step 46], time=0.3026902675628662, ext_time=0.004086732864379883, train_time=0.27727699279785156
[Epoch 2][Step 47], time=0.07090449333190918, ext_time=0.004093170166015625, train_time=0.04554915428161621
[Epoch 2][Step 48], time=0.0688467025756836, ext_time=0.004087924957275391, train_time=0.04344749450683594
[Epoch 2][Step 49], time=0.06973814964294434, ext_time=0.004117250442504883, train_time=0.04424858093261719
[Epoch 2][Step 50], time=0.0691213607788086, ext_time=0.0041043758392333984, train_time=0.04360771179199219
[Epoch 2][Step 51], time=0.0686941146850586, ext_time=0.004086971282958984, train_time=0.04329276084899902
[Epoch 2][Step 52], time=0.07285094261169434, ext_time=0.004108428955078125, train_time=0.047335147857666016
[Epoch 2][Step 53], time=0.06917452812194824, ext_time=0.004092216491699219, train_time=0.04375791549682617
[Epoch 2][Step 54], time=0.06912875175476074, ext_time=0.004111528396606445, train_time=0.04353070259094238
[Epoch 2][Step 55], time=0.06883668899536133, ext_time=0.004108428955078125, train_time=0.04336214065551758
[Epoch 2][Step 56], time=0.07049059867858887, ext_time=0.004092693328857422, train_time=0.045114755630493164
[Epoch 2][Step 57], time=0.06903910636901855, ext_time=0.004086494445800781, train_time=0.043680667877197266
[Epoch 2][Step 58], time=0.06887650489807129, ext_time=0.004107952117919922, train_time=0.043363332748413086
[Epoch 2][Step 59], time=0.06894207000732422, ext_time=0.0040929317474365234, train_time=0.04354119300842285
[Epoch 2][Step 60], time=0.06938934326171875, ext_time=0.004103660583496094, train_time=0.04391050338745117
[Epoch 2][Step 61], time=0.3285045623779297, ext_time=0.004089832305908203, train_time=0.30312275886535645
[Epoch 2][Step 62], time=0.0702064037322998, ext_time=0.0040972232818603516, train_time=0.044771432876586914
[Epoch 2][Step 63], time=0.06902742385864258, ext_time=0.0041010379791259766, train_time=0.04367399215698242
[Epoch 2][Step 64], time=0.06868624687194824, ext_time=0.004084348678588867, train_time=0.043267250061035156
[Epoch 2][Step 65], time=0.06931281089782715, ext_time=0.004107475280761719, train_time=0.04385662078857422
[Epoch 2][Step 66], time=0.06907057762145996, ext_time=0.004094839096069336, train_time=0.043670654296875
[Epoch 2][Step 67], time=0.06913518905639648, ext_time=0.004113674163818359, train_time=0.04357647895812988
[Epoch 2][Step 68], time=0.0689549446105957, ext_time=0.004094362258911133, train_time=0.04352736473083496
[Epoch 2][Step 69], time=0.06883573532104492, ext_time=0.0041043758392333984, train_time=0.04330730438232422
[Epoch 2][Step 70], time=0.06950068473815918, ext_time=0.004095315933227539, train_time=0.044112443923950195
[Epoch 2][Step 71], time=0.06914710998535156, ext_time=0.0041027069091796875, train_time=0.04371905326843262
[Epoch 2][Step 72], time=0.06938052177429199, ext_time=0.00411534309387207, train_time=0.04369711875915527
[Epoch 2][Step 73], time=0.06892085075378418, ext_time=0.0041005611419677734, train_time=0.043488264083862305
[Epoch 2][Step 74], time=0.07457089424133301, ext_time=0.004093647003173828, train_time=0.04316210746765137
[Epoch 2][Step 75], time=0.06897759437561035, ext_time=0.004106998443603516, train_time=0.0434572696685791
[Epoch 2][Step 76], time=0.06935763359069824, ext_time=0.00410151481628418, train_time=0.04389762878417969
[Epoch 2][Step 77], time=0.06884264945983887, ext_time=0.004092216491699219, train_time=0.04342913627624512
[Epoch 2][Step 78], time=0.06873178482055664, ext_time=0.0041005611419677734, train_time=0.0432891845703125
[Epoch 2][Step 79], time=0.06898379325866699, ext_time=0.004098653793334961, train_time=0.04347515106201172
[Epoch 2][Step 80], time=0.07032132148742676, ext_time=0.004099130630493164, train_time=0.0448613166809082
[Epoch 2][Step 81], time=0.06929922103881836, ext_time=0.0040895938873291016, train_time=0.043833017349243164
[Epoch 2][Step 82], time=0.06897139549255371, ext_time=0.004119157791137695, train_time=0.04337477684020996
[Epoch 2][Step 83], time=0.07040905952453613, ext_time=0.004094362258911133, train_time=0.0450739860534668
[Epoch 2][Step 84], time=0.06902241706848145, ext_time=0.004076242446899414, train_time=0.0436711311340332
[Epoch 2][Step 85], time=0.06901717185974121, ext_time=0.004083395004272461, train_time=0.043618202209472656
[Epoch 2][Step 86], time=0.06915163993835449, ext_time=0.004114866256713867, train_time=0.0435943603515625
[Epoch 2][Step 87], time=0.06899094581604004, ext_time=0.004092216491699219, train_time=0.04356074333190918
[Epoch 2][Step 88], time=0.06901335716247559, ext_time=0.004107236862182617, train_time=0.043479204177856445
[Epoch 2][Step 89], time=0.06899714469909668, ext_time=0.004101276397705078, train_time=0.043524742126464844
[Epoch 2][Step 90], time=0.06965517997741699, ext_time=0.004075288772583008, train_time=0.044289350509643555
[Epoch 2][Step 91], time=0.0691225528717041, ext_time=0.004096031188964844, train_time=0.043731689453125
[Epoch 2][Step 92], time=0.06911420822143555, ext_time=0.004101991653442383, train_time=0.043625593185424805
[Epoch 2][Step 93], time=0.06887221336364746, ext_time=0.004099607467651367, train_time=0.043468475341796875
[Epoch 2][Step 94], time=0.06874990463256836, ext_time=0.00410008430480957, train_time=0.04327034950256348
[Epoch 2][Step 95], time=0.06876397132873535, ext_time=0.004100322723388672, train_time=0.04327893257141113
[Epoch 2][Step 96], time=0.06927776336669922, ext_time=0.0041046142578125, train_time=0.04378175735473633
[Epoch 2][Step 97], time=0.0697777271270752, ext_time=0.004072904586791992, train_time=0.04452776908874512
[Epoch 2][Step 98], time=0.0691070556640625, ext_time=0.004117727279663086, train_time=0.04356074333190918
[Epoch 2][Step 99], time=0.06896376609802246, ext_time=0.004092693328857422, train_time=0.043509721755981445
[Epoch 2][Step 100], time=0.06895923614501953, ext_time=0.004113435745239258, train_time=0.043415069580078125
[Epoch 2][Step 101], time=0.07027959823608398, ext_time=0.0040967464447021484, train_time=0.04488539695739746
[Epoch 2][Step 102], time=0.06928610801696777, ext_time=0.004094362258911133, train_time=0.04387044906616211
[Epoch 2][Step 103], time=0.06895756721496582, ext_time=0.00411224365234375, train_time=0.04343438148498535
[Epoch 2][Step 104], time=0.06876897811889648, ext_time=0.004094600677490234, train_time=0.04346346855163574
[Epoch 2][Step 105], time=0.06897926330566406, ext_time=0.004091501235961914, train_time=0.043519020080566406
[Epoch 2][Step 106], time=0.06896233558654785, ext_time=0.004101991653442383, train_time=0.04355287551879883
[Epoch 2][Step 107], time=0.07491064071655273, ext_time=0.004084587097167969, train_time=0.049611806869506836
[Epoch 2][Step 108], time=0.06889128684997559, ext_time=0.004086017608642578, train_time=0.043549537658691406
[Epoch 2][Step 109], time=0.06892681121826172, ext_time=0.004092693328857422, train_time=0.04346823692321777
[Epoch 2][Step 110], time=0.06905412673950195, ext_time=0.004065990447998047, train_time=0.04381132125854492
[Epoch 2][Step 111], time=0.06920242309570312, ext_time=0.004085540771484375, train_time=0.04385662078857422
[Epoch 2][Step 112], time=0.06883955001831055, ext_time=0.004098415374755859, train_time=0.04337573051452637
[Epoch 2][Step 113], time=0.0692899227142334, ext_time=0.004082202911376953, train_time=0.04392290115356445
[Epoch 2][Step 114], time=0.06891965866088867, ext_time=0.004102468490600586, train_time=0.043419599533081055
[Epoch 2][Step 115], time=0.06880712509155273, ext_time=0.0041103363037109375, train_time=0.043324947357177734
[Epoch 2][Step 116], time=0.07468819618225098, ext_time=0.004091739654541016, train_time=0.04350113868713379
[Epoch 2][Step 117], time=0.07058572769165039, ext_time=0.004120349884033203, train_time=0.0450282096862793
[Epoch 2][Step 118], time=0.06980514526367188, ext_time=0.0041010379791259766, train_time=0.043296098709106445
[Epoch 2][Step 119], time=0.06876492500305176, ext_time=0.004103899002075195, train_time=0.04332470893859863
[Epoch 2][Step 120], time=0.364910364151001, ext_time=0.004076242446899414, train_time=0.3395571708679199
[Epoch 2][Step 121], time=0.07016348838806152, ext_time=0.004088401794433594, train_time=0.04477953910827637
[Epoch 2][Step 122], time=0.06875085830688477, ext_time=0.0040857791900634766, train_time=0.04336404800415039
[Epoch 2][Step 123], time=0.06934356689453125, ext_time=0.004090309143066406, train_time=0.04312300682067871
[Epoch 2][Step 124], time=0.06888651847839355, ext_time=0.0040895938873291016, train_time=0.04347085952758789
[Epoch 2], time=9.741712808609009, loss=0.415772408246994
[Epoch 3][Step 0], time=0.06907248497009277, ext_time=0.004111289978027344, train_time=0.04356122016906738
[Epoch 3][Step 1], time=0.06889581680297852, ext_time=0.004114866256713867, train_time=0.04335355758666992
[Epoch 3][Step 2], time=0.06880664825439453, ext_time=0.004086494445800781, train_time=0.04338884353637695
[Epoch 3][Step 3], time=0.0689394474029541, ext_time=0.004107475280761719, train_time=0.04342818260192871
[Epoch 3][Step 4], time=0.06911396980285645, ext_time=0.004101991653442383, train_time=0.04359698295593262
[Epoch 3][Step 5], time=0.06866073608398438, ext_time=0.004081249237060547, train_time=0.043313026428222656
[Epoch 3][Step 6], time=0.06890273094177246, ext_time=0.004098415374755859, train_time=0.043447256088256836
[Epoch 3][Step 7], time=0.06965351104736328, ext_time=0.0040967464447021484, train_time=0.04418373107910156
[Epoch 3][Step 8], time=0.06887435913085938, ext_time=0.004078865051269531, train_time=0.04354262351989746
[Epoch 3][Step 9], time=0.06966257095336914, ext_time=0.004088878631591797, train_time=0.044274330139160156
[Epoch 3][Step 10], time=0.06911420822143555, ext_time=0.004108428955078125, train_time=0.04357647895812988
[Epoch 3][Step 11], time=0.06898212432861328, ext_time=0.0040929317474365234, train_time=0.04356694221496582
[Epoch 3][Step 12], time=0.06897330284118652, ext_time=0.004099130630493164, train_time=0.04353523254394531
[Epoch 3][Step 13], time=0.0690464973449707, ext_time=0.004105567932128906, train_time=0.0435338020324707
[Epoch 3][Step 14], time=0.07309103012084961, ext_time=0.004065990447998047, train_time=0.04786372184753418
[Epoch 3][Step 15], time=0.06902146339416504, ext_time=0.00409698486328125, train_time=0.0435793399810791
[Epoch 3][Step 16], time=0.06871795654296875, ext_time=0.004097700119018555, train_time=0.04326772689819336
[Epoch 3][Step 17], time=0.06879711151123047, ext_time=0.0040814876556396484, train_time=0.04341411590576172
[Epoch 3][Step 18], time=0.06891226768493652, ext_time=0.00410151481628418, train_time=0.04338860511779785
[Epoch 3][Step 19], time=0.0690762996673584, ext_time=0.004091978073120117, train_time=0.04364275932312012
[Epoch 3][Step 20], time=0.06921911239624023, ext_time=0.004107475280761719, train_time=0.04376816749572754
[Epoch 3][Step 21], time=0.0687875747680664, ext_time=0.0040836334228515625, train_time=0.04343104362487793
[Epoch 3][Step 22], time=0.06901359558105469, ext_time=0.004082202911376953, train_time=0.04367804527282715
[Epoch 3][Step 23], time=0.06920552253723145, ext_time=0.0040798187255859375, train_time=0.0438084602355957
[Epoch 3][Step 24], time=0.06892752647399902, ext_time=0.004105567932128906, train_time=0.04344677925109863
[Epoch 3][Step 25], time=0.07049417495727539, ext_time=0.004122734069824219, train_time=0.04487895965576172
[Epoch 3][Step 26], time=0.06882262229919434, ext_time=0.004102945327758789, train_time=0.04335951805114746
[Epoch 3][Step 27], time=0.0694725513458252, ext_time=0.0041120052337646484, train_time=0.04398179054260254
[Epoch 3][Step 28], time=0.06905508041381836, ext_time=0.004088401794433594, train_time=0.04364824295043945
[Epoch 3][Step 29], time=0.06882786750793457, ext_time=0.004106998443603516, train_time=0.04335498809814453
[Epoch 3][Step 30], time=0.27664947509765625, ext_time=0.0041081905364990234, train_time=0.25111842155456543
[Epoch 3][Step 31], time=0.0693960189819336, ext_time=0.0041141510009765625, train_time=0.04390740394592285
[Epoch 3][Step 32], time=0.06907200813293457, ext_time=0.0041048526763916016, train_time=0.043738365173339844
[Epoch 3][Step 33], time=0.0756371021270752, ext_time=0.004099130630493164, train_time=0.04387807846069336
[Epoch 3][Step 34], time=0.06927609443664551, ext_time=0.004094600677490234, train_time=0.0438075065612793
[Epoch 3][Step 35], time=0.06921243667602539, ext_time=0.0040967464447021484, train_time=0.0437777042388916
[Epoch 3][Step 36], time=0.06890487670898438, ext_time=0.004105567932128906, train_time=0.04341268539428711
[Epoch 3][Step 37], time=0.0687859058380127, ext_time=0.0040819644927978516, train_time=0.04342150688171387
[Epoch 3][Step 38], time=0.06904959678649902, ext_time=0.004096031188964844, train_time=0.04358863830566406
[Epoch 3][Step 39], time=0.0691077709197998, ext_time=0.0041196346282958984, train_time=0.043524742126464844
[Epoch 3][Step 40], time=0.06884956359863281, ext_time=0.004111051559448242, train_time=0.043346405029296875
[Epoch 3][Step 41], time=0.06865358352661133, ext_time=0.004079341888427734, train_time=0.04327034950256348
[Epoch 3][Step 42], time=0.06880903244018555, ext_time=0.00408935546875, train_time=0.0434117317199707
[Epoch 3][Step 43], time=0.06924676895141602, ext_time=0.0040819644927978516, train_time=0.04391217231750488
[Epoch 3][Step 44], time=0.0692605972290039, ext_time=0.0040934085845947266, train_time=0.04379129409790039
[Epoch 3][Step 45], time=0.0693063735961914, ext_time=0.0040988922119140625, train_time=0.04389214515686035
[Epoch 3][Step 46], time=0.0688929557800293, ext_time=0.004096269607543945, train_time=0.04346346855163574
[Epoch 3][Step 47], time=0.06924223899841309, ext_time=0.004099607467651367, train_time=0.04381895065307617
[Epoch 3][Step 48], time=0.06878948211669922, ext_time=0.00408935546875, train_time=0.04336285591125488
[Epoch 3][Step 49], time=0.06904816627502441, ext_time=0.004109621047973633, train_time=0.04354286193847656
[Epoch 3][Step 50], time=0.06893754005432129, ext_time=0.004099130630493164, train_time=0.04348158836364746
[Epoch 3][Step 51], time=0.06900477409362793, ext_time=0.004098653793334961, train_time=0.04351639747619629
[Epoch 3][Step 52], time=0.06885695457458496, ext_time=0.004092693328857422, train_time=0.043471336364746094
[Epoch 3][Step 53], time=0.06890487670898438, ext_time=0.0041043758392333984, train_time=0.043468475341796875
[Epoch 3][Step 54], time=0.06949734687805176, ext_time=0.0040972232818603516, train_time=0.044107913970947266
[Epoch 3][Step 55], time=0.06878232955932617, ext_time=0.004099369049072266, train_time=0.043363332748413086
[Epoch 3][Step 56], time=0.06896662712097168, ext_time=0.0040667057037353516, train_time=0.0436863899230957
[Epoch 3][Step 57], time=0.06875967979431152, ext_time=0.004086971282958984, train_time=0.04333639144897461
[Epoch 3][Step 58], time=0.06878447532653809, ext_time=0.004100799560546875, train_time=0.04329323768615723
[Epoch 3][Step 59], time=0.0687565803527832, ext_time=0.004078865051269531, train_time=0.04340696334838867
[Epoch 3][Step 60], time=0.06894826889038086, ext_time=0.00410771369934082, train_time=0.04345512390136719
[Epoch 3][Step 61], time=0.06931161880493164, ext_time=0.004091501235961914, train_time=0.04390549659729004
[Epoch 3][Step 62], time=0.0689852237701416, ext_time=0.004090785980224609, train_time=0.04356789588928223
[Epoch 3][Step 63], time=0.06896734237670898, ext_time=0.00409245491027832, train_time=0.04357576370239258
[Epoch 3][Step 64], time=0.06960678100585938, ext_time=0.0041027069091796875, train_time=0.04413962364196777
[Epoch 3][Step 65], time=0.06937956809997559, ext_time=0.0041081905364990234, train_time=0.04388165473937988
[Epoch 3][Step 66], time=0.0689702033996582, ext_time=0.004109859466552734, train_time=0.04340100288391113
[Epoch 3][Step 67], time=0.06949901580810547, ext_time=0.0041048526763916016, train_time=0.04398393630981445
[Epoch 3][Step 68], time=0.06911110877990723, ext_time=0.004076480865478516, train_time=0.04379415512084961
[Epoch 3][Step 69], time=0.06910395622253418, ext_time=0.004121303558349609, train_time=0.043512821197509766
[Epoch 3][Step 70], time=0.0692138671875, ext_time=0.004102945327758789, train_time=0.04378008842468262
[Epoch 3][Step 71], time=0.06892204284667969, ext_time=0.004093647003173828, train_time=0.043520212173461914
[Epoch 3][Step 72], time=0.06888723373413086, ext_time=0.004102468490600586, train_time=0.04334592819213867
[Epoch 3][Step 73], time=0.06892037391662598, ext_time=0.004099130630493164, train_time=0.04345440864562988
[Epoch 3][Step 74], time=0.06946206092834473, ext_time=0.004108428955078125, train_time=0.043975830078125
[Epoch 3][Step 75], time=0.07462143898010254, ext_time=0.004106998443603516, train_time=0.043415069580078125
[Epoch 3][Step 76], time=0.07269668579101562, ext_time=0.004102945327758789, train_time=0.04727053642272949
[Epoch 3][Step 77], time=0.06891775131225586, ext_time=0.0041005611419677734, train_time=0.043454647064208984
[Epoch 3][Step 78], time=0.06891131401062012, ext_time=0.004094600677490234, train_time=0.04345989227294922
[Epoch 3][Step 79], time=0.06895971298217773, ext_time=0.0041081905364990234, train_time=0.04340529441833496
[Epoch 3][Step 80], time=0.06889915466308594, ext_time=0.0040950775146484375, train_time=0.043482065200805664
[Epoch 3][Step 81], time=0.06898665428161621, ext_time=0.004101276397705078, train_time=0.04345226287841797
[Epoch 3][Step 82], time=0.06891989707946777, ext_time=0.004110574722290039, train_time=0.043372392654418945
[Epoch 3][Step 83], time=0.06892156600952148, ext_time=0.00410151481628418, train_time=0.04344797134399414
[Epoch 3][Step 84], time=0.06893301010131836, ext_time=0.004082441329956055, train_time=0.0437016487121582
[Epoch 3][Step 85], time=0.06910991668701172, ext_time=0.0041086673736572266, train_time=0.043686628341674805
[Epoch 3][Step 86], time=0.06883406639099121, ext_time=0.004106760025024414, train_time=0.043328046798706055
[Epoch 3][Step 87], time=0.07018709182739258, ext_time=0.0041005611419677734, train_time=0.04327225685119629
[Epoch 3][Step 88], time=0.06888866424560547, ext_time=0.004119396209716797, train_time=0.04334902763366699
[Epoch 3][Step 89], time=0.06898832321166992, ext_time=0.004097938537597656, train_time=0.04351210594177246
[Epoch 3][Step 90], time=0.06906533241271973, ext_time=0.004093170166015625, train_time=0.04366183280944824
[Epoch 3][Step 91], time=0.06878829002380371, ext_time=0.00410151481628418, train_time=0.04333829879760742
[Epoch 3][Step 92], time=0.06915140151977539, ext_time=0.00413203239440918, train_time=0.043488502502441406
[Epoch 3][Step 93], time=0.06903409957885742, ext_time=0.004105567932128906, train_time=0.043543100357055664
[Epoch 3][Step 94], time=0.0691077709197998, ext_time=0.0040662288665771484, train_time=0.04380202293395996
[Epoch 3][Step 95], time=0.06877565383911133, ext_time=0.004090070724487305, train_time=0.04339480400085449
[Epoch 3][Step 96], time=0.06910157203674316, ext_time=0.00409388542175293, train_time=0.04367256164550781
[Epoch 3][Step 97], time=0.06891918182373047, ext_time=0.004088401794433594, train_time=0.04355025291442871
[Epoch 3][Step 98], time=0.06911301612854004, ext_time=0.00410151481628418, train_time=0.04365110397338867
[Epoch 3][Step 99], time=0.0691995620727539, ext_time=0.0041048526763916016, train_time=0.04367995262145996
[Epoch 3][Step 100], time=0.06910920143127441, ext_time=0.004125118255615234, train_time=0.04353737831115723
[Epoch 3][Step 101], time=0.06882452964782715, ext_time=0.004105091094970703, train_time=0.04330563545227051
[Epoch 3][Step 102], time=0.06917643547058105, ext_time=0.004102468490600586, train_time=0.043724775314331055
[Epoch 3][Step 103], time=0.0691218376159668, ext_time=0.004120826721191406, train_time=0.04354667663574219
[Epoch 3][Step 104], time=0.0687556266784668, ext_time=0.004097461700439453, train_time=0.04333639144897461
[Epoch 3][Step 105], time=0.0701451301574707, ext_time=0.004117012023925781, train_time=0.044577836990356445
[Epoch 3][Step 106], time=0.06891345977783203, ext_time=0.004092216491699219, train_time=0.043535709381103516
[Epoch 3][Step 107], time=0.06919455528259277, ext_time=0.004098415374755859, train_time=0.04372143745422363
[Epoch 3][Step 108], time=0.0687408447265625, ext_time=0.004079103469848633, train_time=0.04355359077453613
[Epoch 3][Step 109], time=0.06879973411560059, ext_time=0.004096508026123047, train_time=0.04336833953857422
[Epoch 3][Step 110], time=0.06865239143371582, ext_time=0.004070758819580078, train_time=0.043358802795410156
[Epoch 3][Step 111], time=0.06890273094177246, ext_time=0.004099845886230469, train_time=0.043491363525390625
[Epoch 3][Step 112], time=0.07000494003295898, ext_time=0.004114627838134766, train_time=0.04447793960571289
[Epoch 3][Step 113], time=0.06903576850891113, ext_time=0.004092216491699219, train_time=0.04369235038757324
[Epoch 3][Step 114], time=0.06928014755249023, ext_time=0.004129886627197266, train_time=0.04367399215698242
[Epoch 3][Step 115], time=0.07085752487182617, ext_time=0.004109382629394531, train_time=0.0454862117767334
[Epoch 3][Step 116], time=0.06949543952941895, ext_time=0.004123687744140625, train_time=0.04353666305541992
[Epoch 3][Step 117], time=0.07452607154846191, ext_time=0.004105329513549805, train_time=0.043268680572509766
[Epoch 3][Step 118], time=0.06899070739746094, ext_time=0.004116535186767578, train_time=0.043460845947265625
[Epoch 3][Step 119], time=0.06922769546508789, ext_time=0.0041196346282958984, train_time=0.04369211196899414
[Epoch 3][Step 120], time=0.06895899772644043, ext_time=0.004098176956176758, train_time=0.043527841567993164
[Epoch 3][Step 121], time=0.0687720775604248, ext_time=0.004084110260009766, train_time=0.043361663818359375
[Epoch 3][Step 122], time=0.06892085075378418, ext_time=0.0040781497955322266, train_time=0.04355192184448242
[Epoch 3][Step 123], time=0.06887602806091309, ext_time=0.004095554351806641, train_time=0.04346466064453125
[Epoch 3][Step 124], time=0.06875061988830566, ext_time=0.0040705204010009766, train_time=0.04344487190246582
[Epoch 3], time=8.87388300895691, loss=0.42079052329063416
    [Step(average) Profiler Level 1 E3 S999]
        L1  sample           0.021409 | send           0.000000
        L1  recv             0.000000 | copy           0.004090 | convert time 0.000000 | train  0.049840
        L1  feature nbytes    3.18 GB | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes    0.00 Bytes | remote nbytes 0.00 Bytes
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S999]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.004090
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S999]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000000 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.000000 | cache combine cache 0.004064 | cache combine remote 0.000000
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S999]
        p50.00_tail_logl2featcopy=0.004090
        p90.00_tail_logl2featcopy=0.004119
        p95.00_tail_logl2featcopy=0.004127
        p99.00_tail_logl2featcopy=0.004175
        p99.90_tail_logl2featcopy=0.027683
[CUDA] cuda: usage: 78.33 GB
worker 7 running with pid=57416
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=7, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005605, per step: 0.000045
presamping
presamping takes 35.33705949783325
worker 3 running with pid=57408
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=3, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005806, per step: 0.000046
presamping
presamping takes 33.622883558273315
worker 2 running with pid=57406
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=2, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.006350, per step: 0.000051
presamping
presamping takes 34.56370759010315
worker 4 running with pid=57410
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=4, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005691, per step: 0.000046
presamping
presamping takes 35.140889406204224
worker 1 running with pid=57405
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=1, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005591, per step: 0.000045
presamping
presamping takes 33.497485399246216
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  372884 KB |    7295 MB |   11987 GB |   11986 GB |
|       from large pool |  362798 KB |    7286 MB |   11972 GB |   11972 GB |
|       from small pool |   10086 KB |      15 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| Active memory         |  372884 KB |    7295 MB |   11987 GB |   11986 GB |
|       from large pool |  362798 KB |    7286 MB |   11972 GB |   11972 GB |
|       from small pool |   10086 KB |      15 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   15666 MB |   15666 MB |   29446 MB |   13780 MB |
|       from large pool |   15646 MB |   15646 MB |   29424 MB |   13778 MB |
|       from small pool |      20 MB |      20 MB |      22 MB |       2 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    8371 MB |    9627 MB |    7924 GB |    7916 GB |
|       from large pool |    8365 MB |    9621 MB |    7909 GB |    7901 GB |
|       from small pool |       6 MB |      10 MB |      15 GB |      15 GB |
|---------------------------------------------------------------------------|
| Allocations           |      71    |      98    |  158162    |  158091    |
|       from large pool |      22    |      43    |   73000    |   72978    |
|       from small pool |      49    |      60    |   85162    |   85113    |
|---------------------------------------------------------------------------|
| Active allocs         |      71    |      98    |  158162    |  158091    |
|       from large pool |      22    |      43    |   73000    |   72978    |
|       from small pool |      49    |      60    |   85162    |   85113    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      34    |      36    |      48    |      14    |
|       from large pool |      24    |      26    |      37    |      13    |
|       from small pool |      10    |      10    |      11    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      38    |      54    |   60112    |   60074    |
|       from large pool |      19    |      32    |   38715    |   38696    |
|       from small pool |      19    |      31    |   21397    |   21378    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 41.984027 seconds
[EPOCH_TIME] 10.496007 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 9.307976 seconds
worker 5 running with pid=57412
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=5, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005653, per step: 0.000045
presamping
presamping takes 32.50395655632019
worker 6 running with pid=57414
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=6, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005803, per step: 0.000046
presamping
presamping takes 36.48839449882507

