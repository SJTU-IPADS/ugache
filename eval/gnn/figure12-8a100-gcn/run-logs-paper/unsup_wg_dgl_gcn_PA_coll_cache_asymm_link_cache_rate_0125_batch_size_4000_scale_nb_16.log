succeed=True
worker 0 running with pid=51528
config:eval_tsp="2023-08-06 06:52:31"
config:num_worker=8
config:num_intra_size=8
config:root_dir=/datasets_gnn/wholegraph
config:graph_name=ogbn-papers100M
config:epochs=4
config:batchsize=4000
config:skip_epoch=2
config:local_step=125
config:presc_epoch=2
config:neighbors=15,10,5
config:hiddensize=256
config:num_layer=3
config:model=gcn
config:framework=dgl
config:dataloaderworkers=0
config:dropout=0.5
config:weight_decay=0.0005
config:lr=0.003
config:use_nccl=False
config:use_amp=False
config:use_collcache=False
config:cache_percentage=0.25
config:cache_policy=coll_cache_asymm_link
config:omp_thread_num=56
config:unsupervised=True
config:classnum=172
config:global_barrier=<multiprocessing.synchronize.Barrier object at 0x7fe5df7549a0>
config:worker_id=0
creating_intra_node_communicator root=0, local_size=8, world_size=8
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=0, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005968, per step: 0.000048
epoch=4 total_steps=500
start training...
[Epoch 0][Step 0], time=1.4463715553283691, ext_time=0.023937225341796875, train_time=1.3752663135528564
[Epoch 0][Step 1], time=0.09853029251098633, ext_time=0.02087688446044922, train_time=0.047881126403808594
[Epoch 0][Step 2], time=0.09556937217712402, ext_time=0.016740798950195312, train_time=0.057470083236694336
[Epoch 0][Step 3], time=0.09624695777893066, ext_time=0.01605963706970215, train_time=0.05868053436279297
[Epoch 0][Step 4], time=0.08919954299926758, ext_time=0.020572423934936523, train_time=0.047035932540893555
[Epoch 0][Step 5], time=0.0861201286315918, ext_time=0.02116084098815918, train_time=0.04365968704223633
[Epoch 0][Step 6], time=0.08591246604919434, ext_time=0.02095508575439453, train_time=0.04362010955810547
[Epoch 0][Step 7], time=0.08771109580993652, ext_time=0.023095130920410156, train_time=0.04330158233642578
[Epoch 0][Step 8], time=0.0914909839630127, ext_time=0.020661115646362305, train_time=0.04956221580505371
[Epoch 0][Step 9], time=0.0862267017364502, ext_time=0.021152496337890625, train_time=0.04372978210449219
[Epoch 0][Step 10], time=0.08811640739440918, ext_time=0.02115011215209961, train_time=0.04562973976135254
[Epoch 0][Step 11], time=0.08667492866516113, ext_time=0.02116560935974121, train_time=0.044255971908569336
[Epoch 0][Step 12], time=0.0893096923828125, ext_time=0.020667076110839844, train_time=0.0472111701965332
[Epoch 0][Step 13], time=0.08565926551818848, ext_time=0.021070241928100586, train_time=0.043279170989990234
[Epoch 0][Step 14], time=0.0894315242767334, ext_time=0.0206298828125, train_time=0.047502756118774414
[Epoch 0][Step 15], time=0.08622097969055176, ext_time=0.021175622940063477, train_time=0.04365038871765137
[Epoch 0][Step 16], time=0.08653903007507324, ext_time=0.021091222763061523, train_time=0.04420614242553711
[Epoch 0][Step 17], time=0.0889735221862793, ext_time=0.020582199096679688, train_time=0.0471193790435791
[Epoch 0][Step 18], time=0.08604764938354492, ext_time=0.02113175392150879, train_time=0.04360651969909668
[Epoch 0][Step 19], time=0.0896604061126709, ext_time=0.02263355255126953, train_time=0.04557657241821289
[Epoch 0][Step 20], time=0.0858759880065918, ext_time=0.021129846572875977, train_time=0.043485164642333984
[Epoch 0][Step 21], time=0.08628129959106445, ext_time=0.0212557315826416, train_time=0.043537139892578125
[Epoch 0][Step 22], time=0.08619213104248047, ext_time=0.021127700805664062, train_time=0.04369068145751953
[Epoch 0][Step 23], time=0.08937382698059082, ext_time=0.020713090896606445, train_time=0.04730367660522461
[Epoch 0][Step 24], time=0.08594107627868652, ext_time=0.021160125732421875, train_time=0.04337501525878906
[Epoch 0][Step 25], time=0.09292721748352051, ext_time=0.023344039916992188, train_time=0.0480954647064209
[Epoch 0][Step 26], time=0.0860433578491211, ext_time=0.021142959594726562, train_time=0.04355931282043457
[Epoch 0][Step 27], time=0.08611416816711426, ext_time=0.021195411682128906, train_time=0.04356837272644043
[Epoch 0][Step 28], time=0.08971691131591797, ext_time=0.021111726760864258, train_time=0.043515682220458984
[Epoch 0][Step 29], time=0.08611869812011719, ext_time=0.02106332778930664, train_time=0.04372525215148926
[Epoch 0][Step 30], time=0.08615374565124512, ext_time=0.021259784698486328, train_time=0.04344463348388672
[Epoch 0][Step 31], time=0.08714032173156738, ext_time=0.021124839782714844, train_time=0.044594526290893555
[Epoch 0][Step 32], time=0.089111328125, ext_time=0.020684480667114258, train_time=0.04710960388183594
[Epoch 0][Step 33], time=0.09005117416381836, ext_time=0.02071833610534668, train_time=0.04794740676879883
[Epoch 0][Step 34], time=0.08587813377380371, ext_time=0.021114826202392578, train_time=0.04346895217895508
[Epoch 0][Step 35], time=0.08916997909545898, ext_time=0.02021050453186035, train_time=0.04770970344543457
[Epoch 0][Step 36], time=0.08603382110595703, ext_time=0.02112293243408203, train_time=0.043483734130859375
[Epoch 0][Step 37], time=0.08568644523620605, ext_time=0.021053791046142578, train_time=0.04329419136047363
[Epoch 0][Step 38], time=0.0861368179321289, ext_time=0.021219491958618164, train_time=0.04354262351989746
[Epoch 0][Step 39], time=0.0860593318939209, ext_time=0.021190881729125977, train_time=0.0435640811920166
[Epoch 0][Step 40], time=0.08600664138793945, ext_time=0.02118229866027832, train_time=0.04339456558227539
[Epoch 0][Step 41], time=0.08596611022949219, ext_time=0.021210432052612305, train_time=0.04338645935058594
[Epoch 0][Step 42], time=0.08595490455627441, ext_time=0.021083831787109375, train_time=0.043431997299194336
[Epoch 0][Step 43], time=0.08932733535766602, ext_time=0.020546913146972656, train_time=0.04748654365539551
[Epoch 0][Step 44], time=0.08585762977600098, ext_time=0.021166563034057617, train_time=0.04332923889160156
[Epoch 0][Step 45], time=0.08934926986694336, ext_time=0.020648956298828125, train_time=0.04736804962158203
[Epoch 0][Step 46], time=0.08605575561523438, ext_time=0.021188020706176758, train_time=0.0434110164642334
[Epoch 0][Step 47], time=0.08609843254089355, ext_time=0.02115488052368164, train_time=0.043607473373413086
[Epoch 0][Step 48], time=0.08637642860412598, ext_time=0.021132946014404297, train_time=0.043848276138305664
[Epoch 0][Step 49], time=0.08597159385681152, ext_time=0.021075725555419922, train_time=0.043639183044433594
[Epoch 0][Step 50], time=0.08598709106445312, ext_time=0.020968198776245117, train_time=0.04372143745422363
[Epoch 0][Step 51], time=0.08578920364379883, ext_time=0.02109384536743164, train_time=0.04330754280090332
[Epoch 0][Step 52], time=0.08596396446228027, ext_time=0.021184682846069336, train_time=0.04336142539978027
[Epoch 0][Step 53], time=0.08622503280639648, ext_time=0.021265745162963867, train_time=0.04356813430786133
[Epoch 0][Step 54], time=0.08666706085205078, ext_time=0.021195411682128906, train_time=0.04408621788024902
[Epoch 0][Step 55], time=0.0859215259552002, ext_time=0.021183490753173828, train_time=0.04333972930908203
[Epoch 0][Step 56], time=0.08611440658569336, ext_time=0.021065711975097656, train_time=0.04375624656677246
[Epoch 0][Step 57], time=0.08619809150695801, ext_time=0.021189451217651367, train_time=0.04365396499633789
[Epoch 0][Step 58], time=0.08624815940856934, ext_time=0.021098852157592773, train_time=0.04363727569580078
[Epoch 0][Step 59], time=0.08607983589172363, ext_time=0.021077871322631836, train_time=0.04372596740722656
[Epoch 0][Step 60], time=0.08603405952453613, ext_time=0.021178483963012695, train_time=0.04347658157348633
[Epoch 0][Step 61], time=0.08916258811950684, ext_time=0.02065587043762207, train_time=0.0472102165222168
[Epoch 0][Step 62], time=0.0860588550567627, ext_time=0.021172046661376953, train_time=0.0435490608215332
[Epoch 0][Step 63], time=0.08623170852661133, ext_time=0.021318674087524414, train_time=0.0435338020324707
[Epoch 0][Step 64], time=0.08585453033447266, ext_time=0.02112865447998047, train_time=0.04337358474731445
[Epoch 0][Step 65], time=0.08641982078552246, ext_time=0.02123713493347168, train_time=0.04378485679626465
[Epoch 0][Step 66], time=0.08612513542175293, ext_time=0.02114272117614746, train_time=0.043581485748291016
[Epoch 0][Step 67], time=0.08621668815612793, ext_time=0.02117776870727539, train_time=0.0435795783996582
[Epoch 0][Step 68], time=0.08599591255187988, ext_time=0.021142005920410156, train_time=0.043584585189819336
[Epoch 0][Step 69], time=0.08607149124145508, ext_time=0.02115345001220703, train_time=0.04350924491882324
[Epoch 0][Step 70], time=0.09041786193847656, ext_time=0.02116227149963379, train_time=0.04359030723571777
[Epoch 0][Step 71], time=0.08623337745666504, ext_time=0.021187305450439453, train_time=0.04365229606628418
[Epoch 0][Step 72], time=0.08611845970153809, ext_time=0.021199703216552734, train_time=0.04359316825866699
[Epoch 0][Step 73], time=0.08619093894958496, ext_time=0.021231651306152344, train_time=0.04349946975708008
[Epoch 0][Step 74], time=0.0857698917388916, ext_time=0.021090984344482422, train_time=0.04333925247192383
[Epoch 0][Step 75], time=0.0874643325805664, ext_time=0.021177053451538086, train_time=0.0449368953704834
[Epoch 0][Step 76], time=0.0893547534942627, ext_time=0.02055668830871582, train_time=0.04740762710571289
[Epoch 0][Step 77], time=0.08585453033447266, ext_time=0.02109837532043457, train_time=0.0434572696685791
[Epoch 0][Step 78], time=0.08583641052246094, ext_time=0.021126508712768555, train_time=0.04334735870361328
[Epoch 0][Step 79], time=0.4181084632873535, ext_time=0.018387317657470703, train_time=0.37842726707458496
[Epoch 0][Step 80], time=0.08740854263305664, ext_time=0.02088785171508789, train_time=0.04521679878234863
[Epoch 0][Step 81], time=0.09303855895996094, ext_time=0.021036624908447266, train_time=0.05061459541320801
[Epoch 0][Step 82], time=0.08748364448547363, ext_time=0.02090311050415039, train_time=0.045243263244628906
[Epoch 0][Step 83], time=0.08720827102661133, ext_time=0.021037817001342773, train_time=0.04479622840881348
[Epoch 0][Step 84], time=0.08696961402893066, ext_time=0.021143436431884766, train_time=0.044492244720458984
[Epoch 0][Step 85], time=0.08935165405273438, ext_time=0.020390748977661133, train_time=0.04772138595581055
[Epoch 0][Step 86], time=0.08959245681762695, ext_time=0.02247476577758789, train_time=0.0456547737121582
[Epoch 0][Step 87], time=0.08755373954772949, ext_time=0.020995140075683594, train_time=0.04522418975830078
[Epoch 0][Step 88], time=0.09968280792236328, ext_time=0.020959138870239258, train_time=0.05748462677001953
[Epoch 0][Step 89], time=0.08699154853820801, ext_time=0.021001100540161133, train_time=0.044725894927978516
[Epoch 0][Step 90], time=0.08714914321899414, ext_time=0.020999908447265625, train_time=0.04483437538146973
[Epoch 0][Step 91], time=0.08724284172058105, ext_time=0.021053075790405273, train_time=0.04487442970275879
[Epoch 0][Step 92], time=0.08679747581481934, ext_time=0.021137237548828125, train_time=0.0443720817565918
[Epoch 0][Step 93], time=0.08653950691223145, ext_time=0.021042823791503906, train_time=0.0442500114440918
[Epoch 0][Step 94], time=0.08613038063049316, ext_time=0.021091461181640625, train_time=0.04375934600830078
[Epoch 0][Step 95], time=0.08577585220336914, ext_time=0.021149873733520508, train_time=0.04329705238342285
[Epoch 0][Step 96], time=0.08620023727416992, ext_time=0.02118206024169922, train_time=0.04354596138000488
[Epoch 0][Step 97], time=0.08605790138244629, ext_time=0.02114725112915039, train_time=0.043570518493652344
[Epoch 0][Step 98], time=0.0862586498260498, ext_time=0.021211624145507812, train_time=0.0435943603515625
[Epoch 0][Step 99], time=0.08612918853759766, ext_time=0.021184682846069336, train_time=0.04347825050354004
[Epoch 0][Step 100], time=0.08602714538574219, ext_time=0.021080732345581055, train_time=0.043642520904541016
[Epoch 0][Step 101], time=0.08594703674316406, ext_time=0.02129650115966797, train_time=0.04332327842712402
[Epoch 0][Step 102], time=0.08612537384033203, ext_time=0.021174907684326172, train_time=0.04351067543029785
[Epoch 0][Step 103], time=0.08611011505126953, ext_time=0.021256446838378906, train_time=0.04347991943359375
[Epoch 0][Step 104], time=0.08611202239990234, ext_time=0.021152734756469727, train_time=0.04364418983459473
[Epoch 0][Step 105], time=0.0859687328338623, ext_time=0.021158933639526367, train_time=0.04342150688171387
[Epoch 0][Step 106], time=0.0857853889465332, ext_time=0.02104783058166504, train_time=0.043428659439086914
[Epoch 0][Step 107], time=0.08979511260986328, ext_time=0.020766258239746094, train_time=0.047670602798461914
[Epoch 0][Step 108], time=0.08594036102294922, ext_time=0.0212247371673584, train_time=0.043462276458740234
[Epoch 0][Step 109], time=0.08595991134643555, ext_time=0.0212705135345459, train_time=0.04332256317138672
[Epoch 0][Step 110], time=0.08612632751464844, ext_time=0.021147727966308594, train_time=0.04374861717224121
[Epoch 0][Step 111], time=0.08606910705566406, ext_time=0.02119135856628418, train_time=0.04356551170349121
[Epoch 0][Step 112], time=0.09850335121154785, ext_time=0.021227598190307617, train_time=0.0434117317199707
[Epoch 0][Step 113], time=0.08603477478027344, ext_time=0.02118659019470215, train_time=0.04342484474182129
[Epoch 0][Step 114], time=0.08586573600769043, ext_time=0.021063804626464844, train_time=0.043465614318847656
[Epoch 0][Step 115], time=0.08591747283935547, ext_time=0.02121281623840332, train_time=0.04331183433532715
[Epoch 0][Step 116], time=0.08605432510375977, ext_time=0.021177291870117188, train_time=0.04345345497131348
[Epoch 0][Step 117], time=0.08600950241088867, ext_time=0.02114725112915039, train_time=0.043444156646728516
[Epoch 0][Step 118], time=0.08600378036499023, ext_time=0.021168947219848633, train_time=0.043447256088256836
[Epoch 0][Step 119], time=0.08614063262939453, ext_time=0.0211484432220459, train_time=0.04355740547180176
[Epoch 0][Step 120], time=0.08614635467529297, ext_time=0.02115154266357422, train_time=0.043654441833496094
[Epoch 0][Step 121], time=0.0861058235168457, ext_time=0.021202802658081055, train_time=0.04353213310241699
[Epoch 0][Step 122], time=0.08584260940551758, ext_time=0.021203994750976562, train_time=0.04325723648071289
[Epoch 0][Step 123], time=0.08599996566772461, ext_time=0.021079063415527344, train_time=0.04346966743469238
[Epoch 0][Step 124], time=0.08608198165893555, ext_time=0.021328449249267578, train_time=0.04339289665222168
[Epoch 0], time=12.612706899642944, loss=0.45033717155456543
[Epoch 1][Step 0], time=0.08594012260437012, ext_time=0.021181821823120117, train_time=0.04331374168395996
[Epoch 1][Step 1], time=0.08589673042297363, ext_time=0.021071672439575195, train_time=0.04340696334838867
[Epoch 1][Step 2], time=0.08604979515075684, ext_time=0.020984411239624023, train_time=0.04324674606323242
[Epoch 1][Step 3], time=0.08595561981201172, ext_time=0.021084308624267578, train_time=0.04351949691772461
[Epoch 1][Step 4], time=0.0856482982635498, ext_time=0.021170616149902344, train_time=0.04323720932006836
[Epoch 1][Step 5], time=0.0857396125793457, ext_time=0.021025896072387695, train_time=0.04352402687072754
[Epoch 1][Step 6], time=0.08582472801208496, ext_time=0.021071910858154297, train_time=0.043351173400878906
[Epoch 1][Step 7], time=0.08588671684265137, ext_time=0.021155834197998047, train_time=0.04332756996154785
[Epoch 1][Step 8], time=0.08604621887207031, ext_time=0.021037578582763672, train_time=0.04375767707824707
[Epoch 1][Step 9], time=0.08588075637817383, ext_time=0.021231889724731445, train_time=0.043322086334228516
[Epoch 1][Step 10], time=0.086029052734375, ext_time=0.021188020706176758, train_time=0.043430328369140625
[Epoch 1][Step 11], time=0.08609986305236816, ext_time=0.02110910415649414, train_time=0.04372715950012207
[Epoch 1][Step 12], time=0.08614253997802734, ext_time=0.021247386932373047, train_time=0.04350423812866211
[Epoch 1][Step 13], time=0.08611702919006348, ext_time=0.02115941047668457, train_time=0.043543100357055664
[Epoch 1][Step 14], time=0.08602261543273926, ext_time=0.02116107940673828, train_time=0.04354095458984375
[Epoch 1][Step 15], time=0.08614969253540039, ext_time=0.021353721618652344, train_time=0.043378591537475586
[Epoch 1][Step 16], time=0.08602738380432129, ext_time=0.021146535873413086, train_time=0.0435025691986084
[Epoch 1][Step 17], time=0.08636856079101562, ext_time=0.021139860153198242, train_time=0.04393291473388672
[Epoch 1][Step 18], time=0.08602285385131836, ext_time=0.021042823791503906, train_time=0.043648719787597656
[Epoch 1][Step 19], time=0.08618783950805664, ext_time=0.02104806900024414, train_time=0.043854475021362305
[Epoch 1][Step 20], time=0.08606886863708496, ext_time=0.02104783058166504, train_time=0.04362058639526367
[Epoch 1][Step 21], time=0.08609437942504883, ext_time=0.02104663848876953, train_time=0.043830156326293945
[Epoch 1][Step 22], time=0.08603048324584961, ext_time=0.021035194396972656, train_time=0.04370284080505371
[Epoch 1][Step 23], time=0.08619427680969238, ext_time=0.02121591567993164, train_time=0.043645381927490234
[Epoch 1][Step 24], time=0.08588480949401855, ext_time=0.021096229553222656, train_time=0.04348111152648926
[Epoch 1][Step 25], time=0.08626079559326172, ext_time=0.021310806274414062, train_time=0.04348945617675781
[Epoch 1][Step 26], time=0.08598947525024414, ext_time=0.021244287490844727, train_time=0.043359994888305664
[Epoch 1][Step 27], time=0.08600711822509766, ext_time=0.02118539810180664, train_time=0.04347825050354004
[Epoch 1][Step 28], time=0.0861976146697998, ext_time=0.02111339569091797, train_time=0.04372763633728027
[Epoch 1][Step 29], time=0.0899038314819336, ext_time=0.0210874080657959, train_time=0.04343676567077637
[Epoch 1][Step 30], time=0.08591556549072266, ext_time=0.02124500274658203, train_time=0.043296098709106445
[Epoch 1][Step 31], time=0.0861973762512207, ext_time=0.02119898796081543, train_time=0.04352378845214844
[Epoch 1][Step 32], time=0.08619284629821777, ext_time=0.021274328231811523, train_time=0.04346060752868652
[Epoch 1][Step 33], time=0.08620071411132812, ext_time=0.021192073822021484, train_time=0.043678998947143555
[Epoch 1][Step 34], time=0.08606481552124023, ext_time=0.021278858184814453, train_time=0.04336380958557129
[Epoch 1][Step 35], time=0.08609366416931152, ext_time=0.021041393280029297, train_time=0.043814897537231445
[Epoch 1][Step 36], time=0.08600687980651855, ext_time=0.021118640899658203, train_time=0.04347658157348633
[Epoch 1][Step 37], time=0.08585119247436523, ext_time=0.021166324615478516, train_time=0.0432736873626709
[Epoch 1][Step 38], time=0.0859825611114502, ext_time=0.021228790283203125, train_time=0.04332160949707031
[Epoch 1][Step 39], time=0.08608460426330566, ext_time=0.021178722381591797, train_time=0.043482303619384766
[Epoch 1][Step 40], time=0.0896444320678711, ext_time=0.022432804107666016, train_time=0.04565930366516113
[Epoch 1][Step 41], time=0.08608031272888184, ext_time=0.021188020706176758, train_time=0.04345846176147461
[Epoch 1][Step 42], time=0.08588933944702148, ext_time=0.021065235137939453, train_time=0.043520212173461914
[Epoch 1][Step 43], time=0.08787155151367188, ext_time=0.021020889282226562, train_time=0.04547595977783203
[Epoch 1][Step 44], time=0.08597254753112793, ext_time=0.0211331844329834, train_time=0.04354572296142578
[Epoch 1][Step 45], time=0.08583211898803711, ext_time=0.021150827407836914, train_time=0.043343305587768555
[Epoch 1][Step 46], time=0.08580946922302246, ext_time=0.021205902099609375, train_time=0.04325413703918457
[Epoch 1][Step 47], time=0.08617138862609863, ext_time=0.021210908889770508, train_time=0.043527841567993164
[Epoch 1][Step 48], time=0.08594989776611328, ext_time=0.021306991577148438, train_time=0.04326891899108887
[Epoch 1][Step 49], time=0.08595538139343262, ext_time=0.021084308624267578, train_time=0.04350900650024414
[Epoch 1][Step 50], time=0.08580231666564941, ext_time=0.021143674850463867, train_time=0.04330730438232422
[Epoch 1][Step 51], time=0.08581280708312988, ext_time=0.021042585372924805, train_time=0.04348468780517578
[Epoch 1][Step 52], time=0.08705925941467285, ext_time=0.021200180053710938, train_time=0.044497013092041016
[Epoch 1][Step 53], time=0.08593559265136719, ext_time=0.021007299423217773, train_time=0.04365801811218262
[Epoch 1][Step 54], time=0.08603191375732422, ext_time=0.021171092987060547, train_time=0.043372392654418945
[Epoch 1][Step 55], time=0.08587646484375, ext_time=0.021213293075561523, train_time=0.04329347610473633
[Epoch 1][Step 56], time=0.08583402633666992, ext_time=0.021192073822021484, train_time=0.04338502883911133
[Epoch 1][Step 57], time=0.08611607551574707, ext_time=0.021240949630737305, train_time=0.04354524612426758
[Epoch 1][Step 58], time=0.08654212951660156, ext_time=0.021103382110595703, train_time=0.044106483459472656
[Epoch 1][Step 59], time=0.08596396446228027, ext_time=0.020984172821044922, train_time=0.04379439353942871
[Epoch 1][Step 60], time=0.0858311653137207, ext_time=0.021128416061401367, train_time=0.043358564376831055
[Epoch 1][Step 61], time=0.08589601516723633, ext_time=0.020996809005737305, train_time=0.043611764907836914
[Epoch 1][Step 62], time=0.08615565299987793, ext_time=0.02114248275756836, train_time=0.04365873336791992
[Epoch 1][Step 63], time=0.08612775802612305, ext_time=0.02116537094116211, train_time=0.04366111755371094
[Epoch 1][Step 64], time=0.08588218688964844, ext_time=0.02118682861328125, train_time=0.04337120056152344
[Epoch 1][Step 65], time=0.08628225326538086, ext_time=0.021230697631835938, train_time=0.043668270111083984
[Epoch 1][Step 66], time=0.08618712425231934, ext_time=0.02116703987121582, train_time=0.04366779327392578
[Epoch 1][Step 67], time=0.0860891342163086, ext_time=0.021228790283203125, train_time=0.04333853721618652
[Epoch 1][Step 68], time=0.08597922325134277, ext_time=0.021027803421020508, train_time=0.04368424415588379
[Epoch 1][Step 69], time=0.08586573600769043, ext_time=0.020980119705200195, train_time=0.0435178279876709
[Epoch 1][Step 70], time=0.0861663818359375, ext_time=0.02121424674987793, train_time=0.04364156723022461
[Epoch 1][Step 71], time=0.09052014350891113, ext_time=0.02111220359802246, train_time=0.04366183280944824
[Epoch 1][Step 72], time=0.08608245849609375, ext_time=0.02118515968322754, train_time=0.04349255561828613
[Epoch 1][Step 73], time=0.08593106269836426, ext_time=0.021130800247192383, train_time=0.04338359832763672
[Epoch 1][Step 74], time=0.08599424362182617, ext_time=0.021132946014404297, train_time=0.0434269905090332
[Epoch 1][Step 75], time=0.08607745170593262, ext_time=0.021172046661376953, train_time=0.043492794036865234
[Epoch 1][Step 76], time=0.08809065818786621, ext_time=0.021066665649414062, train_time=0.0456995964050293
[Epoch 1][Step 77], time=0.08606100082397461, ext_time=0.021067142486572266, train_time=0.043596744537353516
[Epoch 1][Step 78], time=0.0856931209564209, ext_time=0.021151304244995117, train_time=0.0432133674621582
[Epoch 1][Step 79], time=0.09002923965454102, ext_time=0.020627975463867188, train_time=0.04803276062011719
[Epoch 1][Step 80], time=0.08580446243286133, ext_time=0.02108025550842285, train_time=0.04351687431335449
[Epoch 1][Step 81], time=0.08707523345947266, ext_time=0.02120375633239746, train_time=0.04445052146911621
[Epoch 1][Step 82], time=0.08621954917907715, ext_time=0.02123546600341797, train_time=0.04352760314941406
[Epoch 1][Step 83], time=0.0862586498260498, ext_time=0.02115488052368164, train_time=0.04365253448486328
[Epoch 1][Step 84], time=0.08583354949951172, ext_time=0.02113938331604004, train_time=0.04340076446533203
[Epoch 1][Step 85], time=0.08605265617370605, ext_time=0.021129846572875977, train_time=0.04357719421386719
[Epoch 1][Step 86], time=0.08604049682617188, ext_time=0.0211637020111084, train_time=0.04344940185546875
[Epoch 1][Step 87], time=0.0860300064086914, ext_time=0.02123236656188965, train_time=0.043471574783325195
[Epoch 1][Step 88], time=0.08614873886108398, ext_time=0.021060466766357422, train_time=0.04369378089904785
[Epoch 1][Step 89], time=0.08592867851257324, ext_time=0.02111339569091797, train_time=0.043415069580078125
[Epoch 1][Step 90], time=0.08577656745910645, ext_time=0.021066904067993164, train_time=0.04345130920410156
[Epoch 1][Step 91], time=0.08614969253540039, ext_time=0.021291732788085938, train_time=0.043411970138549805
[Epoch 1][Step 92], time=0.08605766296386719, ext_time=0.02116680145263672, train_time=0.043456315994262695
[Epoch 1][Step 93], time=0.08597254753112793, ext_time=0.021210908889770508, train_time=0.04336071014404297
[Epoch 1][Step 94], time=0.0858912467956543, ext_time=0.02115464210510254, train_time=0.04338383674621582
[Epoch 1][Step 95], time=0.08600616455078125, ext_time=0.02117156982421875, train_time=0.04341268539428711
[Epoch 1][Step 96], time=0.0862874984741211, ext_time=0.021109342575073242, train_time=0.04379868507385254
[Epoch 1][Step 97], time=0.08625292778015137, ext_time=0.02115035057067871, train_time=0.043782711029052734
[Epoch 1][Step 98], time=0.08623838424682617, ext_time=0.021234512329101562, train_time=0.04369354248046875
[Epoch 1][Step 99], time=0.08609199523925781, ext_time=0.021164655685424805, train_time=0.04356074333190918
[Epoch 1][Step 100], time=0.08618998527526855, ext_time=0.021207094192504883, train_time=0.04358077049255371
[Epoch 1][Step 101], time=0.08615541458129883, ext_time=0.02124500274658203, train_time=0.043556928634643555
[Epoch 1][Step 102], time=0.08587908744812012, ext_time=0.021072864532470703, train_time=0.04343128204345703
[Epoch 1][Step 103], time=0.08624792098999023, ext_time=0.021225452423095703, train_time=0.04353666305541992
[Epoch 1][Step 104], time=0.0861210823059082, ext_time=0.021178483963012695, train_time=0.0436708927154541
[Epoch 1][Step 105], time=0.08626961708068848, ext_time=0.021098613739013672, train_time=0.0436861515045166
[Epoch 1][Step 106], time=0.08593988418579102, ext_time=0.021160364151000977, train_time=0.04339098930358887
[Epoch 1][Step 107], time=0.08611583709716797, ext_time=0.021231651306152344, train_time=0.043528079986572266
[Epoch 1][Step 108], time=0.08607602119445801, ext_time=0.02110433578491211, train_time=0.04368162155151367
[Epoch 1][Step 109], time=0.08623838424682617, ext_time=0.02114725112915039, train_time=0.04373979568481445
[Epoch 1][Step 110], time=0.08615708351135254, ext_time=0.021744489669799805, train_time=0.043181419372558594
[Epoch 1][Step 111], time=0.08584094047546387, ext_time=0.021029949188232422, train_time=0.04361701011657715
[Epoch 1][Step 112], time=0.08601999282836914, ext_time=0.02118682861328125, train_time=0.04341387748718262
[Epoch 1][Step 113], time=0.09026217460632324, ext_time=0.021190404891967773, train_time=0.04352235794067383
[Epoch 1][Step 114], time=0.08613038063049316, ext_time=0.021123886108398438, train_time=0.04369378089904785
[Epoch 1][Step 115], time=0.08605074882507324, ext_time=0.02116847038269043, train_time=0.0434877872467041
[Epoch 1][Step 116], time=0.08623313903808594, ext_time=0.021181106567382812, train_time=0.04368424415588379
[Epoch 1][Step 117], time=0.08590555191040039, ext_time=0.02105402946472168, train_time=0.04345369338989258
[Epoch 1][Step 118], time=0.08601140975952148, ext_time=0.021114110946655273, train_time=0.04342222213745117
[Epoch 1][Step 119], time=0.08605623245239258, ext_time=0.02108931541442871, train_time=0.04359889030456543
[Epoch 1][Step 120], time=0.08605074882507324, ext_time=0.021177053451538086, train_time=0.04349327087402344
[Epoch 1][Step 121], time=0.08594226837158203, ext_time=0.021122217178344727, train_time=0.043501853942871094
[Epoch 1][Step 122], time=0.08601713180541992, ext_time=0.021210908889770508, train_time=0.04341316223144531
[Epoch 1][Step 123], time=0.08591032028198242, ext_time=0.020873546600341797, train_time=0.043708086013793945
[Epoch 1][Step 124], time=0.08600592613220215, ext_time=0.021198034286499023, train_time=0.04341626167297363
[Epoch 1], time=10.787014722824097, loss=0.4280410408973694
[Epoch 2][Step 0], time=0.08595609664916992, ext_time=0.02112889289855957, train_time=0.04350638389587402
[Epoch 2][Step 1], time=0.08571887016296387, ext_time=0.020979881286621094, train_time=0.0433197021484375
[Epoch 2][Step 2], time=0.08756327629089355, ext_time=0.021125316619873047, train_time=0.04505181312561035
[Epoch 2][Step 3], time=0.08595681190490723, ext_time=0.021183490753173828, train_time=0.0433964729309082
[Epoch 2][Step 4], time=0.08604025840759277, ext_time=0.02113819122314453, train_time=0.04347491264343262
[Epoch 2][Step 5], time=0.08593082427978516, ext_time=0.021152973175048828, train_time=0.04343605041503906
[Epoch 2][Step 6], time=0.08593606948852539, ext_time=0.0210568904876709, train_time=0.04342389106750488
[Epoch 2][Step 7], time=0.08590221405029297, ext_time=0.021015405654907227, train_time=0.0435948371887207
[Epoch 2][Step 8], time=0.08577370643615723, ext_time=0.0210115909576416, train_time=0.043562889099121094
[Epoch 2][Step 9], time=0.0861976146697998, ext_time=0.021230697631835938, train_time=0.04352402687072754
[Epoch 2][Step 10], time=0.0858604907989502, ext_time=0.02113485336303711, train_time=0.04338431358337402
[Epoch 2][Step 11], time=0.0859839916229248, ext_time=0.021136999130249023, train_time=0.04350733757019043
[Epoch 2][Step 12], time=0.08589863777160645, ext_time=0.021224498748779297, train_time=0.04340386390686035
[Epoch 2][Step 13], time=0.08648300170898438, ext_time=0.021237611770629883, train_time=0.043859004974365234
[Epoch 2][Step 14], time=0.08636665344238281, ext_time=0.021234989166259766, train_time=0.04380083084106445
[Epoch 2][Step 15], time=0.08609986305236816, ext_time=0.021090984344482422, train_time=0.04378247261047363
[Epoch 2][Step 16], time=0.08583807945251465, ext_time=0.021081209182739258, train_time=0.043477773666381836
[Epoch 2][Step 17], time=0.08586835861206055, ext_time=0.021164417266845703, train_time=0.04331612586975098
[Epoch 2][Step 18], time=0.08588051795959473, ext_time=0.02117466926574707, train_time=0.04333901405334473
[Epoch 2][Step 19], time=0.08610129356384277, ext_time=0.02112746238708496, train_time=0.04357147216796875
[Epoch 2][Step 20], time=0.08604741096496582, ext_time=0.02111673355102539, train_time=0.04367470741271973
[Epoch 2][Step 21], time=0.08592772483825684, ext_time=0.021204710006713867, train_time=0.043322086334228516
[Epoch 2][Step 22], time=0.08605694770812988, ext_time=0.020987987518310547, train_time=0.04384160041809082
[Epoch 2][Step 23], time=0.08629155158996582, ext_time=0.021221160888671875, train_time=0.04372978210449219
[Epoch 2][Step 24], time=0.08625912666320801, ext_time=0.021219730377197266, train_time=0.04363536834716797
[Epoch 2][Step 25], time=0.08722996711730957, ext_time=0.02116990089416504, train_time=0.04463768005371094
[Epoch 2][Step 26], time=0.08606553077697754, ext_time=0.02105093002319336, train_time=0.04367470741271973
[Epoch 2][Step 27], time=0.0861215591430664, ext_time=0.021233320236206055, train_time=0.04349470138549805
[Epoch 2][Step 28], time=0.08588862419128418, ext_time=0.021079301834106445, train_time=0.04341530799865723
[Epoch 2][Step 29], time=0.08614277839660645, ext_time=0.02106308937072754, train_time=0.04381895065307617
[Epoch 2][Step 30], time=0.09012174606323242, ext_time=0.021154403686523438, train_time=0.04339027404785156
[Epoch 2][Step 31], time=0.08607888221740723, ext_time=0.02124929428100586, train_time=0.04342389106750488
[Epoch 2][Step 32], time=0.0860288143157959, ext_time=0.021200180053710938, train_time=0.04344630241394043
[Epoch 2][Step 33], time=0.08579182624816895, ext_time=0.02113652229309082, train_time=0.04330277442932129
[Epoch 2][Step 34], time=0.08597922325134277, ext_time=0.021246671676635742, train_time=0.0433354377746582
[Epoch 2][Step 35], time=0.0861673355102539, ext_time=0.021092653274536133, train_time=0.043682098388671875
[Epoch 2][Step 36], time=0.08623576164245605, ext_time=0.021068334579467773, train_time=0.04378247261047363
[Epoch 2][Step 37], time=0.08582401275634766, ext_time=0.02116680145263672, train_time=0.04332399368286133
[Epoch 2][Step 38], time=0.08614635467529297, ext_time=0.021263837814331055, train_time=0.04343271255493164
[Epoch 2][Step 39], time=0.08742642402648926, ext_time=0.02276754379272461, train_time=0.043313026428222656
[Epoch 2][Step 40], time=0.08622193336486816, ext_time=0.02122950553894043, train_time=0.04350543022155762
[Epoch 2][Step 41], time=0.08619451522827148, ext_time=0.021183490753173828, train_time=0.043547868728637695
[Epoch 2][Step 42], time=0.08593392372131348, ext_time=0.0211639404296875, train_time=0.04342770576477051
[Epoch 2][Step 43], time=0.08603191375732422, ext_time=0.020907163619995117, train_time=0.04394721984863281
[Epoch 2][Step 44], time=0.08626341819763184, ext_time=0.02133631706237793, train_time=0.04346513748168945
[Epoch 2][Step 45], time=0.08607840538024902, ext_time=0.02114391326904297, train_time=0.043634653091430664
[Epoch 2][Step 46], time=0.08611035346984863, ext_time=0.02124333381652832, train_time=0.04347944259643555
[Epoch 2][Step 47], time=0.08644962310791016, ext_time=0.02119612693786621, train_time=0.04377317428588867
[Epoch 2][Step 48], time=0.08609700202941895, ext_time=0.021363496780395508, train_time=0.04333233833312988
[Epoch 2][Step 49], time=0.0861046314239502, ext_time=0.021185636520385742, train_time=0.04361295700073242
[Epoch 2][Step 50], time=0.08593988418579102, ext_time=0.021012067794799805, train_time=0.0435640811920166
[Epoch 2][Step 51], time=0.08611559867858887, ext_time=0.02092266082763672, train_time=0.043843746185302734
[Epoch 2][Step 52], time=0.08602786064147949, ext_time=0.021164894104003906, train_time=0.04340553283691406
[Epoch 2][Step 53], time=0.08622145652770996, ext_time=0.021101713180541992, train_time=0.043770790100097656
[Epoch 2][Step 54], time=0.0862276554107666, ext_time=0.0212705135345459, train_time=0.04349350929260254
[Epoch 2][Step 55], time=0.08615446090698242, ext_time=0.02127361297607422, train_time=0.04340982437133789
[Epoch 2][Step 56], time=0.08599638938903809, ext_time=0.021234512329101562, train_time=0.0434110164642334
[Epoch 2][Step 57], time=0.08620715141296387, ext_time=0.021198272705078125, train_time=0.04355573654174805
[Epoch 2][Step 58], time=0.08614349365234375, ext_time=0.021080493927001953, train_time=0.043581485748291016
[Epoch 2][Step 59], time=0.08606433868408203, ext_time=0.021119117736816406, train_time=0.04370570182800293
[Epoch 2][Step 60], time=0.08619165420532227, ext_time=0.0212709903717041, train_time=0.04352617263793945
[Epoch 2][Step 61], time=0.08603835105895996, ext_time=0.021233320236206055, train_time=0.043440818786621094
[Epoch 2][Step 62], time=0.0862579345703125, ext_time=0.021125078201293945, train_time=0.04369974136352539
[Epoch 2][Step 63], time=0.08588743209838867, ext_time=0.021219968795776367, train_time=0.043294668197631836
[Epoch 2][Step 64], time=0.08638143539428711, ext_time=0.02127385139465332, train_time=0.04373884201049805
[Epoch 2][Step 65], time=0.08611559867858887, ext_time=0.021303176879882812, train_time=0.04344797134399414
[Epoch 2][Step 66], time=0.08611059188842773, ext_time=0.02119731903076172, train_time=0.04348015785217285
[Epoch 2][Step 67], time=0.09008455276489258, ext_time=0.020373106002807617, train_time=0.04703474044799805
[Epoch 2][Step 68], time=0.08609700202941895, ext_time=0.021109580993652344, train_time=0.0436854362487793
[Epoch 2][Step 69], time=0.08616113662719727, ext_time=0.021147727966308594, train_time=0.043671369552612305
[Epoch 2][Step 70], time=0.08608436584472656, ext_time=0.021204710006713867, train_time=0.04356837272644043
[Epoch 2][Step 71], time=0.08607625961303711, ext_time=0.021155595779418945, train_time=0.04361557960510254
[Epoch 2][Step 72], time=0.09036087989807129, ext_time=0.02110743522644043, train_time=0.043462276458740234
[Epoch 2][Step 73], time=0.0861043930053711, ext_time=0.021123409271240234, train_time=0.043593645095825195
[Epoch 2][Step 74], time=0.08590507507324219, ext_time=0.021119356155395508, train_time=0.04337573051452637
[Epoch 2][Step 75], time=0.08596968650817871, ext_time=0.021187305450439453, train_time=0.04337739944458008
[Epoch 2][Step 76], time=0.08607769012451172, ext_time=0.021083354949951172, train_time=0.04364919662475586
[Epoch 2][Step 77], time=0.08596634864807129, ext_time=0.021238088607788086, train_time=0.04335761070251465
[Epoch 2][Step 78], time=0.08584308624267578, ext_time=0.021185636520385742, train_time=0.04330897331237793
[Epoch 2][Step 79], time=0.0862417221069336, ext_time=0.02117753028869629, train_time=0.04367375373840332
[Epoch 2][Step 80], time=0.08609366416931152, ext_time=0.021112918853759766, train_time=0.04364132881164551
[Epoch 2][Step 81], time=0.08645343780517578, ext_time=0.02070474624633789, train_time=0.043376922607421875
[Epoch 2][Step 82], time=0.08595848083496094, ext_time=0.021159887313842773, train_time=0.043395280838012695
[Epoch 2][Step 83], time=0.08598160743713379, ext_time=0.021153926849365234, train_time=0.04341387748718262
[Epoch 2][Step 84], time=0.08572101593017578, ext_time=0.021067142486572266, train_time=0.043549299240112305
[Epoch 2][Step 85], time=0.08610987663269043, ext_time=0.020952701568603516, train_time=0.04384446144104004
[Epoch 2][Step 86], time=0.08614921569824219, ext_time=0.021179914474487305, train_time=0.04366493225097656
[Epoch 2][Step 87], time=0.08596324920654297, ext_time=0.021219968795776367, train_time=0.04343080520629883
[Epoch 2][Step 88], time=0.08600997924804688, ext_time=0.02113485336303711, train_time=0.04349398612976074
[Epoch 2][Step 89], time=0.0858919620513916, ext_time=0.02112126350402832, train_time=0.0434417724609375
[Epoch 2][Step 90], time=0.08590388298034668, ext_time=0.021180391311645508, train_time=0.04339194297790527
[Epoch 2][Step 91], time=0.08612871170043945, ext_time=0.021256685256958008, train_time=0.043435096740722656
[Epoch 2][Step 92], time=0.08591413497924805, ext_time=0.021173715591430664, train_time=0.04331374168395996
[Epoch 2][Step 93], time=0.08616209030151367, ext_time=0.021158695220947266, train_time=0.043622732162475586
[Epoch 2][Step 94], time=0.08554768562316895, ext_time=0.021051883697509766, train_time=0.04327201843261719
[Epoch 2][Step 95], time=0.08655500411987305, ext_time=0.020447492599487305, train_time=0.0433659553527832
[Epoch 2][Step 96], time=0.0862276554107666, ext_time=0.021207571029663086, train_time=0.04359698295593262
[Epoch 2][Step 97], time=0.08596324920654297, ext_time=0.021224260330200195, train_time=0.04345822334289551
[Epoch 2][Step 98], time=0.08597445487976074, ext_time=0.021121978759765625, train_time=0.043447256088256836
[Epoch 2][Step 99], time=0.08592629432678223, ext_time=0.02122020721435547, train_time=0.04330778121948242
[Epoch 2][Step 100], time=0.08591008186340332, ext_time=0.0212094783782959, train_time=0.04332256317138672
[Epoch 2][Step 101], time=0.08607935905456543, ext_time=0.021216392517089844, train_time=0.043512582778930664
[Epoch 2][Step 102], time=0.08607339859008789, ext_time=0.02103281021118164, train_time=0.04369997978210449
[Epoch 2][Step 103], time=0.08649826049804688, ext_time=0.021124601364135742, train_time=0.04401564598083496
[Epoch 2][Step 104], time=0.08617496490478516, ext_time=0.02127981185913086, train_time=0.043584346771240234
[Epoch 2][Step 105], time=0.08573698997497559, ext_time=0.0210874080657959, train_time=0.04329323768615723
[Epoch 2][Step 106], time=0.08612537384033203, ext_time=0.02113175392150879, train_time=0.04377436637878418
[Epoch 2][Step 107], time=0.08624935150146484, ext_time=0.02112412452697754, train_time=0.04384183883666992
[Epoch 2][Step 108], time=0.0860147476196289, ext_time=0.02121710777282715, train_time=0.043415069580078125
[Epoch 2][Step 109], time=0.08629631996154785, ext_time=0.02080249786376953, train_time=0.04347991943359375
[Epoch 2][Step 110], time=0.0867013931274414, ext_time=0.020747900009155273, train_time=0.04471755027770996
[Epoch 2][Step 111], time=0.08609437942504883, ext_time=0.021162748336791992, train_time=0.04371380805969238
[Epoch 2][Step 112], time=0.08601045608520508, ext_time=0.021154165267944336, train_time=0.04340314865112305
[Epoch 2][Step 113], time=0.08647632598876953, ext_time=0.021007776260375977, train_time=0.04423356056213379
[Epoch 2][Step 114], time=0.09059667587280273, ext_time=0.021075010299682617, train_time=0.0436549186706543
[Epoch 2][Step 115], time=0.08610177040100098, ext_time=0.021123409271240234, train_time=0.04363393783569336
[Epoch 2][Step 116], time=0.08629250526428223, ext_time=0.021282434463500977, train_time=0.043581485748291016
[Epoch 2][Step 117], time=0.08569645881652832, ext_time=0.02094864845275879, train_time=0.0434720516204834
[Epoch 2][Step 118], time=0.08591318130493164, ext_time=0.0211484432220459, train_time=0.04335141181945801
[Epoch 2][Step 119], time=0.08589029312133789, ext_time=0.021049022674560547, train_time=0.04347372055053711
[Epoch 2][Step 120], time=0.08641481399536133, ext_time=0.021116971969604492, train_time=0.043913841247558594
[Epoch 2][Step 121], time=0.08585238456726074, ext_time=0.021201372146606445, train_time=0.04328584671020508
[Epoch 2][Step 122], time=0.08579778671264648, ext_time=0.021131277084350586, train_time=0.04334378242492676
[Epoch 2][Step 123], time=0.0859379768371582, ext_time=0.021097183227539062, train_time=0.04359722137451172
[Epoch 2][Step 124], time=0.08608007431030273, ext_time=0.02116107940673828, train_time=0.0436863899230957
[Epoch 2], time=10.786272287368774, loss=0.42028364539146423
[Epoch 3][Step 0], time=0.08602166175842285, ext_time=0.021234750747680664, train_time=0.04338645935058594
[Epoch 3][Step 1], time=0.08594202995300293, ext_time=0.02115178108215332, train_time=0.043366432189941406
[Epoch 3][Step 2], time=0.08611726760864258, ext_time=0.021213054656982422, train_time=0.04359722137451172
[Epoch 3][Step 3], time=0.08596014976501465, ext_time=0.021147727966308594, train_time=0.043531179428100586
[Epoch 3][Step 4], time=0.08602762222290039, ext_time=0.021149158477783203, train_time=0.04342174530029297
[Epoch 3][Step 5], time=0.08577203750610352, ext_time=0.021192550659179688, train_time=0.04323530197143555
[Epoch 3][Step 6], time=0.08608651161193848, ext_time=0.021208524703979492, train_time=0.04348421096801758
[Epoch 3][Step 7], time=0.08577656745910645, ext_time=0.021131515502929688, train_time=0.04330801963806152
[Epoch 3][Step 8], time=0.08997225761413574, ext_time=0.020387887954711914, train_time=0.04837346076965332
[Epoch 3][Step 9], time=0.08580636978149414, ext_time=0.0211489200592041, train_time=0.043349504470825195
[Epoch 3][Step 10], time=0.08601212501525879, ext_time=0.021217823028564453, train_time=0.043364763259887695
[Epoch 3][Step 11], time=0.08577513694763184, ext_time=0.021193981170654297, train_time=0.04323291778564453
[Epoch 3][Step 12], time=0.08631110191345215, ext_time=0.021253585815429688, train_time=0.04351210594177246
[Epoch 3][Step 13], time=0.08648014068603516, ext_time=0.021025896072387695, train_time=0.04415726661682129
[Epoch 3][Step 14], time=0.08611893653869629, ext_time=0.02132725715637207, train_time=0.04346489906311035
[Epoch 3][Step 15], time=0.08607792854309082, ext_time=0.021184444427490234, train_time=0.043520450592041016
[Epoch 3][Step 16], time=0.08600306510925293, ext_time=0.021162986755371094, train_time=0.04349470138549805
[Epoch 3][Step 17], time=0.08603286743164062, ext_time=0.021160364151000977, train_time=0.043564796447753906
[Epoch 3][Step 18], time=0.0860130786895752, ext_time=0.02117609977722168, train_time=0.04346871376037598
[Epoch 3][Step 19], time=0.08624029159545898, ext_time=0.021313190460205078, train_time=0.04349827766418457
[Epoch 3][Step 20], time=0.08600044250488281, ext_time=0.021190404891967773, train_time=0.04340720176696777
[Epoch 3][Step 21], time=0.08594846725463867, ext_time=0.021183490753173828, train_time=0.04338502883911133
[Epoch 3][Step 22], time=0.08601093292236328, ext_time=0.021087646484375, train_time=0.04365730285644531
[Epoch 3][Step 23], time=0.08619046211242676, ext_time=0.021200180053710938, train_time=0.04361128807067871
[Epoch 3][Step 24], time=0.08611035346984863, ext_time=0.021221399307250977, train_time=0.04343605041503906
[Epoch 3][Step 25], time=0.08605241775512695, ext_time=0.021208763122558594, train_time=0.0434725284576416
[Epoch 3][Step 26], time=0.08610391616821289, ext_time=0.02117466926574707, train_time=0.04351663589477539
[Epoch 3][Step 27], time=0.08656573295593262, ext_time=0.021163225173950195, train_time=0.04406285285949707
[Epoch 3][Step 28], time=0.0858464241027832, ext_time=0.02107858657836914, train_time=0.04343581199645996
[Epoch 3][Step 29], time=0.0859980583190918, ext_time=0.02116680145263672, train_time=0.04349994659423828
[Epoch 3][Step 30], time=0.08617687225341797, ext_time=0.02129387855529785, train_time=0.043433427810668945
[Epoch 3][Step 31], time=0.09053730964660645, ext_time=0.02118992805480957, train_time=0.043550729751586914
[Epoch 3][Step 32], time=0.08601546287536621, ext_time=0.02124166488647461, train_time=0.04334592819213867
[Epoch 3][Step 33], time=0.08597612380981445, ext_time=0.021197080612182617, train_time=0.04337668418884277
[Epoch 3][Step 34], time=0.08555340766906738, ext_time=0.021054744720458984, train_time=0.04326152801513672
[Epoch 3][Step 35], time=0.08606147766113281, ext_time=0.02109360694885254, train_time=0.04369354248046875
[Epoch 3][Step 36], time=0.08600306510925293, ext_time=0.021056175231933594, train_time=0.04347991943359375
[Epoch 3][Step 37], time=0.08592009544372559, ext_time=0.021111011505126953, train_time=0.043410301208496094
[Epoch 3][Step 38], time=0.08585667610168457, ext_time=0.021178245544433594, train_time=0.04332423210144043
[Epoch 3][Step 39], time=0.08608865737915039, ext_time=0.021168947219848633, train_time=0.04350399971008301
[Epoch 3][Step 40], time=0.09035968780517578, ext_time=0.022504806518554688, train_time=0.04627060890197754
[Epoch 3][Step 41], time=0.0863032341003418, ext_time=0.021279335021972656, train_time=0.043554067611694336
[Epoch 3][Step 42], time=0.08597898483276367, ext_time=0.02115797996520996, train_time=0.04349493980407715
[Epoch 3][Step 43], time=0.0859975814819336, ext_time=0.021170616149902344, train_time=0.04351401329040527
[Epoch 3][Step 44], time=0.08586382865905762, ext_time=0.021108627319335938, train_time=0.04333782196044922
[Epoch 3][Step 45], time=0.08614134788513184, ext_time=0.021190881729125977, train_time=0.043534040451049805
[Epoch 3][Step 46], time=0.08587908744812012, ext_time=0.02117300033569336, train_time=0.043314218521118164
[Epoch 3][Step 47], time=0.08606719970703125, ext_time=0.021153926849365234, train_time=0.04343700408935547
[Epoch 3][Step 48], time=0.08601880073547363, ext_time=0.02127695083618164, train_time=0.043477535247802734
[Epoch 3][Step 49], time=0.08593583106994629, ext_time=0.021182775497436523, train_time=0.04343557357788086
[Epoch 3][Step 50], time=0.08608484268188477, ext_time=0.02115631103515625, train_time=0.04354429244995117
[Epoch 3][Step 51], time=0.08579516410827637, ext_time=0.021120548248291016, train_time=0.04337501525878906
[Epoch 3][Step 52], time=0.08587360382080078, ext_time=0.021207094192504883, train_time=0.043288230895996094
[Epoch 3][Step 53], time=0.08650612831115723, ext_time=0.021162033081054688, train_time=0.043903350830078125
[Epoch 3][Step 54], time=0.0868229866027832, ext_time=0.02128744125366211, train_time=0.04413127899169922
[Epoch 3][Step 55], time=0.08597874641418457, ext_time=0.021169424057006836, train_time=0.04355502128601074
[Epoch 3][Step 56], time=0.08600974082946777, ext_time=0.02117156982421875, train_time=0.04366421699523926
[Epoch 3][Step 57], time=0.08592581748962402, ext_time=0.021092891693115234, train_time=0.04359865188598633
[Epoch 3][Step 58], time=0.08607125282287598, ext_time=0.02119278907775879, train_time=0.04342985153198242
[Epoch 3][Step 59], time=0.08572983741760254, ext_time=0.02112722396850586, train_time=0.04331159591674805
[Epoch 3][Step 60], time=0.08575606346130371, ext_time=0.021139144897460938, train_time=0.043243408203125
[Epoch 3][Step 61], time=0.08611083030700684, ext_time=0.02124190330505371, train_time=0.04347872734069824
[Epoch 3][Step 62], time=0.08600258827209473, ext_time=0.02114391326904297, train_time=0.04343080520629883
[Epoch 3][Step 63], time=0.08584117889404297, ext_time=0.021205663681030273, train_time=0.043295860290527344
[Epoch 3][Step 64], time=0.08580589294433594, ext_time=0.02127671241760254, train_time=0.04323983192443848
[Epoch 3][Step 65], time=0.08629822731018066, ext_time=0.021329402923583984, train_time=0.04357624053955078
[Epoch 3][Step 66], time=0.08608746528625488, ext_time=0.021215438842773438, train_time=0.043474435806274414
[Epoch 3][Step 67], time=0.0863187313079834, ext_time=0.02125263214111328, train_time=0.04363417625427246
[Epoch 3][Step 68], time=0.08605122566223145, ext_time=0.020951032638549805, train_time=0.04381275177001953
[Epoch 3][Step 69], time=0.08618760108947754, ext_time=0.021260738372802734, train_time=0.04353952407836914
[Epoch 3][Step 70], time=0.08627557754516602, ext_time=0.02114248275756836, train_time=0.0438232421875
[Epoch 3][Step 71], time=0.08596515655517578, ext_time=0.021264076232910156, train_time=0.043350934982299805
[Epoch 3][Step 72], time=0.08600091934204102, ext_time=0.02123236656188965, train_time=0.0433657169342041
[Epoch 3][Step 73], time=0.09078335762023926, ext_time=0.02124333381652832, train_time=0.043367624282836914
[Epoch 3][Step 74], time=0.0857839584350586, ext_time=0.021214723587036133, train_time=0.043253183364868164
[Epoch 3][Step 75], time=0.08600950241088867, ext_time=0.021246910095214844, train_time=0.04334115982055664
[Epoch 3][Step 76], time=0.08600783348083496, ext_time=0.021056413650512695, train_time=0.04365825653076172
[Epoch 3][Step 77], time=0.08592605590820312, ext_time=0.02115154266357422, train_time=0.04343461990356445
[Epoch 3][Step 78], time=0.08614635467529297, ext_time=0.021000146865844727, train_time=0.04382157325744629
[Epoch 3][Step 79], time=0.08603763580322266, ext_time=0.021252155303955078, train_time=0.04336118698120117
[Epoch 3][Step 80], time=0.08580732345581055, ext_time=0.02106332778930664, train_time=0.04338812828063965
[Epoch 3][Step 81], time=0.08603858947753906, ext_time=0.02121758460998535, train_time=0.04341268539428711
[Epoch 3][Step 82], time=0.086517333984375, ext_time=0.0210721492767334, train_time=0.04396200180053711
[Epoch 3][Step 83], time=0.08609652519226074, ext_time=0.02120375633239746, train_time=0.043454885482788086
[Epoch 3][Step 84], time=0.08617281913757324, ext_time=0.02124190330505371, train_time=0.043616294860839844
[Epoch 3][Step 85], time=0.08587932586669922, ext_time=0.021127939224243164, train_time=0.04346609115600586
[Epoch 3][Step 86], time=0.08590030670166016, ext_time=0.02113652229309082, train_time=0.04342007637023926
[Epoch 3][Step 87], time=0.08621692657470703, ext_time=0.021213531494140625, train_time=0.04361987113952637
[Epoch 3][Step 88], time=0.08610892295837402, ext_time=0.020812511444091797, train_time=0.04389023780822754
[Epoch 3][Step 89], time=0.08580398559570312, ext_time=0.02109837532043457, train_time=0.04331207275390625
[Epoch 3][Step 90], time=0.0859224796295166, ext_time=0.021132946014404297, train_time=0.04341006278991699
[Epoch 3][Step 91], time=0.08600378036499023, ext_time=0.021178245544433594, train_time=0.043480634689331055
[Epoch 3][Step 92], time=0.08614993095397949, ext_time=0.02119588851928711, train_time=0.04354381561279297
[Epoch 3][Step 93], time=0.08619117736816406, ext_time=0.021233081817626953, train_time=0.04361271858215332
[Epoch 3][Step 94], time=0.08599996566772461, ext_time=0.021190643310546875, train_time=0.043509721755981445
[Epoch 3][Step 95], time=0.08598566055297852, ext_time=0.02115345001220703, train_time=0.04349255561828613
[Epoch 3][Step 96], time=0.08619308471679688, ext_time=0.021229982376098633, train_time=0.043502092361450195
[Epoch 3][Step 97], time=0.08631038665771484, ext_time=0.020931482315063477, train_time=0.044149160385131836
[Epoch 3][Step 98], time=0.08605790138244629, ext_time=0.021217823028564453, train_time=0.043445587158203125
[Epoch 3][Step 99], time=0.0858757495880127, ext_time=0.021131277084350586, train_time=0.043380022048950195
[Epoch 3][Step 100], time=0.08603906631469727, ext_time=0.02126169204711914, train_time=0.04338359832763672
[Epoch 3][Step 101], time=0.08603143692016602, ext_time=0.021204471588134766, train_time=0.04340314865112305
[Epoch 3][Step 102], time=0.08590197563171387, ext_time=0.02108931541442871, train_time=0.04347538948059082
[Epoch 3][Step 103], time=0.08600020408630371, ext_time=0.0211944580078125, train_time=0.04339861869812012
[Epoch 3][Step 104], time=0.08609676361083984, ext_time=0.021147727966308594, train_time=0.04369997978210449
[Epoch 3][Step 105], time=0.08596587181091309, ext_time=0.021154403686523438, train_time=0.043393850326538086
[Epoch 3][Step 106], time=0.08599567413330078, ext_time=0.02121758460998535, train_time=0.043398141860961914
[Epoch 3][Step 107], time=0.08619546890258789, ext_time=0.02115321159362793, train_time=0.043749094009399414
[Epoch 3][Step 108], time=0.0859670639038086, ext_time=0.02124333381652832, train_time=0.0434112548828125
[Epoch 3][Step 109], time=0.08672332763671875, ext_time=0.02091693878173828, train_time=0.04436969757080078
[Epoch 3][Step 110], time=0.0860593318939209, ext_time=0.021077394485473633, train_time=0.04381561279296875
[Epoch 3][Step 111], time=0.08605456352233887, ext_time=0.021147727966308594, train_time=0.04367661476135254
[Epoch 3][Step 112], time=0.08614969253540039, ext_time=0.021251201629638672, train_time=0.04343724250793457
[Epoch 3][Step 113], time=0.08604955673217773, ext_time=0.02126932144165039, train_time=0.04340410232543945
[Epoch 3][Step 114], time=0.08598470687866211, ext_time=0.0210878849029541, train_time=0.0434877872467041
[Epoch 3][Step 115], time=0.09049129486083984, ext_time=0.021213293075561523, train_time=0.04338192939758301
[Epoch 3][Step 116], time=0.08632254600524902, ext_time=0.02132415771484375, train_time=0.04350018501281738
[Epoch 3][Step 117], time=0.08585762977600098, ext_time=0.021094083786010742, train_time=0.043325185775756836
[Epoch 3][Step 118], time=0.08597612380981445, ext_time=0.021225452423095703, train_time=0.043356895446777344
[Epoch 3][Step 119], time=0.0859677791595459, ext_time=0.02110886573791504, train_time=0.0435338020324707
[Epoch 3][Step 120], time=0.08614420890808105, ext_time=0.021176815032958984, train_time=0.043637990951538086
[Epoch 3][Step 121], time=0.08614444732666016, ext_time=0.021214962005615234, train_time=0.04361104965209961
[Epoch 3][Step 122], time=0.08577466011047363, ext_time=0.021066665649414062, train_time=0.04333043098449707
[Epoch 3][Step 123], time=0.08578944206237793, ext_time=0.02116107940673828, train_time=0.043289899826049805
[Epoch 3][Step 124], time=0.08653998374938965, ext_time=0.021145105361938477, train_time=0.04404950141906738
[Epoch 3], time=10.784643650054932, loss=0.4083915054798126
    [Step(average) Profiler Level 1 E3 S999]
        L1  sample           0.021372 | send           0.000000
        L1  recv             0.000000 | copy           0.019996 | convert time 0.000000 | train  0.044877
        L1  feature nbytes 0.00 Bytes | label nbytes 0.00 Bytes
        L1  id nbytes      0.00 Bytes | graph nbytes 0.00 Bytes
        L1  miss nbytes    0.00 Bytes | remote nbytes 0.00 Bytes
        L1  num nodes               0 | num samples           0
        L1  seq duration     0.000000 | refresh duration   0.000000
    [Step(average) Profiler Level 2 E3 S999]
        L2  shuffle     0.000000 | core sample  0.000000 | id remap        0.000000
        L2  graph copy  0.000000 | id copy      0.000000 | cache feat copy 0.019996
        L2  last layer sample time 0.000000 | size 0.000000
    [Step(average) Profiler Level 3 E3 S999]
        L3  khop sample coo  0.000000 | khop sort coo      0.000000 | khop count edge     0.000000 | khop compact edge 0.000000
        L3  walk sample coo  0.000000 | walk topk total    0.000000 | walk topk step1     0.000000 | walk topk step2   0.000000
        L3  walk topk step3  0.000000 | walk topk step4    0.000000 | walk topk step5     0.000000
        L3  walk topk step6  0.000000 | walk topk step7    0.000000
        L3  remap unique     0.000000 | remap populate     0.000000 | remap mapnode       0.000000 | remap mapedge     0.000000
        L3  cache get_index  0.000000 | cache copy_index   0.000000 | cache extract_miss  0.000000
        L3  cache copy_miss  0.000000 | cache combine_miss 0.000000 | cache combine cache 0.000000 | cache combine remote 0.000000
        L3  label extract  0.000000
    [Profiler Level Percentiles E3 S999]
        p50.00_tail_logl2featcopy=0.021128
        p90.00_tail_logl2featcopy=0.021270
        p95.00_tail_logl2featcopy=0.021306
        p99.00_tail_logl2featcopy=0.022382
        p99.90_tail_logl2featcopy=0.024320
[CUDA] cuda: usage: 64.19 GB
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    3621 MB |   10603 MB |   13455 GB |   13452 GB |
|       from large pool |    3611 MB |   10593 MB |   13442 GB |   13439 GB |
|       from small pool |       9 MB |      15 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Active memory         |    3621 MB |   10603 MB |   13455 GB |   13452 GB |
|       from large pool |    3611 MB |   10593 MB |   13442 GB |   13439 GB |
|       from small pool |       9 MB |      15 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   51710 MB |   51710 MB |   51710 MB |       0 B  |
|       from large pool |   51688 MB |   51688 MB |   51688 MB |       0 B  |
|       from small pool |      22 MB |      22 MB |      22 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |    4094 MB |   13074 MB |    8043 GB |    8039 GB |
|       from large pool |    4088 MB |   13068 MB |    8029 GB |    8025 GB |
|       from small pool |       6 MB |      10 MB |      13 GB |      13 GB |
|---------------------------------------------------------------------------|
| Allocations           |      72    |      99    |  143412    |  143340    |
|       from large pool |      23    |      44    |   67000    |   66977    |
|       from small pool |      49    |      60    |   76412    |   76363    |
|---------------------------------------------------------------------------|
| Active allocs         |      72    |      99    |  143412    |  143340    |
|       from large pool |      23    |      44    |   67000    |   66977    |
|       from small pool |      49    |      60    |   76412    |   76363    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      46    |      46    |      46    |       0    |
|       from large pool |      35    |      35    |      35    |       0    |
|       from small pool |      11    |      11    |      11    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      39    |      54    |   53007    |   52968    |
|       from large pool |      19    |      31    |   36459    |   36440    |
|       from small pool |      20    |      29    |   16548    |   16528    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

[TRAIN_TIME] train time is 44.971416 seconds
[EPOCH_TIME] 11.242854 seconds, maybe large due to not enough epoch skipped.
[EPOCH_TIME] 10.785620 seconds
worker 7 running with pid=51542
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=7, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005594, per step: 0.000045
worker 1 running with pid=51530
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=1, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005572, per step: 0.000045
worker 3 running with pid=51534
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=3, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005695, per step: 0.000046
worker 5 running with pid=51538
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=5, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005630, per step: 0.000045
worker 2 running with pid=51532
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=2, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005519, per step: 0.000044
worker 4 running with pid=51536
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=4, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005559, per step: 0.000044
worker 6 running with pid=51540
loading train edge idx from disk
loading train edge idx from disk done
tensor([ 367470406, 3158702471, 3196913887,  534369026, 2622541626, 2707306334,
        2288750783,  341639546, 2394912536,   34847343,  697837268, 1665128055,
        3225579545,  808096539,  530910714,  726851972, 1000854521, 1061370191,
         594157634,  526478766,  496425173, 1382980281, 1363464091,  971316959,
        1643305663, 1559556869,  792051811, 2593715952, 2627476216,  865811872,
        1279512108,  281028249, 2848334841, 1142277951, 1624797255, 1682408139,
         511476070,  753655501, 1190696744, 1515340461,  396873136, 3079813016,
         574876927,   31156367,  830710319, 1664434120,  774080817, 2850247945,
         655998496,  888602410,  350570870,  847108703,   69778231,  638128676,
        1443388341, 1479712286, 2858804367,  550628163,  198023098, 2583590303,
        2518492873,    2593795, 2722269746,  659106604, 1369497066,  458403464,
         645516367, 3053389785,  144872323,  792250187, 1401047469, 1545651011,
        1827797263, 1275030316, 2075895474, 2919804301, 2493902937,  112768749,
          11701078,  723465431,  404737965,  786261376,  934911988,  757583227,
        1135493038, 3162165660, 1144120843, 2861906075,  223193336, 1866677628,
         326241255,   64202517, 1840253021,  970607610,  419197513, 3025516425,
         133597469,  978276161, 2348166713,  303649761])
Rank=6, Graph loaded.
!!!!dist_homo_graph enumerate latency per epoch: 0.005692, per step: 0.000046

