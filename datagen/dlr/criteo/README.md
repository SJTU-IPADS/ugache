# Overall

This directory contains tools to preprocess Criteo dataset.

In Criteo dataset, each input consists of 1 label, 13 decimal feature and 26 sparse feature.

To download Criteo TB dataset, follow the [link](https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/). After unzip the downloaded zip file and getting day_0.gz ~ day_23.gz, you can run scripts to preprocess this dataset. To transfer the raw Criteo(Kaggle or TB) dataset into a UGache dataset, simply run `bash criteo.sh`. Note that day_0.gz ~ day_23.gz should be placed under `/datasets_dlr/data-raw/criteo_tb/`. The output should be like:
```bash
> bash criteo.sh
Processing data files under root directory '/datasets_dlr/'..
gzip -cd /datasets_dlr/data-raw/criteo_tb/day_0.gz > /datasets_dlr/data-raw/criteo_tb/day_0...
gzip -cd /datasets_dlr/data-raw/criteo_tb/day_1.gz > /datasets_dlr/data-raw/criteo_tb/day_1...
...
gzip -cd /datasets_dlr/data-raw/criteo_tb/day_23.gz > /datasets_dlr/data-raw/criteo_tb/day_23...
./step_1_unique_keys.out /datasets_dlr/data-raw/criteo_tb/ /datasets_dlr/data-raw/criteo_tb/ day_0 day_1 ... day_23 ...
./step_2_replace_keys.out /datasets_dlr/data-raw/criteo_tb/ /datasets_dlr/processed/criteo_tb/ /datasets_dlr/data-raw/criteo_tb/ day_0 day_1 ... day_23...
./step_3_slice_processed.out /datasets_dlr/processed/criteo_tb/day_0 13 26 1 ...
./step_3_slice_processed.out /datasets_dlr/processed/criteo_tb/day_1 13 26 1 ...
...
./step_3_slice_processed.out /datasets_dlr/processed/criteo_tb/day_23 13 26 1 ...
```
And finally you should get `day_concat.label`, `day_concat.dense` and `day_concat.sparse` under directory `/datasets_dlr/processed/criteo_tb/`, which are the data files used by our system.

# Preprocess Criteo dataset in detail

If you run into errors using `criteo.sh`, you can try run preprocessing scripts by yourself. Before running the following steps, use `gzip -cd day_$i.gz > day_$i` to decompress day_0.gz ~ day_23.gz. Then run the following 3 steps:

1. record unique keys in each slot, use `./step_1_unique_keys.out [input_dir] [output_dir] [input_file1] [input_file2] ...` to generate unique key files for each slot. 
For example: `./step_1_unique_keys.out /datasets_dlr/data-raw/criteo_tb/ /datasets_dlr/data-raw/criteo_tb/ day_1 day_2` will calculate all the  unique keys appeared in file `/datasets_dlr/data-raw/criteo_tb/day_1` and `/datasets_dlr/data-raw/criteo_tb/day_2`, and port unique keys of each slot into `/datasets_dlr/data-raw/criteo_tb/`.

2. replace the origin key in raw data file(one by one) with continuous integer keys starting from 0, use `./step_2_replace_keys.out [input_dir] [output_dir] [key_dir] [input_file1] [input_file2] ...` to process a set of raw data files. 
For example: `./step_2_replace_keys.out /datasets_dlr/data-raw/criteo_tb/ /datasets_dlr/processed/criteo_tb/ /datasets_dlr/data-raw/criteo_tb/ day_1 day_2` will replace keys in raw data file `/datasets_dlr/data-raw/criteo_tb/day_1` and `/datasets_dlr/data-raw/criteo_tb/day_2`, and output processed data file into `/datasets_dlr/processed/criteo_tb/`.

3. Use command `./step_3_slice_processed.out [input_file] 13 26 1` to split a single data file(generated by step 2) into 3 data files: `xx.label`, `xx.dense` and `xx.sparse`. For example: `./step_3_slice_processed.out /datasets_dlr/processed/criteo_tb/day_1 13 26 1` will split file `/datasets_dlr/processed/criteo_tb/day_1` into `/datasets_dlr/processed/criteo_tb/day_1.label`, `/datasets_dlr/processed/criteo_tb/day_1.sparse` and `/datasets_dlr/processed/criteo_tb/day_1.dense`.
You can further concat processed data file into one by simply `cat day_2.label >> day_concat.label; cat day_2.dense >> day_concat.dense; cat day_2.sparse >> day_concat.sparse; rm day_2.*`.

